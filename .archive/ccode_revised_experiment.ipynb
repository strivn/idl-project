{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset, Dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ivw/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/ivw/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' it?\" I asked.\\n\\n\"I think about it all the time,\" he said. \"What time is it?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"afterless/reverse-pythia-160m\"\n",
    ")\n",
    "model = GPTNeoXForCausalLM.from_pretrained(\n",
    "    \"afterless/reverse-pythia-160m\",\n",
    "    cache_dir=\"./.cache/reverse-pythia-160m\",\n",
    ")\n",
    "\n",
    "inputs = tokenizer(\n",
    "    \"What time is it?\",\n",
    "    return_token_type_ids=False,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "inputs['input_ids'] = t.flip(inputs.input_ids, (1,))\n",
    "tokens = t.flip(model.generate(**inputs), (1,))\n",
    "tokenizer.decode(tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "def load_models(device='cpu'):\n",
    "    # Forward model\n",
    "    fo_model = GPTNeoXForCausalLM.from_pretrained(\n",
    "        \"EleutherAI/pythia-160m-deduped\",\n",
    "        revision=\"step143000\",\n",
    "        cache_dir=\"./.cache/pythia-160m-deduped/step143000\",\n",
    "    ).to(device)\n",
    "    \n",
    "    fo_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"EleutherAI/pythia-160m-deduped\",\n",
    "        revision=\"step143000\",\n",
    "        cache_dir=\"./.cache/pythia-160m-deduped/step143000\",\n",
    "    )\n",
    "    \n",
    "    # Backward model\n",
    "    ba_model = GPTNeoXForCausalLM.from_pretrained(\n",
    "        \"afterless/reverse-pythia-160m\",\n",
    "        cache_dir=\"./.cache/reverse-pythia-160m\",\n",
    "    ).to(device)\n",
    "    \n",
    "    ba_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"afterless/reverse-pythia-160m\",\n",
    "        cache_dir=\"./.cache/reverse-pythia-160m\",\n",
    "    )\n",
    "    \n",
    "    return fo_model, fo_tokenizer, ba_model, ba_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "\n",
    "def calculate_llm_score(prompt, response, model, tokenizer, direction=\"forward\"):\n",
    "    \"\"\"\n",
    "    Calculate log probability of response given prompt or vice versa.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The prompt text\n",
    "        response (str): The response text\n",
    "        model: The language model\n",
    "        tokenizer: The corresponding tokenizer\n",
    "        direction (str): \"forward\" for P(response|prompt) or \"backward\" for P(prompt|response)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains token-wise and sequence log probabilities\n",
    "    \"\"\"\n",
    "    # Prepare text based on direction\n",
    "    if direction == \"forward\":\n",
    "        # Forward: calculate P(Response|Prompt)\n",
    "        full_text = prompt + response\n",
    "        input_ids = tokenizer.encode(full_text, return_tensors=\"pt\").to(model.device)\n",
    "        prompt_len = len(tokenizer.encode(prompt)) - 1  # -1 because we don't count the first token in scoring\n",
    "        target_len = len(tokenizer.encode(response))\n",
    "    else:\n",
    "        # Backward: calculate P(Prompt|Response)\n",
    "        # For backward model, we reverse the text and calculate from the end\n",
    "        full_text = response + prompt\n",
    "        input_ids = tokenizer.encode(full_text, return_tensors=\"pt\").to(model.device)\n",
    "        prompt_len = len(tokenizer.encode(response)) - 1\n",
    "        target_len = len(tokenizer.encode(prompt))\n",
    "    \n",
    "    # Get model output\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    # Extract token probabilities for the target sequence\n",
    "    token_probs = []\n",
    "    for i in range(prompt_len, prompt_len + target_len - 1):\n",
    "        next_token_logits = logits[0, i, :]\n",
    "        next_token_id = input_ids[0, i+1].item()\n",
    "        \n",
    "        # Convert logits to probabilities\n",
    "        next_token_probs = F.softmax(next_token_logits, dim=0)\n",
    "        prob = next_token_probs[next_token_id].item()\n",
    "        log_prob = np.log(prob)\n",
    "        \n",
    "        token_text = tokenizer.decode([next_token_id])\n",
    "        token_probs.append({\n",
    "            'token': token_text,\n",
    "            'token_id': next_token_id,\n",
    "            'log_prob': log_prob\n",
    "        })\n",
    "    \n",
    "    # Calculate sequence probability\n",
    "    sequence_log_prob = sum(tp['log_prob'] for tp in token_probs)\n",
    "    # Normalize by length to get per-token average\n",
    "    normalized_log_prob = sequence_log_prob / len(token_probs)\n",
    "    # Convert to perplexity if needed\n",
    "    perplexity = np.exp(-sequence_log_prob / len(token_probs))\n",
    "    \n",
    "    return {\n",
    "        'token_log_probs': token_probs,\n",
    "        'sequence_log_prob': sequence_log_prob,\n",
    "        'normalized_log_prob': normalized_log_prob,\n",
    "        'perplexity': perplexity\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "def score_text_pair(prompt, response, fo_model, fo_tokenizer, ba_model, ba_tokenizer):\n",
    "    # Forward score: P(Response|Prompt)\n",
    "    forward_score = calculate_llm_score(\n",
    "        prompt, response, \n",
    "        fo_model, fo_tokenizer, \n",
    "        direction=\"forward\"\n",
    "    )\n",
    "    \n",
    "    # Backward score: P(Prompt|Response)\n",
    "    backward_score = calculate_llm_score(\n",
    "        response, prompt,\n",
    "        ba_model, ba_tokenizer,\n",
    "        direction=\"backward\"\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'forward': forward_score,\n",
    "        'backward': backward_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward (P(Response|Prompt)) - Log probability: -27.7338, Perplexity: 1026.0319\n",
      "Backward (P(Prompt|Response)) - Log probability: -36.1751, Perplexity: 8465.7544\n"
     ]
    }
   ],
   "source": [
    "# Load the models\n",
    "fo_model, fo_tokenizer, ba_model, ba_tokenizer = load_models(device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Example text\n",
    "prompt = \"ABCDE in reverse\"\n",
    "response = \" is simply EDCBA\"\n",
    "\n",
    "# Get scores\n",
    "scores = score_text_pair(prompt, response, fo_model, fo_tokenizer, ba_model, ba_tokenizer)\n",
    "\n",
    "# Print results\n",
    "print(f\"Forward (P(Response|Prompt)) - Log probability: {scores['forward']['sequence_log_prob']:.4f}, Perplexity: {scores['forward']['perplexity']:.4f}\")\n",
    "print(f\"Backward (P(Prompt|Response)) - Log probability: {scores['backward']['sequence_log_prob']:.4f}, Perplexity: {scores['backward']['perplexity']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct highlight scores:\n",
      "Forward score: -3.4735\n",
      "Backward score: -4.8827\n",
      "Combined score: -8.3562\n",
      "\n",
      "Adverse highlight scores:\n",
      "Forward score: -7.0841\n",
      "Backward score: -10.2840\n",
      "Combined score: -17.3682\n"
     ]
    }
   ],
   "source": [
    "# Your example texts\n",
    "sentence = \"Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him.\"\n",
    "highlight = \"Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday\"\n",
    "adverse_highlight = \"Daniel Craig is recasted as James Bond again\"\n",
    "\n",
    "# Define prompts\n",
    "SP = None  # Scoring Prompt\n",
    "CP = \"is summarized by\"  # Conditioning Prompt\n",
    "Q = sentence  # Query\n",
    "A = highlight  # Response\n",
    "\n",
    "def score_summary(original_text, summary_text, condition_prompt, score_prompt=None):\n",
    "    \"\"\"\n",
    "    Score how well a summary represents the original text\n",
    "    \"\"\"\n",
    "    # Create the inputs as specified\n",
    "    input_text = f\"{score_prompt or ''}{original_text}\"\n",
    "    output_text = f\"{condition_prompt}{summary_text}\"\n",
    "    \n",
    "    # Score using forward model (P(summary|original))\n",
    "    forward_score = calculate_llm_score(\n",
    "        input_text, output_text,\n",
    "        fo_model, fo_tokenizer,\n",
    "        direction=\"forward\"\n",
    "    )\n",
    "    \n",
    "    # Score using backward model (P(original|summary))\n",
    "    backward_score = calculate_llm_score(\n",
    "        output_text, input_text,\n",
    "        ba_model, ba_tokenizer,\n",
    "        direction=\"backward\"\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'forward_score': forward_score['normalized_log_prob'],\n",
    "        'backward_score': backward_score['normalized_log_prob'],\n",
    "        'combined_score': forward_score['normalized_log_prob'] + backward_score['normalized_log_prob']\n",
    "    }\n",
    "\n",
    "# Score the correct highlight\n",
    "correct_scores = score_summary(Q, A, CP, SP)\n",
    "print(\"Correct highlight scores:\")\n",
    "print(f\"Forward score: {correct_scores['forward_score']:.4f}\")\n",
    "print(f\"Backward score: {correct_scores['backward_score']:.4f}\")\n",
    "print(f\"Combined score: {correct_scores['combined_score']:.4f}\")\n",
    "\n",
    "# Score the adverse highlight for comparison\n",
    "adverse_scores = score_summary(Q, adverse_highlight, CP, SP)\n",
    "print(\"\\nAdverse highlight scores:\")\n",
    "print(f\"Forward score: {adverse_scores['forward_score']:.4f}\")\n",
    "print(f\"Backward score: {adverse_scores['backward_score']:.4f}\")\n",
    "print(f\"Combined score: {adverse_scores['combined_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CausalLMOutputWithPast(loss=None, logits=tensor([[[-2.5526, -2.8882,  7.4647,  ..., -2.8904, -2.8837, -2.8828],\n",
      "         [-1.0366, -1.3005, 10.2495,  ..., -1.2970, -1.2920, -1.2922],\n",
      "         [-2.0659, -1.9825,  6.2695,  ..., -1.9774, -1.9772, -1.9792],\n",
      "         ...,\n",
      "         [-1.3120, -1.6416,  9.7668,  ..., -1.6346, -1.6288, -1.6395],\n",
      "         [ 0.6956,  0.2210,  9.2045,  ...,  0.2218,  0.2219,  0.2160],\n",
      "         [-1.4803, -1.6478,  8.9454,  ..., -1.6395, -1.6371, -1.6486]]]), past_key_values=DynamicCache(), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m ba_model(input_ids\u001b[38;5;241m=\u001b[39mreversed_CP_plus_A)\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(outputs)\n\u001b[0;32m---> 32\u001b[0m     trlm_ba_score \u001b[38;5;241m=\u001b[39m \u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTRLM-Ba Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrlm_ba_score\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# prompt = f\"{sentence} is summarized by: {highlight}\"\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# tokens = ba_tokenizer(prompt, return_tensors=\"pt\").input_ids\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# print(tokens)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m \n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# trlm_ba_scores\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'item'"
     ]
    }
   ],
   "source": [
    "fo_model, fo_tokenizer, ba_model, ba_tokenizer = load_models()\n",
    "\n",
    "trlm_ba_scores = []\n",
    "\n",
    "sentence = \"Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won\\'t cast a spell on him.\" \n",
    "highlight = \"Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday\"\n",
    "adverse_highlight = \"Daniel Craig is recasted as James Bond again\"\n",
    "\n",
    "# Define prompts\n",
    "SP = None  # Scoring Prompt\n",
    "CP = \"is summarized by\"   # Conditioning Prompt\n",
    "Q = sentence       # Query\n",
    "A = highlight      # Response\n",
    "\n",
    "# Create the components according to the algorithm\n",
    "SP_plus_Q = f\"{SP}{Q}\"\n",
    "CP_plus_A = f\"{CP}{A}\"\n",
    "\n",
    "# Tokenize each part\n",
    "tokens_SP_plus_Q = ba_tokenizer(SP_plus_Q, return_tensors=\"pt\").input_ids\n",
    "tokens_CP_plus_A = ba_tokenizer(CP_plus_A, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Reverse the tokens\n",
    "reversed_SP_plus_Q = t.flip(tokens_SP_plus_Q, dims=[1])\n",
    "reversed_CP_plus_A = t.flip(tokens_CP_plus_A, dims=[1])\n",
    "\n",
    "# Calculate conditional probability\n",
    "# We use reversed_CP_plus_A as input and reversed_SP_plus_Q as target\n",
    "with t.no_grad():\n",
    "    outputs = ba_model(input_ids=reversed_CP_plus_A, labels=reversed_SP_plus_Q)\n",
    "    print(outputs)\n",
    "    trlm_ba_score = outputs.loss.item()\n",
    "\n",
    "print(f\"TRLM-Ba Score: {trlm_ba_score}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# prompt = f\"{sentence} is summarized by: {highlight}\"\n",
    "# tokens = ba_tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "# print(tokens)\n",
    "# reversed_tokens = t.flip(tokens, dims=[1])\n",
    "# print(reversed_tokens)\n",
    "\n",
    "# with t.no_grad():\n",
    "#     outputs = ba_model(reversed_tokens, labels=reversed_tokens)\n",
    "#     trlm_ba_scores.append(outputs.loss.item())\n",
    "    \n",
    "    \n",
    "\n",
    "# # Reverse strings\n",
    "# reverse_sp_q = scoring_prompt + query\n",
    "# reverse_cp_a = conditioning_prompt + response\n",
    "\n",
    "# # Tokenize inputs\n",
    "# inputs = ba_tokenizer(reverse_cp_a, return_tensors=\"pt\")\n",
    "# with ba_tokenizer.as_target_tokenizer():\n",
    "#     labels = ba_tokenizer(reverse_sp_q, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# # Calculate log probability\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(**inputs, labels=labels)\n",
    "#     print(-outputs.loss.item())  # Negative loss approximates log probability\n",
    "\n",
    "# trlm_ba_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' has happened to me, and I don’t know how to get out of it.\\n\\nis summarized by the sentence Daniel Radcliffe gets £20M fortune as he turns 18'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "def load_cnn_dataset(num_samples=10):\n",
    "    try:\n",
    "        # Try with a specific cache directory\n",
    "        dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", cache_dir=\".cache\")\n",
    "        print(\"Dataset loaded successfully\")\n",
    "        \n",
    "        # Verify the structure - this helps debug\n",
    "        if num_samples > 0:\n",
    "            print(\"Example dataset item:\", dataset['train'][0])\n",
    "            \n",
    "        # Take only a small sample for testing\n",
    "        if hasattr(dataset, 'train'):\n",
    "            return dataset['train'].select(range(min(num_samples, len(dataset['train']))))\n",
    "        \n",
    "        return dataset['train'][:num_samples]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading full dataset: {e}\")\n",
    "        \n",
    "        # Create a tiny synthetic dataset for testing\n",
    "        print(\"Creating synthetic test dataset instead...\")\n",
    "        \n",
    "        sample_data = {\n",
    "            'article': [\n",
    "                \"John likes to play basketball. He goes to the court every evening. His friends join him on weekends.\",\n",
    "                \"The company announced record profits. Investors were pleased. The stock price increased by 10%.\"\n",
    "            ],\n",
    "            'highlights': [\n",
    "                \"John plays basketball regularly with friends.\",\n",
    "                \"Company profits lead to stock price increase.\"\n",
    "            ],\n",
    "            'id': ['test1', 'test2']  # Added ID field\n",
    "        }\n",
    "        \n",
    "        return Dataset.from_dict(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(example):\n",
    "    \"\"\"\n",
    "    Process the CNN dataset example to extract article and highlight sentences\n",
    "    \"\"\"\n",
    "    # Handle the article - could be a string or a list\n",
    "    if isinstance(example['article'], list):\n",
    "        # Already in list format\n",
    "        article_sentences = example['article']\n",
    "    else:\n",
    "        # Need to tokenize\n",
    "        article_sentences = nltk.sent_tokenize(example['article'])\n",
    "    \n",
    "    # Handle the highlights - could be a string or a list\n",
    "    if isinstance(example['highlights'], list):\n",
    "        # Already in list format\n",
    "        highlight_sentences = example['highlights']\n",
    "    else:\n",
    "        # Need to tokenize\n",
    "        highlight_sentences = nltk.sent_tokenize(example['highlights'])\n",
    "    \n",
    "    return {\n",
    "        'article_sentences': article_sentences,\n",
    "        'highlight_sentences': highlight_sentences\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def forward_baseline_score(model, tokenizer, article_sentence, highlight, device='cpu'):\n",
    "#     \"\"\"Compute P(highlight|article_sentence)\"\"\"\n",
    "#     # Format: [article_sentence] is summarized by: [highlight]\n",
    "#     input_text = f\"{article_sentence} is summarized by: \"\n",
    "    \n",
    "#     # Tokenize the input and target separately\n",
    "#     input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "#     target_ids = tokenizer(highlight, return_tensors=\"pt\").input_ids.to(device)\n",
    "    \n",
    "#     # Concatenate for full sequence, but remember where the split is\n",
    "#     full_ids = t.cat([input_ids, target_ids[:, 1:]], dim=1)  # Skip BOS token for target\n",
    "    \n",
    "#     # Setup for loss calculation - we only want loss on highlight tokens\n",
    "#     labels = t.full_like(full_ids, -100)  # -100 is ignored in loss calculation\n",
    "#     labels[:, input_ids.shape[1]:] = target_ids[:, 1:]  # Only compute loss on highlight\n",
    "    \n",
    "#     # Calculate loss\n",
    "#     outputs = model(full_ids, labels=labels)\n",
    "#     return outputs.loss.item()\n",
    "\n",
    "# def trlm_fo_score(model, tokenizer, article_sentence, highlight, device='cpu'):\n",
    "#     \"\"\"Compute P(article_sentence|highlight)\"\"\"\n",
    "#     # Format: [article_sentence] is a summary of: [highlight]\n",
    "#     input_text = f\"{highlight} is a summary of: \"\n",
    "    \n",
    "#     # Tokenize the input and target separately\n",
    "#     input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "#     target_ids = tokenizer(article_sentence, return_tensors=\"pt\").input_ids.to(device)\n",
    "    \n",
    "#     # Concatenate for full sequence\n",
    "#     full_ids = t.cat([input_ids, target_ids[:, 1:]], dim=1)\n",
    "    \n",
    "#     # Setup for loss calculation - we only want loss on article sentence tokens\n",
    "#     labels = t.full_like(full_ids, -100)\n",
    "#     labels[:, input_ids.shape[1]:] = target_ids[:, 1:]\n",
    "    \n",
    "#     # Calculate loss\n",
    "#     outputs = model(full_ids, labels=labels)\n",
    "#     return outputs.loss.item()\n",
    "\n",
    "# def trlm_ba_score(backward_model, backward_tokenizer, article_sentence, highlight, device='cpu'):\n",
    "#     \"\"\"Compute P(article_sentence|highlight) using backward model\"\"\"\n",
    "#     # Format according to paper: [highlight] is summarized by: [article_sentence]\n",
    "    \n",
    "#     # We need to tokenize each part separately to know token boundaries\n",
    "#     highlight_tokens = backward_tokenizer(highlight, return_tensors=\"pt\").input_ids.to(device)\n",
    "#     connector_tokens = backward_tokenizer(\" is summarized by: \", return_tensors=\"pt\").input_ids.to(device)\n",
    "#     article_tokens = backward_tokenizer(article_sentence, return_tensors=\"pt\").input_ids.to(device)\n",
    "    \n",
    "#     # Combine all tokens (removing extra BOS tokens if needed)\n",
    "#     # Keep only first BOS token, remove others\n",
    "#     if connector_tokens.size(1) > 1:\n",
    "#         connector_tokens = connector_tokens[:, 1:]\n",
    "#     if article_tokens.size(1) > 1:\n",
    "#         article_tokens = article_tokens[:, 1:]\n",
    "    \n",
    "#     combined_tokens = t.cat([highlight_tokens, connector_tokens, article_tokens], dim=1)\n",
    "    \n",
    "#     # Now reverse the combined tokens\n",
    "#     reversed_tokens = t.flip(combined_tokens, dims=[1])\n",
    "    \n",
    "#     # Create labels tensor - start with all -100 (ignored positions)\n",
    "#     labels = t.full_like(reversed_tokens, -100)\n",
    "    \n",
    "#     # In the reversed sequence, the article tokens appear at the beginning\n",
    "#     # The length of article_tokens tells us how many tokens to score\n",
    "#     article_length = article_tokens.size(1)\n",
    "    \n",
    "#     # Set the labels for article tokens (now at the beginning of reversed sequence)\n",
    "#     labels[:, :article_length] = reversed_tokens[:, :article_length]\n",
    "    \n",
    "#     # Calculate loss only on the article tokens\n",
    "#     outputs = backward_model(reversed_tokens, labels=labels)\n",
    "#     return outputs.loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(predicted_sentence, gold_sentences):\n",
    "    \"\"\"\n",
    "    Calculate similarity metrics between predicted sentence and gold references\n",
    "    \"\"\"\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt')\n",
    "    \n",
    "    # Handle empty inputs\n",
    "    if not predicted_sentence or not gold_sentences:\n",
    "        return {\n",
    "            'tfidf_similarity': 0,\n",
    "            'embedding_similarity': 0,\n",
    "            'rouge1': 0,\n",
    "            'rougeL': 0,\n",
    "            'bleu': 0\n",
    "        }\n",
    "    \n",
    "    # Load a sentence embedding model\n",
    "    sentence_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "    \n",
    "    # TF-IDF similarity\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    try:\n",
    "        tfidf_matrix = vectorizer.fit_transform([predicted_sentence] + gold_sentences)\n",
    "        tfidf_similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:])[0]\n",
    "        tfidf_score = max(tfidf_similarities) if len(tfidf_similarities) > 0 else 0\n",
    "    except:\n",
    "        tfidf_score = 0\n",
    "    \n",
    "    # Embedding similarity\n",
    "    try:\n",
    "        pred_embedding = sentence_model.encode(predicted_sentence)\n",
    "        gold_embeddings = sentence_model.encode(gold_sentences)\n",
    "        embedding_similarities = cosine_similarity([pred_embedding], gold_embeddings)[0]\n",
    "        embedding_score = max(embedding_similarities) if len(embedding_similarities) > 0 else 0\n",
    "    except:\n",
    "        embedding_score = 0\n",
    "    \n",
    "    # ROUGE score\n",
    "    try:\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "        rouge_scores = [scorer.score(predicted_sentence, gold_sent) for gold_sent in gold_sentences]\n",
    "        rouge1_score = max([score['rouge1'].fmeasure for score in rouge_scores]) if rouge_scores else 0\n",
    "        rougeL_score = max([score['rougeL'].fmeasure for score in rouge_scores]) if rouge_scores else 0\n",
    "    except:\n",
    "        rouge1_score = 0\n",
    "        rougeL_score = 0\n",
    "    \n",
    "    # BLEU score with smoothing to avoid zero scores\n",
    "    try:\n",
    "        # Tokenize for BLEU calculation\n",
    "        predicted_tokens = nltk.word_tokenize(predicted_sentence.lower())\n",
    "        gold_tokens = [nltk.word_tokenize(gold.lower()) for gold in gold_sentences]\n",
    "        \n",
    "        # Use smoothing function to mitigate \"0 counts of n-gram overlaps\" warnings\n",
    "        smoother = SmoothingFunction().method1\n",
    "        \n",
    "        # Calculate BLEU score - take the best score against any reference\n",
    "        bleu_scores = [\n",
    "            sentence_bleu([gold_tok], predicted_tokens, \n",
    "                         weights=(0.25, 0.25, 0.25, 0.25),\n",
    "                         smoothing_function=smoother)\n",
    "            for gold_tok in gold_tokens\n",
    "        ]\n",
    "        bleu_score = max(bleu_scores) if bleu_scores else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating BLEU: {e}\")\n",
    "        bleu_score = 0\n",
    "    \n",
    "    return {\n",
    "        'tfidf_similarity': tfidf_score,\n",
    "        'embedding_similarity': embedding_score,\n",
    "        'rouge1': rouge1_score,\n",
    "        'rougeL': rougeL_score,\n",
    "        'bleu': bleu_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def calculate_model_scores(fo_model, fo_tokenizer, ba_model, ba_tokenizer, \n",
    "#                           article_sentences, highlight, device='cpu'):\n",
    "#     \"\"\"\n",
    "#     Calculate scores for both forward and backward models properly.\n",
    "#     Returns scores and selected sentences.\n",
    "#     \"\"\"\n",
    "#     # Forward baseline scoring: P(highlight|sentence)\n",
    "#     fw_scores = []\n",
    "#     for sentence in article_sentences:\n",
    "#         # Condition = sentence, Target = highlight\n",
    "#         condition = f\"{sentence} is summarized by: \"\n",
    "        \n",
    "#         # Tokenize condition and target separately\n",
    "#         condition_ids = fo_tokenizer(condition, return_tensors=\"pt\").input_ids.to(device)\n",
    "#         target_ids = fo_tokenizer(highlight, return_tensors=\"pt\").input_ids.to(device)\n",
    "        \n",
    "#         # Combine them (skipping BOS token in target)\n",
    "#         input_ids = t.cat([condition_ids, target_ids[:, 1:] if target_ids.size(1) > 1 else target_ids], dim=1)\n",
    "        \n",
    "#         # Create label mask that only calculates loss on target tokens\n",
    "#         labels = t.full_like(input_ids, -100)\n",
    "#         labels[:, condition_ids.size(1):] = input_ids[:, condition_ids.size(1):]\n",
    "        \n",
    "#         # Calculate loss\n",
    "#         with t.no_grad():\n",
    "#             outputs = fo_model(input_ids.to(device), labels=labels.to(device))\n",
    "#             fw_scores.append(outputs.loss.item())\n",
    "    \n",
    "#     # TRLM-Fo scoring: P(sentence|highlight)\n",
    "#     trlm_fo_scores = []\n",
    "#     for sentence in article_sentences:\n",
    "#         # Condition = highlight, Target = sentence\n",
    "#         condition = f\"{highlight} is a summary of: \"\n",
    "        \n",
    "#         # Tokenize condition and target separately\n",
    "#         condition_ids = fo_tokenizer(condition, return_tensors=\"pt\").input_ids.to(device)\n",
    "#         target_ids = fo_tokenizer(sentence, return_tensors=\"pt\").input_ids.to(device)\n",
    "        \n",
    "#         # Combine them\n",
    "#         input_ids = t.cat([condition_ids, target_ids[:, 1:] if target_ids.size(1) > 1 else target_ids], dim=1)\n",
    "        \n",
    "#         # Create label mask\n",
    "#         labels = t.full_like(input_ids, -100)\n",
    "#         labels[:, condition_ids.size(1):] = input_ids[:, condition_ids.size(1):]\n",
    "        \n",
    "#         # Calculate loss\n",
    "#         with t.no_grad():\n",
    "#             outputs = fo_model(input_ids.to(device), labels=labels.to(device))\n",
    "#             trlm_fo_scores.append(outputs.loss.item())\n",
    "    \n",
    "#     # TRLM-Ba scoring (using backward model)\n",
    "#     trlm_ba_scores = []\n",
    "#     for sentence in article_sentences:\n",
    "#         # Format text according to TRLM-Ba\n",
    "#         text = f\"{highlight} is summarized by: {sentence}\"\n",
    "        \n",
    "#         # Tokenize and reverse\n",
    "#         tokens = ba_tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "#         reversed_tokens = t.flip(tokens, dims=[1])\n",
    "        \n",
    "#         # We would need a more sophisticated approach to properly mask the labels\n",
    "#         # For simplicity, we'll use the full sequence loss here\n",
    "#         with t.no_grad():\n",
    "#             outputs = ba_model(reversed_tokens, labels=reversed_tokens)\n",
    "#             trlm_ba_scores.append(outputs.loss.item())\n",
    "    \n",
    "#     # Find best match for each model\n",
    "#     fw_best_idx = np.argmin(fw_scores)\n",
    "#     trlm_fo_best_idx = np.argmin(trlm_fo_scores)\n",
    "#     trlm_ba_best_idx = np.argmin(trlm_ba_scores)\n",
    "    \n",
    "#     return {\n",
    "#         'forward': {\n",
    "#             'scores': fw_scores,\n",
    "#             'best_sentence': article_sentences[fw_best_idx],\n",
    "#             'best_score': fw_scores[fw_best_idx]\n",
    "#         },\n",
    "#         'trlm_fo': {\n",
    "#             'scores': trlm_fo_scores,\n",
    "#             'best_sentence': article_sentences[trlm_fo_best_idx],\n",
    "#             'best_score': trlm_fo_scores[trlm_fo_best_idx]\n",
    "#         },\n",
    "#         'trlm_ba': {\n",
    "#             'scores': trlm_ba_scores,\n",
    "#             'best_sentence': article_sentences[trlm_ba_best_idx],\n",
    "#             'best_score': trlm_ba_scores[trlm_ba_best_idx]\n",
    "#         }\n",
    "#     }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_model_scores(fo_model, fo_tokenizer, ba_model, ba_tokenizer, \n",
    "                          article_sentences, highlight, device='cpu'):\n",
    "    \"\"\"\n",
    "    Calculate scores for all models and return the best matching sentences.\n",
    "    \"\"\"\n",
    "    # Calculate scores for each model\n",
    "    fw_scores = [forward_baseline_score(fo_model, fo_tokenizer, sentence, highlight, device) \n",
    "                for sentence in article_sentences]\n",
    "    \n",
    "    trlm_fo_scores = [trlm_fo_score(fo_model, fo_tokenizer, sentence, highlight, device) \n",
    "                     for sentence in article_sentences]\n",
    "    \n",
    "    trlm_ba_scores = [trlm_ba_score(ba_model, ba_tokenizer, sentence, highlight, device) \n",
    "                     for sentence in article_sentences]\n",
    "    \n",
    "    # Find best match for each model\n",
    "    fw_best_idx = np.argmin(fw_scores)\n",
    "    trlm_fo_best_idx = np.argmin(trlm_fo_scores)\n",
    "    trlm_ba_best_idx = np.argmin(trlm_ba_scores)\n",
    "    \n",
    "    return {\n",
    "        'forward': {\n",
    "            'scores': fw_scores,\n",
    "            'best_sentence': article_sentences[fw_best_idx],\n",
    "            'best_score': fw_scores[fw_best_idx]\n",
    "        },\n",
    "        'trlm_fo': {\n",
    "            'scores': trlm_fo_scores,\n",
    "            'best_sentence': article_sentences[trlm_fo_best_idx],\n",
    "            'best_score': trlm_fo_scores[trlm_fo_best_idx]\n",
    "        },\n",
    "        'trlm_ba': {\n",
    "            'scores': trlm_ba_scores,\n",
    "            'best_sentence': article_sentences[trlm_ba_best_idx],\n",
    "            'best_score': trlm_ba_scores[trlm_ba_best_idx]\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_model_scores(fo_model, fo_tokenizer, ba_model, ba_tokenizer, \n",
    "                          article_sentences, highlight, device='cpu'):\n",
    "    \"\"\"\n",
    "    A simplified, more direct implementation to debug the issue.\n",
    "    \"\"\"\n",
    "    # Forward baseline scoring (query → response)\n",
    "    fw_scores = []\n",
    "    for sentence in article_sentences:\n",
    "        # Standard way: how likely is the highlight given the sentence\n",
    "        prompt = f\"{sentence} is summarized by: {highlight}\"\n",
    "        inputs = fo_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with t.no_grad():\n",
    "            outputs = fo_model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            fw_scores.append(outputs.loss.item())\n",
    "    \n",
    "    # TRLM-Fo scoring (response → query using forward model)\n",
    "    trlm_fo_scores = []\n",
    "    for sentence in article_sentences:\n",
    "        # Reverse direction: how likely is the sentence given the highlight\n",
    "        prompt = f\"{highlight} is a summary of: {sentence}\"\n",
    "        inputs = fo_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with t.no_grad():\n",
    "            outputs = fo_model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            trlm_fo_scores.append(outputs.loss.item())\n",
    "    \n",
    "    # TRLM-Ba scoring (response → query using backward model)\n",
    "    trlm_ba_scores = []\n",
    "    for sentence in article_sentences:\n",
    "        # For backward model: reverse the entire prompt\n",
    "        prompt = f\"{sentence} is summarized by: {highlight}\"\n",
    "        tokens = ba_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "        reversed_tokens = t.flip(tokens, dims=[1])\n",
    "        \n",
    "        with t.no_grad():\n",
    "            outputs = ba_model(reversed_tokens, labels=reversed_tokens)\n",
    "            trlm_ba_scores.append(outputs.loss.item())\n",
    "    \n",
    "    # Find best match for each model\n",
    "    fw_best_idx = np.argmin(fw_scores)\n",
    "    trlm_fo_best_idx = np.argmin(trlm_fo_scores)\n",
    "    trlm_ba_best_idx = np.argmin(trlm_ba_scores)\n",
    "    \n",
    "    # Print some debug info\n",
    "    print(\"\\nDEBUG INFO:\")\n",
    "    print(f\"Forward scores (min: {min(fw_scores):.4f}, idx: {fw_best_idx})\")\n",
    "    print(f\"TRLM-Fo scores (min: {min(trlm_fo_scores):.4f}, idx: {trlm_fo_best_idx})\")\n",
    "    print(f\"TRLM-Ba scores (min: {min(trlm_ba_scores):.4f}, idx: {trlm_ba_best_idx})\")\n",
    "    \n",
    "    return {\n",
    "        'forward': {\n",
    "            'scores': fw_scores,\n",
    "            'best_sentence': article_sentences[fw_best_idx],\n",
    "            'best_score': fw_scores[fw_best_idx]\n",
    "        },\n",
    "        'trlm_fo': {\n",
    "            'scores': trlm_fo_scores,\n",
    "            'best_sentence': article_sentences[trlm_fo_best_idx],\n",
    "            'best_score': trlm_fo_scores[trlm_fo_best_idx]\n",
    "        },\n",
    "        'trlm_ba': {\n",
    "            'scores': trlm_ba_scores,\n",
    "            'best_sentence': article_sentences[trlm_ba_best_idx],\n",
    "            'best_score': trlm_ba_scores[trlm_ba_best_idx]\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def calculate_model_scores(fo_model, fo_tokenizer, ba_model, ba_tokenizer,\n",
    "                          article_sentences, highlight, device='cpu'):\n",
    "    \"\"\"\n",
    "    A simplified implementation that sticks to the core functionality.\n",
    "    \n",
    "    Args:\n",
    "        fo_model: Forward language model\n",
    "        fo_tokenizer: Tokenizer for forward model\n",
    "        ba_model: Backward language model (trained on reversed tokens)\n",
    "        ba_tokenizer: Tokenizer for backward model\n",
    "        article_sentences: List of candidate sentences from the article\n",
    "        highlight: The highlight/summary to match with article sentences\n",
    "        device: Device to run models on ('cpu' or 'cuda')\n",
    "    \"\"\"\n",
    "    # Initialize scores\n",
    "    fw_scores = []\n",
    "    trlm_fo_scores = []\n",
    "    trlm_ba_scores = []\n",
    "    \n",
    "    # Ensure padding token exists for both tokenizers\n",
    "    if fo_tokenizer.pad_token is None:\n",
    "        fo_tokenizer.pad_token = fo_tokenizer.eos_token\n",
    "    if ba_tokenizer.pad_token is None:\n",
    "        ba_tokenizer.pad_token = ba_tokenizer.eos_token\n",
    "    \n",
    "    # Process each sentence individually (no batching)\n",
    "    for sentence in article_sentences:\n",
    "        # 1. FORWARD MODEL SCORING (query → response)\n",
    "        try:\n",
    "            # Create prompt\n",
    "            prompt = f\"{sentence} is summarized by: {highlight}\"\n",
    "            \n",
    "            # Tokenize\n",
    "            inputs = fo_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "            \n",
    "            # Calculate perplexity\n",
    "            with torch.no_grad():\n",
    "                outputs = fo_model(**inputs)\n",
    "                \n",
    "                # Get logits\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                # Calculate loss directly\n",
    "                labels = inputs.input_ids.clone()\n",
    "                \n",
    "                # Shift for causal language modeling\n",
    "                shift_logits = logits[:, :-1, :]\n",
    "                shift_labels = labels[:, 1:]\n",
    "                \n",
    "                # Simple loss calculation\n",
    "                loss_fn = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "                loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), \n",
    "                              shift_labels.view(-1))\n",
    "                \n",
    "                # Normalize by sequence length\n",
    "                seq_length = shift_labels.numel()\n",
    "                fw_scores.append(loss.item() / seq_length)\n",
    "        except Exception as e:\n",
    "            print(f\"Forward scoring error: {e}\")\n",
    "            fw_scores.append(float('inf'))\n",
    "        \n",
    "        # 2. TRLM-FO SCORING (response → query using forward model)\n",
    "        try:\n",
    "            # Create prompt\n",
    "            prompt = f\"{highlight} is a summary of: {sentence}\"\n",
    "            \n",
    "            # Tokenize\n",
    "            inputs = fo_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "            \n",
    "            # Calculate perplexity\n",
    "            with torch.no_grad():\n",
    "                outputs = fo_model(**inputs)\n",
    "                \n",
    "                # Get logits\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                # Calculate loss directly\n",
    "                labels = inputs.input_ids.clone()\n",
    "                \n",
    "                # Shift for causal language modeling\n",
    "                shift_logits = logits[:, :-1, :]\n",
    "                shift_labels = labels[:, 1:]\n",
    "                \n",
    "                # Simple loss calculation\n",
    "                loss_fn = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "                loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), \n",
    "                              shift_labels.view(-1))\n",
    "                \n",
    "                # Normalize by sequence length\n",
    "                seq_length = shift_labels.numel()\n",
    "                trlm_fo_scores.append(loss.item() / seq_length)\n",
    "        except Exception as e:\n",
    "            print(f\"TRLM-Fo scoring error: {e}\")\n",
    "            trlm_fo_scores.append(float('inf'))\n",
    "        \n",
    "        # 3. TRLM-BA SCORING (backward model)\n",
    "        try:\n",
    "            # For backward model, simply reverse the tokens\n",
    "            prompt = f\"{sentence} is summarized by: {highlight}\"\n",
    "            tokens = ba_tokenizer.encode(prompt, add_special_tokens=True)\n",
    "            reversed_tokens = tokens[::-1]\n",
    "            \n",
    "            # Convert to tensor\n",
    "            input_ids = torch.tensor([reversed_tokens]).to(device)\n",
    "            \n",
    "            # Calculate perplexity\n",
    "            with torch.no_grad():\n",
    "                outputs = ba_model(input_ids)\n",
    "                \n",
    "                # Get logits\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                # Calculate loss directly\n",
    "                labels = input_ids.clone()\n",
    "                \n",
    "                # Shift for causal language modeling\n",
    "                shift_logits = logits[:, :-1, :]\n",
    "                shift_labels = labels[:, 1:]\n",
    "                \n",
    "                # Simple loss calculation\n",
    "                loss_fn = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "                loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), \n",
    "                              shift_labels.view(-1))\n",
    "                \n",
    "                # Normalize by sequence length\n",
    "                seq_length = shift_labels.numel()\n",
    "                trlm_ba_scores.append(loss.item() / seq_length)\n",
    "        except Exception as e:\n",
    "            print(f\"TRLM-Ba scoring error: {e}\")\n",
    "            trlm_ba_scores.append(float('inf'))\n",
    "    \n",
    "    # Find best match for each approach\n",
    "    fw_best_idx = np.argmin(fw_scores) if fw_scores else -1\n",
    "    trlm_fo_best_idx = np.argmin(trlm_fo_scores) if trlm_fo_scores else -1\n",
    "    trlm_ba_best_idx = np.argmin(trlm_ba_scores) if trlm_ba_scores else -1\n",
    "    \n",
    "    # Print debug information\n",
    "    print(\"\\nDEBUG INFO:\")\n",
    "    if fw_scores:\n",
    "        print(f\"Forward scores (min: {min(fw_scores):.4f}, idx: {fw_best_idx})\")\n",
    "    if trlm_fo_scores:\n",
    "        print(f\"TRLM-Fo scores (min: {min(trlm_fo_scores):.4f}, idx: {trlm_fo_best_idx})\")\n",
    "    if trlm_ba_scores:\n",
    "        print(f\"TRLM-Ba scores (min: {min(trlm_ba_scores):.4f}, idx: {trlm_ba_best_idx})\")\n",
    "    \n",
    "    # Return results\n",
    "    return {\n",
    "        'forward': {\n",
    "            'scores': fw_scores,\n",
    "            'best_sentence': article_sentences[fw_best_idx] if fw_best_idx >= 0 else None,\n",
    "            'best_score': fw_scores[fw_best_idx] if fw_best_idx >= 0 else float('inf')\n",
    "        },\n",
    "        'trlm_fo': {\n",
    "            'scores': trlm_fo_scores,\n",
    "            'best_sentence': article_sentences[trlm_fo_best_idx] if trlm_fo_best_idx >= 0 else None,\n",
    "            'best_score': trlm_fo_scores[trlm_fo_best_idx] if trlm_fo_best_idx >= 0 else float('inf')\n",
    "        },\n",
    "        'trlm_ba': {\n",
    "            'scores': trlm_ba_scores,\n",
    "            'best_sentence': article_sentences[trlm_ba_best_idx] if trlm_ba_best_idx >= 0 else None,\n",
    "            'best_score': trlm_ba_scores[trlm_ba_best_idx] if trlm_ba_best_idx >= 0 else float('inf')\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_evaluation(num_samples=10):\n",
    "    # Setup part remains the same\n",
    "    if t.cuda.is_available(): \n",
    "        device = 'cuda' \n",
    "    elif t.backends.mps.is_available():\n",
    "        device = 'mps'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    fo_model, fo_tokenizer, ba_model, ba_tokenizer = load_models(device)\n",
    "    dataset = load_cnn_dataset(num_samples)\n",
    "    \n",
    "    print(f\"Dataset type: {type(dataset)}\")\n",
    "    print(f\"Dataset keys: {dataset.keys()}\")\n",
    "    \n",
    "    # Results list to store all evaluation results\n",
    "    results = []\n",
    "    \n",
    "    # Process each example\n",
    "    num_examples = len(dataset['article']) if 'article' in dataset else 0\n",
    "    print(f\"Processing {num_examples} examples\")\n",
    "    \n",
    "    for idx in tqdm(range(num_examples)):\n",
    "        try:\n",
    "            # Prepare the example\n",
    "            example = {\n",
    "                'article': dataset['article'][idx],\n",
    "                'highlights': dataset['highlights'][idx],\n",
    "                'id': dataset['id'][idx] if 'id' in dataset else f\"example_{idx}\"\n",
    "            }\n",
    "            \n",
    "            # Print info\n",
    "            print(f\"\\n--- Example {idx+1} ---\")\n",
    "            print(f\"Article (truncated): {example['article'][:150]}...\")\n",
    "            print(f\"Highlight: {example['highlights']}\")\n",
    "            \n",
    "            # Preprocess\n",
    "            processed = preprocess_text(example)\n",
    "            article_sentences = processed['article_sentences']\n",
    "            \n",
    "            # Skip if article is too short\n",
    "            if len(article_sentences) < 3:\n",
    "                print(\"Article too short, skipping\")\n",
    "                continue\n",
    "                \n",
    "            # Evaluate on first highlight sentence (if available)\n",
    "            if processed['highlight_sentences']:\n",
    "                highlight = processed['highlight_sentences'][0]\n",
    "                \n",
    "                # Get scores for all models\n",
    "                model_results = calculate_model_scores(\n",
    "                    fo_model, fo_tokenizer, ba_model, ba_tokenizer,\n",
    "                    article_sentences, highlight, device\n",
    "                )\n",
    "                \n",
    "                # Calculate metrics\n",
    "                fw_metrics = calculate_metrics(\n",
    "                    model_results['forward']['best_sentence'], \n",
    "                    processed['highlight_sentences']\n",
    "                )\n",
    "                \n",
    "                trlm_fo_metrics = calculate_metrics(\n",
    "                    model_results['trlm_fo']['best_sentence'], \n",
    "                    processed['highlight_sentences']\n",
    "                )\n",
    "                \n",
    "                trlm_ba_metrics = calculate_metrics(\n",
    "                    model_results['trlm_ba']['best_sentence'], \n",
    "                    processed['highlight_sentences']\n",
    "                )\n",
    "                \n",
    "                # Store results\n",
    "                result = {\n",
    "                    'example_id': example['id'],\n",
    "                    'highlight': highlight,\n",
    "                    'forward_sentence': model_results['forward']['best_sentence'],\n",
    "                    'trlm_fo_sentence': model_results['trlm_fo']['best_sentence'],\n",
    "                    'trlm_ba_sentence': model_results['trlm_ba']['best_sentence'],\n",
    "                    'forward_score': model_results['forward']['best_score'],\n",
    "                    'trlm_fo_score': model_results['trlm_fo']['best_score'],\n",
    "                    'trlm_ba_score': model_results['trlm_ba']['best_score']\n",
    "                }\n",
    "                \n",
    "                # Add all metrics\n",
    "                for metric, value in fw_metrics.items():\n",
    "                    result[f'forward_{metric}'] = value\n",
    "                \n",
    "                for metric, value in trlm_fo_metrics.items():\n",
    "                    result[f'trlm_fo_{metric}'] = value\n",
    "                    \n",
    "                for metric, value in trlm_ba_metrics.items():\n",
    "                    result[f'trlm_ba_{metric}'] = value\n",
    "                \n",
    "                results.append(result)\n",
    "                \n",
    "                # Print metrics\n",
    "                print(f\"Forward: {model_results['forward']['best_sentence']}\")\n",
    "                print(f\"TRLM-Fo: {model_results['trlm_fo']['best_sentence']}\")\n",
    "                print(f\"TRLM-Ba: {model_results['trlm_ba']['best_sentence']}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing example {idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_detailed_dataframe(results):\n",
    "    \"\"\"\n",
    "    Create a simplified DataFrame focusing only on the loss/score comparisons.\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Create a new DataFrame with multi-index columns\n",
    "    example_cols = ['example_id', 'highlight']\n",
    "    model_names = ['forward', 'trlm_fo', 'trlm_ba']\n",
    "    \n",
    "    # Create multi-index column list\n",
    "    columns = []\n",
    "    for col in example_cols:\n",
    "        columns.append((col, ''))\n",
    "    \n",
    "    # Add sentences and scores columns\n",
    "    for model in model_names:\n",
    "        # Add sentence column\n",
    "        columns.append((model, 'sentence'))\n",
    "        # Add score column\n",
    "        columns.append((model, 'score'))\n",
    "    \n",
    "    # Create multi-index DataFrame\n",
    "    multi_df = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Add example columns\n",
    "    for col in example_cols:\n",
    "        multi_df[(col, '')] = df[col]\n",
    "    \n",
    "    # Add sentence and score columns\n",
    "    for model in model_names:\n",
    "        # Add sentence\n",
    "        multi_df[(model, 'sentence')] = df[f'{model}_sentence']\n",
    "        # Add score\n",
    "        multi_df[(model, 'score')] = df[f'{model}_score']\n",
    "    \n",
    "    # Set column multi-index\n",
    "    multi_df.columns = pd.MultiIndex.from_tuples(columns)\n",
    "    \n",
    "    return multi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def create_summary_dataframe(results):\n",
    "    \"\"\"\n",
    "    Create a summary DataFrame with models as rows and metrics as columns.\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Convert to DataFrame first\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Models we're comparing\n",
    "    models = ['forward', 'trlm_fo', 'trlm_ba']\n",
    "    \n",
    "    # Metrics we want to summarize\n",
    "    metrics = [\n",
    "        'score',\n",
    "        'perplexity',\n",
    "        'tfidf_similarity',\n",
    "        'embedding_similarity', \n",
    "        'rouge1',\n",
    "        'rougeL',\n",
    "        'bleu'\n",
    "    ]\n",
    "    \n",
    "    # Create the summary DataFrame\n",
    "    summary_data = {}\n",
    "    \n",
    "    for metric in metrics:\n",
    "        for model in models:\n",
    "            col_name = f'{model}_{metric}'\n",
    "            if col_name in df.columns:\n",
    "                if metric not in summary_data:\n",
    "                    summary_data[metric] = {}\n",
    "                summary_data[metric][model] = df[col_name].mean()\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Dataset loaded successfully\n",
      "Example dataset item: {'article': 'LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won\\'t cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don\\'t plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don\\'t think I\\'ll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he\\'ll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I\\'ll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe\\'s earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say \\'kid star goes off the rails,\\'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films.  Watch I-Reporter give her review of Potter\\'s latest » . There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer\\'s \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he\\'s legally an adult: \"I just think I\\'m going to be more sort of fair game,\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.', 'highlights': \"Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\\nYoung actor says he has no plans to fritter his cash away .\\nRadcliffe's earnings from first five Potter films have been held in trust fund .\", 'id': '42c027e4ff9730fbb3de84c1af0d2c506e41c3e4'}\n",
      "Dataset type: <class 'dict'>\n",
      "Dataset keys: dict_keys(['article', 'highlights', 'id'])\n",
      "Processing 5 examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80a6a4fb62164e92bf06da88b9088319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 1 ---\n",
      "Article (truncated): LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monda...\n",
      "Highlight: Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\n",
      "Young actor says he has no plans to fritter his cash away .\n",
      "Radcliffe's earnings from first five Potter films have been held in trust fund .\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "\n",
      "DEBUG INFO:\n",
      "Forward scores (min: 127.5821, idx: 22)\n",
      "TRLM-Fo scores (min: 140.5469, idx: 21)\n",
      "TRLM-Ba scores (min: inf, idx: 0)\n",
      "Forward: Copyright 2007 Reuters.\n",
      "TRLM-Fo: E-mail to a friend .\n",
      "TRLM-Ba: LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him.\n",
      "\n",
      "--- Example 2 ---\n",
      "Article (truncated): Editor's note: In our Behind the Scenes series, CNN correspondents share their experiences in covering news and analyze the stories behind the events....\n",
      "Highlight: Mentally ill inmates in Miami are housed on the \"forgotten floor\"\n",
      "Judge Steven Leifman says most are there as a result of \"avoidable felonies\"\n",
      "While CNN tours facility, patient shouts: \"I am the son of the president\"\n",
      "Leifman says the system is unjust and he's fighting for change .\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "\n",
      "DEBUG INFO:\n",
      "Forward scores (min: 336.5616, idx: 21)\n",
      "TRLM-Fo scores (min: 342.7540, idx: 21)\n",
      "TRLM-Ba scores (min: inf, idx: 0)\n",
      "Forward: \"I am the son of the president.\n",
      "TRLM-Fo: \"I am the son of the president.\n",
      "TRLM-Ba: Editor's note: In our Behind the Scenes series, CNN correspondents share their experiences in covering news and analyze the stories behind the events.\n",
      "\n",
      "--- Example 3 ---\n",
      "Article (truncated): MINNEAPOLIS, Minnesota (CNN) -- Drivers who were on the Minneapolis bridge when it collapsed told harrowing tales of survival. \"The whole bridge from ...\n",
      "Highlight: NEW: \"I thought I was going to die,\" driver says .\n",
      "Man says pickup truck was folded in half; he just has cut on face .\n",
      "Driver: \"I probably had a 30-, 35-foot free fall\"\n",
      "Minnesota bridge collapsed during rush hour Wednesday .\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "\n",
      "DEBUG INFO:\n",
      "Forward scores (min: 108.1157, idx: 28)\n",
      "TRLM-Fo scores (min: 102.3764, idx: 39)\n",
      "TRLM-Ba scores (min: inf, idx: 0)\n",
      "Forward: It was just complete chaos.\"\n",
      "TRLM-Fo: \"It's really very surreal.\"\n",
      "TRLM-Ba: MINNEAPOLIS, Minnesota (CNN) -- Drivers who were on the Minneapolis bridge when it collapsed told harrowing tales of survival.\n",
      "\n",
      "--- Example 4 ---\n",
      "Article (truncated): WASHINGTON (CNN) -- Doctors removed five small polyps from President Bush's colon on Saturday, and \"none appeared worrisome,\" a White House spokesman ...\n",
      "Highlight: Five small polyps found during procedure; \"none worrisome,\" spokesman says .\n",
      "President reclaims powers transferred to vice president .\n",
      "Bush undergoes routine colonoscopy at Camp David .\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "\n",
      "DEBUG INFO:\n",
      "Forward scores (min: 152.0419, idx: 17)\n",
      "TRLM-Fo scores (min: 152.5036, idx: 23)\n",
      "TRLM-Ba scores (min: inf, idx: 0)\n",
      "Forward: Small polyps may be removed during the procedure.\n",
      "TRLM-Fo: E-mail to a friend .\n",
      "TRLM-Ba: WASHINGTON (CNN) -- Doctors removed five small polyps from President Bush's colon on Saturday, and \"none appeared worrisome,\" a White House spokesman said.\n",
      "\n",
      "--- Example 5 ---\n",
      "Article (truncated): (CNN)  -- The National Football League has indefinitely suspended Atlanta Falcons quarterback Michael Vick without pay, officials with the league said...\n",
      "Highlight: NEW: NFL chief, Atlanta Falcons owner critical of Michael Vick's conduct .\n",
      "NFL suspends Falcons quarterback indefinitely without pay .\n",
      "Vick admits funding dogfighting operation but says he did not gamble .\n",
      "Vick due in federal court Monday; future in NFL remains uncertain .\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n",
      "TRLM-Ba scoring error: Could not infer dtype of NoneType\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m   \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Create detailed DataFrame\u001b[39;00m\n\u001b[1;32m      4\u001b[0m detailed_df \u001b[38;5;241m=\u001b[39m create_detailed_dataframe(results)\n",
      "Cell \u001b[0;32mIn[53], line 52\u001b[0m, in \u001b[0;36mrun_evaluation\u001b[0;34m(num_samples)\u001b[0m\n\u001b[1;32m     49\u001b[0m highlight \u001b[38;5;241m=\u001b[39m processed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhighlight_sentences\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Get scores for all models\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m model_results \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_model_scores\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfo_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfo_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mba_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mba_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43marticle_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhighlight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Calculate metrics\u001b[39;00m\n\u001b[1;32m     58\u001b[0m fw_metrics \u001b[38;5;241m=\u001b[39m calculate_metrics(\n\u001b[1;32m     59\u001b[0m     model_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_sentence\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     60\u001b[0m     processed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhighlight_sentences\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     61\u001b[0m )\n",
      "Cell \u001b[0;32mIn[52], line 46\u001b[0m, in \u001b[0;36mcalculate_model_scores\u001b[0;34m(fo_model, fo_tokenizer, ba_model, ba_tokenizer, article_sentences, highlight, device)\u001b[0m\n\u001b[1;32m     44\u001b[0m     next_token_id \u001b[38;5;241m=\u001b[39m labels[\u001b[38;5;241m0\u001b[39m, i]\n\u001b[1;32m     45\u001b[0m     log_prob \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog_softmax(next_token_logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[next_token_id]\n\u001b[0;32m---> 46\u001b[0m     log_probs\u001b[38;5;241m.\u001b[39mappend(\u001b[43mlog_prob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Total log probability (sum because log probabilities add)\u001b[39;00m\n\u001b[1;32m     49\u001b[0m total_log_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(log_probs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = run_evaluation(num_samples=5)   \n",
    " \n",
    "# Create detailed DataFrame\n",
    "detailed_df = create_detailed_dataframe(results)\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary_df = create_summary_dataframe(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>example_id</th>\n",
       "      <th>highlight</th>\n",
       "      <th colspan=\"2\" halign=\"left\">forward</th>\n",
       "      <th colspan=\"2\" halign=\"left\">trlm_fo</th>\n",
       "      <th colspan=\"2\" halign=\"left\">trlm_ba</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>score</th>\n",
       "      <th>sentence</th>\n",
       "      <th>score</th>\n",
       "      <th>sentence</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42c027e4ff9730fbb3de84c1af0d2c506e41c3e4</td>\n",
       "      <td>Harry Potter star Daniel Radcliffe gets £20M f...</td>\n",
       "      <td>LONDON, England (Reuters) -- Harry Potter star...</td>\n",
       "      <td>3.312327</td>\n",
       "      <td>LONDON, England (Reuters) -- Harry Potter star...</td>\n",
       "      <td>3.438889</td>\n",
       "      <td>LONDON, England (Reuters) -- Harry Potter star...</td>\n",
       "      <td>3.671095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ee8871b15c50d0db17b0179a6d2beab35065f1e9</td>\n",
       "      <td>Mentally ill inmates in Miami are housed on th...</td>\n",
       "      <td>Starting in 2008, many inmates who would other...</td>\n",
       "      <td>3.835283</td>\n",
       "      <td>Leifman says in 1955 there were more than half...</td>\n",
       "      <td>3.854144</td>\n",
       "      <td>Leifman says in 1955 there were more than half...</td>\n",
       "      <td>3.835589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>06352019a19ae31e527f37f7571c6dd7f0c5da37</td>\n",
       "      <td>NEW: \"I thought I was going to die,\" driver sa...</td>\n",
       "      <td>\"I knew the deck was going down, there was no ...</td>\n",
       "      <td>3.347004</td>\n",
       "      <td>\"I knew the deck was going down, there was no ...</td>\n",
       "      <td>3.083808</td>\n",
       "      <td>\"I knew the deck was going down, there was no ...</td>\n",
       "      <td>3.357843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24521a2abb2e1f5e34e6824e0f9e56904a2b0e88</td>\n",
       "      <td>Five small polyps found during procedure; \"non...</td>\n",
       "      <td>A colonoscopy is the most sensitive test for c...</td>\n",
       "      <td>4.082535</td>\n",
       "      <td>The procedure was supervised by Dr. Richard Tu...</td>\n",
       "      <td>3.942272</td>\n",
       "      <td>The procedure was supervised by Dr. Richard Tu...</td>\n",
       "      <td>3.843853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7fe70cc8b12fab2d0a258fababf7d9c6b5e1262a</td>\n",
       "      <td>NEW: NFL chief, Atlanta Falcons owner critical...</td>\n",
       "      <td>\"Such costs may include, but are not limited t...</td>\n",
       "      <td>3.635967</td>\n",
       "      <td>The charge is punishable by up to five years i...</td>\n",
       "      <td>3.538003</td>\n",
       "      <td>The charge is punishable by up to five years i...</td>\n",
       "      <td>3.589679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 example_id  \\\n",
       "                                              \n",
       "0  42c027e4ff9730fbb3de84c1af0d2c506e41c3e4   \n",
       "1  ee8871b15c50d0db17b0179a6d2beab35065f1e9   \n",
       "2  06352019a19ae31e527f37f7571c6dd7f0c5da37   \n",
       "3  24521a2abb2e1f5e34e6824e0f9e56904a2b0e88   \n",
       "4  7fe70cc8b12fab2d0a258fababf7d9c6b5e1262a   \n",
       "\n",
       "                                           highlight  \\\n",
       "                                                       \n",
       "0  Harry Potter star Daniel Radcliffe gets £20M f...   \n",
       "1  Mentally ill inmates in Miami are housed on th...   \n",
       "2  NEW: \"I thought I was going to die,\" driver sa...   \n",
       "3  Five small polyps found during procedure; \"non...   \n",
       "4  NEW: NFL chief, Atlanta Falcons owner critical...   \n",
       "\n",
       "                                             forward            \\\n",
       "                                            sentence     score   \n",
       "0  LONDON, England (Reuters) -- Harry Potter star...  3.312327   \n",
       "1  Starting in 2008, many inmates who would other...  3.835283   \n",
       "2  \"I knew the deck was going down, there was no ...  3.347004   \n",
       "3  A colonoscopy is the most sensitive test for c...  4.082535   \n",
       "4  \"Such costs may include, but are not limited t...  3.635967   \n",
       "\n",
       "                                             trlm_fo            \\\n",
       "                                            sentence     score   \n",
       "0  LONDON, England (Reuters) -- Harry Potter star...  3.438889   \n",
       "1  Leifman says in 1955 there were more than half...  3.854144   \n",
       "2  \"I knew the deck was going down, there was no ...  3.083808   \n",
       "3  The procedure was supervised by Dr. Richard Tu...  3.942272   \n",
       "4  The charge is punishable by up to five years i...  3.538003   \n",
       "\n",
       "                                             trlm_ba            \n",
       "                                            sentence     score  \n",
       "0  LONDON, England (Reuters) -- Harry Potter star...  3.671095  \n",
       "1  Leifman says in 1955 there were more than half...  3.835589  \n",
       "2  \"I knew the deck was going down, there was no ...  3.357843  \n",
       "3  The procedure was supervised by Dr. Richard Tu...  3.843853  \n",
       "4  The charge is punishable by up to five years i...  3.589679  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>tfidf_similarity</th>\n",
       "      <th>embedding_similarity</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>forward</th>\n",
       "      <td>3.642623</td>\n",
       "      <td>0.231524</td>\n",
       "      <td>0.529002</td>\n",
       "      <td>0.255832</td>\n",
       "      <td>0.232182</td>\n",
       "      <td>0.107367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trlm_fo</th>\n",
       "      <td>3.571423</td>\n",
       "      <td>0.214766</td>\n",
       "      <td>0.511483</td>\n",
       "      <td>0.231062</td>\n",
       "      <td>0.216945</td>\n",
       "      <td>0.094781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trlm_ba</th>\n",
       "      <td>3.659612</td>\n",
       "      <td>0.214766</td>\n",
       "      <td>0.511483</td>\n",
       "      <td>0.231062</td>\n",
       "      <td>0.216945</td>\n",
       "      <td>0.094781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            score  tfidf_similarity  embedding_similarity    rouge1    rougeL  \\\n",
       "forward  3.642623          0.231524              0.529002  0.255832  0.232182   \n",
       "trlm_fo  3.571423          0.214766              0.511483  0.231062  0.216945   \n",
       "trlm_ba  3.659612          0.214766              0.511483  0.231062  0.216945   \n",
       "\n",
       "             bleu  \n",
       "forward  0.107367  \n",
       "trlm_fo  0.094781  \n",
       "trlm_ba  0.094781  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
