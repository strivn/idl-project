{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune with full scale dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from peft import (LoraConfig, PeftModel, get_peft_model,\n",
    "                  prepare_model_for_kbit_training)\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (AutoTokenizer, BitsAndBytesConfig,\n",
    "                          DataCollatorForLanguageModeling, GPTNeoXForCausalLM,\n",
    "                          Trainer, TrainingArguments, TrainerCallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Working in '/ocean/projects/cis250068p/iwiryadi/idl-project'\n",
      "✓ Directory contains 'idl-project'\n"
     ]
    }
   ],
   "source": [
    "current_dir = os.path.abspath(os.getcwd())\n",
    "\n",
    "# Check if 'idl-project' is in the path\n",
    "if 'idl-project' not in current_dir:\n",
    "    raise Exception(\"Current directory '{current_dir}' is not within 'idl-project'\")\n",
    "\n",
    "print(f\"✓ Working in '{current_dir}'\")\n",
    "print(f\"✓ Directory contains 'idl-project'\")\n",
    "\n",
    "OUTPUT_DIR = \"output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.finetune_lora_config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lora-tuning-6'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RUN_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Append paths for the src folder\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.getcwd(), 'idl-project')))\n",
    "\n",
    "# Additional imports \n",
    "from src.model import load_fo_model\n",
    "from src.data import load_flan_dataset, load_summarization_datasets\n",
    "from src.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Tesla V100-SXM2-32GB\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test prompts\n",
    "test_prompts = [\n",
    "    \"Who is Barack Obama?\",\n",
    "    \"What is Carnegie Mellon University?\",\n",
    "    \"Classify this restaurant review sentiment: 'The food was absolutely delicious but the service was extremely slow and the waiter seemed uninterested in helping us.'\",\n",
    "    \"Compare and contrast Carnegie Mellon University's Computer Science and Information Systems programs in terms of research focus and career outcomes.\",\n",
    "    \"Summarize in one sentence: Dr. Sarah Chen, lead scientist on the mission, called it 'the most significant discovery in the history of space exploration.' The finding suggests that Mars once had a much more hospitable environment with liquid water and possibly a thicker atmosphere. The agency plans to send a sample return mission within the next five years to bring these fossils back to Earth for more detailed analysis. This discovery has profound implications for our understanding of how life might develop throughout the universe.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "267cd9ac1b364f2dbf8cdb4fd81cbf38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing niv2_zsopt_data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be9261f8cd524ee3a30aaf03e7d17110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5644e08c84164749aef4906ca8b58479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully: niv2_zsopt_data\n",
      "Original niv2_zsopt_data size: 5030900\n",
      "Found summarization examples in niv2_zsopt_data\n",
      "Processing cot_zsopt_data...\n",
      "Dataset loaded successfully: cot_zsopt_data\n",
      "Original cot_zsopt_data size: 95570\n",
      "Found summarization examples in cot_zsopt_data\n",
      "Processing dialog_zsopt_data...\n",
      "Dataset loaded successfully: dialog_zsopt_data\n",
      "Original dialog_zsopt_data size: 2715160\n",
      "Found summarization examples in dialog_zsopt_data\n",
      "Combined dataset with 1688035 summarization examples\n"
     ]
    }
   ],
   "source": [
    "# dataset = load_summarization_datasets(subset_names=['niv2_zsopt_data', 'cot_zsopt_data', 'dialog_zsopt_data'])\n",
    "dataset = load_summarization_datasets(\n",
    "    subset_names=FLAN_SUBSET, subset_frac=SUBSET_FRAC, p=TASK_DIVERSITY_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Teacher:Given a phrase describing the relationship between two words, extract the words and the lexical relationship between them. The relation has to be of the type 'MemberOf', 'MadeOf', 'Synonym', 'Entails', 'HasA', 'HasProperty', 'PartOf', 'Antonym' or 'IsA'. The output should have the format: word1 relation word2.\n",
      "Teacher: Now, understand the problem? Solve this instance: pass can be used as the opposite of meet\n",
      "Student:\n",
      "pass Antonym meet\n",
      "================================================================================\n",
      "1\n",
      "Detailed Instructions: In this task, you're given reviews from Amazon's products. Your task is to generate the Summary of the review.\n",
      "Q: This charger destroyed my phone.\n",
      "A:\n",
      "Isn’t compatible with iPhone although they say it is. Fried my phone\n",
      "================================================================================\n",
      "2\n",
      "In this task, You are given an amazon food product review and its summary. Your task is to Generate \"True\" if given review and its summary match, otherwise generate \"False\".\n",
      "Q: These chips taste average and they are insanely expensive compare to the big bags of chips from safeway or costco, for $2, you can get a big bag with 10 times the amount of chips compare to these 0.8oz bag chips, so for $5 in safeway, you can get $30 worth of popchips, these bags are so tiny, it's like serve size for fat people trying to lose weigh or something, for me, i never gain weigh, i rather go buy big bag of chips, these are expensive ripoff chips, will never buy again. Maybe i think the price is high because i bought it from amazon.com, maybe elsewhere is cheaper, but chips as junk food, shouldn't be that expensive compare to local stores. \n",
      " Summary: TASTE AVERAGE, INSANELY EXPENSIVE\n",
      "A: \n",
      "True\n",
      "================================================================================\n",
      "3\n",
      "Given the task definition and input, reply with output. In this task, you are given a text of article and corresponding title of an article. Your task is to generate label \"yes\" if headline (title) is right for article, otherwise generate \"no\".\n",
      "\n",
      "Article: an aeroflot boeing-### jet crashed sunday on the outskirts of perm in russia 's ural mountains killing all ## passengers and crew on board . Title: suu kyi meets doctor amid health worries\n",
      "\n",
      "no\n",
      "================================================================================\n",
      "4\n",
      "You will be given a definition of a task first, then some input of the task.\n",
      "Given a phrase describing the relationship between two words, extract the words and the lexical relationship between them. The relation has to be of the type 'MemberOf', 'MadeOf', 'Synonym', 'Entails', 'HasA', 'HasProperty', 'PartOf', 'Antonym' or 'IsA'. The output should have the format: word1 relation word2.\n",
      "\n",
      "order is a kind of state\n",
      "Output:\n",
      "order IsA state\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(i)\n",
    "    print(dataset[i]['inputs'])\n",
    "    print(dataset[i]['targets'])\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset[\u001b[43mi\u001b[49m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "dataset[i]['inputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset[\u001b[43mi\u001b[49m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtargets\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "dataset[i]['targets']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  LORA\n",
    "\n",
    "### Configure BitsAndBytes for 4-bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure BitsAndBytes for 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",           # Use 4-bit NormalFloat quantization\n",
    "    bnb_4bit_use_double_quant=True,      # Use double quantization for additional memory savings\n",
    "    bnb_4bit_compute_dtype=torch.float32  # Compute in float32 (can also use torch.bfloat16 if available)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID       = \"EleutherAI/pythia-160m-deduped\"\n",
    "MODEL_REVISION = \"step143000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    revision=MODEL_REVISION,\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "\n",
    "# Ensure the tokenizer has padding token set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the model with quantization\n",
    "model = GPTNeoXForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    revision=MODEL_REVISION,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"  # Automatically distribute layers across available GPUs\n",
    ")\n",
    "\n",
    "# Prepare the model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LoRA configuration\n",
    "# For Pythia models, the target module is \"query_key_value\" for attention layers\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_RANK,                    # Rank dimension\n",
    "    lora_alpha=LORA_ALPHA,          # LoRA scaling factor\n",
    "    target_modules=TARGET_MODULES,  # Target specific attention modules\n",
    "    lora_dropout=LORA_DROPOUT,      # Dropout probability for LoRA layers\n",
    "    bias=\"none\",            # Don't apply LoRA to bias terms\n",
    "    task_type=\"CAUSAL_LM\"   # Task type for causal language modeling\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "peft_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 294912 || all params: 120150528 || trainable%: 0.25\n"
     ]
    }
   ],
   "source": [
    "# Print trainable parameters information\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(peft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Dataset Formatting and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0300ec34c634056b6c8d18c8aa16a6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/151009 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to format the dataset for causal language modeling\n",
    "def format_for_clm(examples):\n",
    "    # Format as: \"Instruction: {instruction} Input: {input} Output: {output}\"\n",
    "    # Adjust this format based on your specific dataset structure\n",
    "    if 'inputs' in examples and 'targets' in examples:\n",
    "        texts = [\n",
    "            f\"{inp}\\n{target}{tokenizer.eos_token}\"\n",
    "            for inp, target in zip(examples['inputs'], examples['targets'])\n",
    "        ]\n",
    "    else:\n",
    "        # Fallback for other dataset structures\n",
    "        texts = examples['text'] if 'text' in examples else []\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Apply formatting \n",
    "if isinstance(dataset, Dataset):\n",
    "    # For non-streaming datasets\n",
    "    if 'inputs' in dataset.column_names and 'targets' in dataset.column_names:\n",
    "        dataset = dataset.map(format_for_clm, batched=True, num_proc=4)\n",
    "else:\n",
    "    # For streaming datasets, we need to format each example as it comes\n",
    "    dataset = dataset.map(lambda example: {\n",
    "        'text': f\"Instruction: {example['inputs']}\\nOutput: {example['targets']}\" \n",
    "        if 'inputs' in example and 'targets' in example \n",
    "        else example.get('text', '')\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58befaf261f04fa8906b0be84288cf4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/151009 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Tokenize function for the dataset\n",
    "def tokenize_function(example):\n",
    "    # Handle single examples for streaming datasets\n",
    "    text = example[\"text\"] if \"text\" in example else \"\"\n",
    "    \n",
    "    # Tokenize with padding and truncation\n",
    "    outputs = tokenizer(\n",
    "        text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=1024,  # Adjust based on your needs and GPU memory\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Remove the batch dimension for single examples\n",
    "    for key in outputs:\n",
    "        if isinstance(outputs[key], torch.Tensor) and outputs[key].ndim > 1:\n",
    "            outputs[key] = outputs[key].squeeze(0)\n",
    "    \n",
    "    # Set labels equal to input_ids for causal language modeling\n",
    "    outputs[\"labels\"] = outputs[\"input_ids\"].clone()\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "# Apply tokenization to the dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = tokenized_dataset.train_test_split(test_size=TEST_SIZE)\n",
    "train_dataset = split_dataset['train']\n",
    "val_dataset   = split_dataset['test']  # Note: called 'test' by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Since we're using streaming datasets, convert to list for training\n",
    "# # This is needed because Trainer expects a non-streaming dataset\n",
    "# # We'll create a buffer of examples for training\n",
    "# buffer_size = 25000  # Adjust based on your memory constraints\n",
    "# tokenized_examples = []\n",
    "# for example in tqdm(tokenized_dataset, total=buffer_size):\n",
    "#     tokenized_examples.append(example)\n",
    "#     if len(tokenized_examples) >= buffer_size:\n",
    "#         break\n",
    "\n",
    "# print(f\"Collected {len(tokenized_examples)} examples for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert to Dataset object for training\n",
    "# from datasets import Dataset as HFDataset\n",
    "# train_dataset = HFDataset.from_list(tokenized_examples)\n",
    "\n",
    "# print(f\"Training dataset created with columns: {train_dataset.column_names}\")\n",
    "# print(f\"Number of examples: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation test callback\n",
    "class GenerationTestCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Callback to generate text samples at evaluation steps.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, test_prompts, max_length=100, do_sample=True, \n",
    "                 num_beams=2, temperature=0.1, device=\"cuda\"):\n",
    "        \"\"\"\n",
    "        Initialize with tokenizer and test prompts.\n",
    "        \"\"\"\n",
    "        self.tokenizer    = tokenizer\n",
    "        self.test_prompts = test_prompts\n",
    "        self.device       = device\n",
    "        self.max_length   = max_length\n",
    "        self.do_sample    = do_sample\n",
    "        self.num_beams    = num_beams\n",
    "        self.temperature  = temperature\n",
    "            \n",
    "            \n",
    "    def on_evaluate(self, args, state, control, model, **kwargs):\n",
    "        \"\"\"\n",
    "        Run after each evaluation to generate two types of samples:\n",
    "        1. Free-form completion with sampling\n",
    "        2. Greedy decoding for deterministic output\n",
    "        \"\"\"\n",
    "        print(\"GenerationTestCallback\")\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"Generating samples at step {state.global_step}:\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for prompt in self.test_prompts:\n",
    "                inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "                \n",
    "                # 1. Generate with sampling\n",
    "                sample_output_ids = model.generate(\n",
    "                    inputs[\"input_ids\"],\n",
    "                    attention_mask    = inputs[\"attention_mask\"],\n",
    "                    max_length        = self.max_length,\n",
    "                    do_sample         = True,\n",
    "                    temperature       = self.temperature,\n",
    "                    pad_token_id      = self.tokenizer.eos_token_id,\n",
    "                    # eos_token_id      = self.tokenizer.eos_token_id\n",
    "                )\n",
    "                \n",
    "                # 2. Generate with greedy decoding (deterministic) and no sampling\n",
    "                greedy_output_ids = model.generate(\n",
    "                    inputs[\"input_ids\"],\n",
    "                    attention_mask    = inputs[\"attention_mask\"],\n",
    "                    max_length        = self.max_length,\n",
    "                    do_sample         = True,\n",
    "                    pad_token_id      = self.tokenizer.eos_token_id,\n",
    "                    # pad_token_id      = self.tokenizer.pad_token_id,\n",
    "                    # eos_token_id      = self.tokenizer.eos_token_id\n",
    "                )\n",
    "                \n",
    "                # Decode both outputs\n",
    "                sampled_text = self.tokenizer.decode(sample_output_ids[0], skip_special_tokens=True)\n",
    "                greedy_text  = self.tokenizer.decode(greedy_output_ids[0], skip_special_tokens=True)\n",
    "                \n",
    "                # Print the results\n",
    "                print(f\"\\nPrompt: {prompt}\")\n",
    "                print(f\"Greedy: {greedy_text}\")\n",
    "                print(\"-\"*50)\n",
    "                \n",
    "                # Log to W&B if you're using it\n",
    "                if args.report_to == \"wandb\":\n",
    "                    import wandb\n",
    "                    wandb.log({\n",
    "                        f\"generation/{prompt}/sampled\": wandb.Html(\n",
    "                            f\"<b>Step {state.global_step}</b><br>\"\n",
    "                            f\"<p><b>Prompt:</b> {prompt}</p>\"\n",
    "                            f\"<p><b>Sampled:</b> {sampled_text}</p>\"\n",
    "                        ),\n",
    "                        f\"generation/{prompt}/greedy\": wandb.Html(\n",
    "                            f\"<b>Step {state.global_step}</b><br>\"\n",
    "                            f\"<p><b>Prompt:</b> {prompt}</p>\"\n",
    "                            f\"<p><b>Greedy:</b> {greedy_text}</p>\"\n",
    "                        )\n",
    "                    }, step=state.global_step)\n",
    "        \n",
    "        return control\n",
    "\n",
    "# Create generation callback\n",
    "generation_callback = GenerationTestCallback(\n",
    "    tokenizer     = tokenizer,\n",
    "    test_prompts  = test_prompts,\n",
    "    max_length    = 250,\n",
    "    num_beams     = 3,\n",
    "    temperature   = 0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miwiryadi\u001b[0m (\u001b[33midl-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/ocean/projects/cis250068p/iwiryadi/idl-project/wandb/run-20250415_145044-rp9h9293</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/11785_finetuning/ivan-testing-team/runs/rp9h9293' target=\"_blank\">lora-tuning-5</a></strong> to <a href='https://wandb.ai/11785_finetuning/ivan-testing-team' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/11785_finetuning/ivan-testing-team' target=\"_blank\">https://wandb.ai/11785_finetuning/ivan-testing-team</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/11785_finetuning/ivan-testing-team/runs/rp9h9293' target=\"_blank\">https://wandb.ai/11785_finetuning/ivan-testing-team/runs/rp9h9293</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(entity=\"11785_finetuning\", project='ivan-testing-team', name=RUN_NAME, reinit=True)\n",
    "wandb.save(\"notebooks/finetune_lora_config.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/iwiryadi/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/jet/home/iwiryadi/.local/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n",
      "/jet/home/iwiryadi/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/jet/home/iwiryadi/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/jet/home/iwiryadi/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/jet/home/iwiryadi/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/jet/home/iwiryadi/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/jet/home/iwiryadi/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/jet/home/iwiryadi/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/jet/home/iwiryadi/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/jet/home/iwiryadi/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/jet/home/iwiryadi/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/jet/home/iwiryadi/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/jet/home/iwiryadi/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='3360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  10/3360 00:35 < 4:09:09, 0.22 it/s, Epoch 0.01/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 55\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2556\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2549\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2550\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2551\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2553\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2554\u001b[0m )\n\u001b[1;32m   2555\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2556\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2559\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2560\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2561\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2562\u001b[0m ):\n\u001b[1;32m   2563\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2564\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:3764\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   3762\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 3764\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3766\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/accelerator.py:2450\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2449\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2450\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2451\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[1;32m   2452\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set format to PyTorch\n",
    "train_dataset.set_format(type=\"torch\")\n",
    "val_dataset.set_format(type=\"torch\")\n",
    "\n",
    "# Create training arguments with parameters\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size  = BATCH_SIZE,\n",
    "    per_device_eval_batch_size   = BATCH_SIZE,\n",
    "    gradient_accumulation_steps  = GRADIENT_ACCUMULATION_STEPS,\n",
    "    warmup_ratio                 = WARMUP_RATIO,\n",
    "    num_train_epochs             = NUM_EPOCHS,\n",
    "    learning_rate                = LEARNING_RATE,\n",
    "    lr_scheduler_type            = LR_SCHEDULER_TYPE,\n",
    "    fp16                         = FP16,\n",
    "    logging_steps                = LOGGING_STEPS,\n",
    "    save_steps                   = SAVE_STEPS,\n",
    "    eval_strategy                = \"steps\",\n",
    "    eval_steps                   = EVAL_STEPS,\n",
    "    output_dir                   = OUTPUT_DIR,\n",
    "    optim                        = \"paged_adamw_8bit\", \n",
    "    save_total_limit             = SAVE_TOTAL_LIMIT,\n",
    "    \n",
    "    report_to                    = \"wandb\",\n",
    "    weight_decay                 = WEIGHT_DECAY,\n",
    "    \n",
    "    \n",
    "    logging_first_step           = True,  \n",
    "    max_grad_norm                = 1.0,\n",
    "    dataloader_num_workers       = 4,\n",
    "    \n",
    "    # load_best_model_at_end       = True,\n",
    "    # metric_for_best_model        = \"eval_loss\",\n",
    "    # greater_is_better            = False,\n",
    ")\n",
    "\n",
    "\n",
    "# Set up the trainer with validation\n",
    "trainer = Trainer(\n",
    "    model                     = peft_model,\n",
    "    args                      = training_args,\n",
    "    train_dataset             = train_dataset,\n",
    "    eval_dataset              = val_dataset,\n",
    "    data_collator             = DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    compute_metrics           = None,  \n",
    "    callbacks                 = [generation_callback],  \n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "# Disable caching during training to avoid memory issues\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "peft_model.save_pretrained(f\"{OUTPUT_DIR}/{RUN_NAME}\")\n",
    "tokenizer.save_pretrained(f\"{OUTPUT_DIR}/{RUN_NAME}\")\n",
    "\n",
    "print(\"Training complete and model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_test_model():\n",
    "    print(\"\\nTesting fine-tuned model:\")\n",
    "    \n",
    "    # Load the base model and LoRA adapter\n",
    "    # base_model = GPTNeoXForCausalLM.from_pretrained(\n",
    "    #     MODEL_ID,\n",
    "    #     revision=MODEL_REVISION,\n",
    "    #     cache_dir=CACHE_DIR,\n",
    "    #     device_map=\"auto\"\n",
    "    # )\n",
    "    \n",
    "    # # Load and apply the fine-tuned LoRA weights\n",
    "    # fine_tuned_model = PeftModel.from_pretrained(\n",
    "    #     base_model, \n",
    "    #     f\"{OUTPUT_DIR}/lora_model\",\n",
    "    #     device_map=\"auto\"\n",
    "    # )\n",
    "    \n",
    "    fine_tuned_model = peft_model\n",
    "    \n",
    "    # Test the model with the prompts\n",
    "    for test_input_string in test_prompts:\n",
    "        inputs = tokenizer(test_input_string, return_tensors=\"pt\").to(DEVICE)\n",
    "        # print(tokens[0])\n",
    "        tokens = fine_tuned_model.generate(\n",
    "            **inputs, \n",
    "            max_length=100, \n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        print(\"Without sampling: \" + tokenizer.decode(tokens[0], skip_special_tokens=True))\n",
    "        \n",
    "        print(\"---------------\")\n",
    "        tokens = fine_tuned_model.generate(\n",
    "            **inputs, \n",
    "            max_length=100, \n",
    "            pad_token_id=tokenizer.eos_token_id, \n",
    "            do_sample=True,\n",
    "        )\n",
    "        print(\"With sampling   : \" + tokenizer.decode(tokens[0], skip_special_tokens=True))\n",
    "        \n",
    "        print(\"\\n===============\")\n",
    "    \n",
    "load_and_test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: False reverse\n",
      "Context: Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him.is a summary of\n",
      "Target: Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday\n",
      "Full sentence: Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him.is a summary ofHarry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday\n",
      "\n",
      "\n",
      "-3.3839298650308702 29.48642137573805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/iwiryadi/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/jet/home/iwiryadi/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from src.utils import * \n",
    "article = \"Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him.\"\n",
    "summary = \"Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday\"\n",
    "adverse_summary = \"Daniel Craig is recasted as James Bond again\"\n",
    "# In normal, query is sentence/article, and answer is summary/highlight (S->A direction)\n",
    "base = calculate_score(summary, article, model, tokenizer, backward=False, query_direction=\"reverse\", debug=True)\n",
    "\n",
    "print(base['normalized_log_prob'], base['perplexity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>▁▁</td></tr><tr><td>eval/samples_per_second</td><td>█▁</td></tr><tr><td>eval/steps_per_second</td><td>█▁</td></tr><tr><td>train/epoch</td><td>▁▁█</td></tr><tr><td>train/global_step</td><td>▁▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>3.26366</td></tr><tr><td>eval/runtime</td><td>0.1651</td></tr><tr><td>eval/samples_per_second</td><td>78.747</td></tr><tr><td>eval/steps_per_second</td><td>12.115</td></tr><tr><td>total_flos</td><td>804418618392576.0</td></tr><tr><td>train/epoch</td><td>4.8</td></tr><tr><td>train/global_step</td><td>15</td></tr><tr><td>train_loss</td><td>4.20255</td></tr><tr><td>train_runtime</td><td>48.4882</td></tr><tr><td>train_samples_per_second</td><td>24.233</td></tr><tr><td>train_steps_per_second</td><td>0.309</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lora-tuning-1</strong> at: <a href='https://wandb.ai/11785_finetuning/ivan-testing/runs/vp93ouy5' target=\"_blank\">https://wandb.ai/11785_finetuning/ivan-testing/runs/vp93ouy5</a><br> View project at: <a href='https://wandb.ai/11785_finetuning/ivan-testing' target=\"_blank\">https://wandb.ai/11785_finetuning/ivan-testing</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250412_211946-vp93ouy5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
