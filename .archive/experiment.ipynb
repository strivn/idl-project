{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo_model = GPTNeoXForCausalLM.from_pretrained(\n",
    "  \"EleutherAI/pythia-160m-deduped\",\n",
    "  revision=\"step143000\",\n",
    "  cache_dir=\"./.cache/pythia-160m-deduped/step143000\",\n",
    ")\n",
    "\n",
    "fo_tokenizer = AutoTokenizer.from_pretrained(\n",
    "  \"EleutherAI/pythia-160m-deduped\",\n",
    "  revision=\"step143000\",\n",
    "  cache_dir=\"./.cache/pythia-160m-deduped/step143000\",\n",
    ")\n",
    "\n",
    "inputs = fo_tokenizer(\"To be or not to be, that is the\", return_tensors=\"pt\")\n",
    "tokens = fo_model.generate(**inputs)\n",
    "fo_tokenizer.decode(tokens[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ba_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"afterless/reverse-pythia-160m\",\n",
    "    # cache_dir=\"./.cache/reverse-pythia-160m\",\n",
    ")\n",
    "\n",
    "ba_model = GPTNeoXForCausalLM.from_pretrained(\n",
    "    \"afterless/reverse-pythia-160m\",\n",
    "    # cache_dir=\"./.cache/reverse-pythia-160m\",\n",
    ")\n",
    "\n",
    "inputs = ba_tokenizer(\n",
    "    \"the cheese was the best\",\n",
    "    return_token_type_ids=False,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "inputs['input_ids'] = t.flip(inputs.input_ids, (1,))\n",
    "tokens = t.flip(ba_model.generate(**inputs), (1,))\n",
    "ba_tokenizer.decode(tokens[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivw/Workspace/Programming_Projects/idls25/idl-project/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /Users/ivw/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/ivw/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as t\n",
    "import numpy as np\n",
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "import pandas as pd\n",
    "import rouge \n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "def load_models():\n",
    "    # Forward model\n",
    "    fo_model = GPTNeoXForCausalLM.from_pretrained(\n",
    "        \"EleutherAI/pythia-160m-deduped\",\n",
    "        revision=\"step143000\",\n",
    "        cache_dir=\"./.cache/pythia-160m-deduped/step143000\",\n",
    "    )\n",
    "    \n",
    "    fo_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"EleutherAI/pythia-160m-deduped\",\n",
    "        revision=\"step143000\",\n",
    "        cache_dir=\"./.cache/pythia-160m-deduped/step143000\",\n",
    "    )\n",
    "    \n",
    "    # Backward model\n",
    "    ba_model = GPTNeoXForCausalLM.from_pretrained(\n",
    "        \"afterless/reverse-pythia-160m\",\n",
    "        cache_dir=\"./.cache/reverse-pythia-160m\",\n",
    "    )\n",
    "    \n",
    "    ba_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"afterless/reverse-pythia-160m\",\n",
    "        cache_dir=\"./.cache/reverse-pythia-160m\",\n",
    "    )\n",
    "    \n",
    "    return fo_model, fo_tokenizer, ba_model, ba_tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "def load_cnn_dataset(num_samples=10):\n",
    "    try:\n",
    "        # Try with a specific cache directory\n",
    "        dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", cache_dir=\"./dataset_cache\")\n",
    "        print(\"Dataset loaded successfully\")\n",
    "        \n",
    "        # Verify the structure - this helps debug\n",
    "        if num_samples > 0:\n",
    "            print(\"Example dataset item:\", dataset['train'][0])\n",
    "            \n",
    "        # Take only a small sample for testing\n",
    "        if hasattr(dataset, 'train'):\n",
    "            return dataset['train'].select(range(min(num_samples, len(dataset['train']))))\n",
    "        \n",
    "        return dataset['train'][:num_samples]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading full dataset: {e}\")\n",
    "        \n",
    "        # Create a tiny synthetic dataset for testing\n",
    "        print(\"Creating synthetic test dataset instead...\")\n",
    "        from datasets import Dataset\n",
    "        \n",
    "        sample_data = {\n",
    "            'article': [\n",
    "                \"John likes to play basketball. He goes to the court every evening. His friends join him on weekends.\",\n",
    "                \"The company announced record profits. Investors were pleased. The stock price increased by 10%.\"\n",
    "            ],\n",
    "            'highlights': [\n",
    "                \"John plays basketball regularly with friends.\",\n",
    "                \"Company profits lead to stock price increase.\"\n",
    "            ],\n",
    "            'id': ['test1', 'test2']  # Added ID field\n",
    "        }\n",
    "        \n",
    "        return Dataset.from_dict(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(example):\n",
    "    \"\"\"\n",
    "    Process the CNN dataset example to extract article and highlight sentences\n",
    "    \"\"\"\n",
    "    # Handle the article - could be a string or a list\n",
    "    if isinstance(example['article'], list):\n",
    "        # Already in list format\n",
    "        article_sentences = example['article']\n",
    "    else:\n",
    "        # Need to tokenize\n",
    "        article_sentences = nltk.sent_tokenize(example['article'])\n",
    "    \n",
    "    # Handle the highlights - could be a string or a list\n",
    "    if isinstance(example['highlights'], list):\n",
    "        # Already in list format\n",
    "        highlight_sentences = example['highlights']\n",
    "    else:\n",
    "        # Need to tokenize\n",
    "        highlight_sentences = nltk.sent_tokenize(example['highlights'])\n",
    "    \n",
    "    return {\n",
    "        'article_sentences': article_sentences,\n",
    "        'highlight_sentences': highlight_sentences\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward model citation evaluation\n",
    "def evaluate_forward_model(model, tokenizer, article_sentences, highlight_sentence, device='cpu'):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    scores = []\n",
    "    with t.no_grad():\n",
    "        for sentence in article_sentences:\n",
    "            # Create a prompt for citation task\n",
    "            prompt = f\"{sentence} is summarized by {highlight_sentence}\"\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "            \n",
    "            # Calculate loss/perplexity\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            loss = outputs.loss\n",
    "            scores.append(loss.item())\n",
    "    \n",
    "    # Return the best match\n",
    "    best_match_idx = np.argmin(scores)\n",
    "    return article_sentences[best_match_idx], scores[best_match_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward model citation evaluation\n",
    "def evaluate_backward_model(model, tokenizer, article_sentences, highlight_sentence, device='cpu'):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    scores = []\n",
    "    with t.no_grad():\n",
    "        for sentence in article_sentences:\n",
    "            # For backward model, we reverse the direction: \"highlight is derived from article\"\n",
    "            prompt = f\"{highlight_sentence} is derived from {sentence}\"\n",
    "            inputs = tokenizer(prompt, return_token_type_ids=False, return_tensors=\"pt\")\n",
    "            \n",
    "            # Flip the input tokens for backward model\n",
    "            inputs['input_ids'] = t.flip(inputs.input_ids, (1,))\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Calculate loss\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            loss = outputs.loss\n",
    "            scores.append(loss.item())\n",
    "    \n",
    "    # Return the best match\n",
    "    best_match_idx = np.argmin(scores)\n",
    "    return article_sentences[best_match_idx], scores[best_match_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics\n",
    "def calculate_metrics(predicted_sentence, gold_sentences):\n",
    "    # Load a sentence embedding model\n",
    "    sentence_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "    \n",
    "    # TF-IDF similarity\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    try:\n",
    "        tfidf_matrix = vectorizer.fit_transform([predicted_sentence] + gold_sentences)\n",
    "        tfidf_similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:])[0]\n",
    "        tfidf_score = max(tfidf_similarities)\n",
    "    except:\n",
    "        tfidf_score = 0\n",
    "    \n",
    "    # Embedding similarity\n",
    "    pred_embedding = sentence_model.encode(predicted_sentence)\n",
    "    gold_embeddings = sentence_model.encode(gold_sentences)\n",
    "    embedding_similarities = cosine_similarity([pred_embedding], gold_embeddings)[0]\n",
    "    embedding_score = max(embedding_similarities)\n",
    "    \n",
    "    # ROUGE score\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = [scorer.score(predicted_sentence, gold_sent) for gold_sent in gold_sentences]\n",
    "    rouge1_score = max([score['rouge1'].fmeasure for score in rouge_scores])\n",
    "    rougeL_score = max([score['rougeL'].fmeasure for score in rouge_scores])\n",
    "    \n",
    "    return {\n",
    "        'tfidf_similarity': tfidf_score,\n",
    "        'embedding_similarity': embedding_score,\n",
    "        'rouge1': rouge1_score,\n",
    "        'rougeL': rougeL_score\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(predicted_sentence, gold_sentences):\n",
    "\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt')\n",
    "    \n",
    "    # Handle empty inputs\n",
    "    if not predicted_sentence or not gold_sentences:\n",
    "        return {\n",
    "            'tfidf_similarity': 0,\n",
    "            'embedding_similarity': 0,\n",
    "            'rouge1': 0,\n",
    "            'rougeL': 0,\n",
    "            'bleu': 0\n",
    "        }\n",
    "    \n",
    "    # Load a sentence embedding model\n",
    "    sentence_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "    \n",
    "    # TF-IDF similarity\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    try:\n",
    "        tfidf_matrix = vectorizer.fit_transform([predicted_sentence] + gold_sentences)\n",
    "        tfidf_similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:])[0]\n",
    "        tfidf_score = max(tfidf_similarities) if len(tfidf_similarities) > 0 else 0\n",
    "    except:\n",
    "        tfidf_score = 0\n",
    "    \n",
    "    # Embedding similarity\n",
    "    try:\n",
    "        pred_embedding = sentence_model.encode(predicted_sentence)\n",
    "        gold_embeddings = sentence_model.encode(gold_sentences)\n",
    "        embedding_similarities = cosine_similarity([pred_embedding], gold_embeddings)[0]\n",
    "        embedding_score = max(embedding_similarities) if len(embedding_similarities) > 0 else 0\n",
    "    except:\n",
    "        embedding_score = 0\n",
    "    \n",
    "    # ROUGE score\n",
    "    try:\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "        rouge_scores = [scorer.score(predicted_sentence, gold_sent) for gold_sent in gold_sentences]\n",
    "        rouge1_score = max([score['rouge1'].fmeasure for score in rouge_scores]) if rouge_scores else 0\n",
    "        rougeL_score = max([score['rougeL'].fmeasure for score in rouge_scores]) if rouge_scores else 0\n",
    "    except:\n",
    "        rouge1_score = 0\n",
    "        rougeL_score = 0\n",
    "    \n",
    "    # BLEU score\n",
    "    try:\n",
    "        # Tokenize for BLEU calculation\n",
    "        predicted_tokens = nltk.word_tokenize(predicted_sentence.lower())\n",
    "        gold_tokens = [nltk.word_tokenize(gold.lower()) for gold in gold_sentences]\n",
    "        \n",
    "        # Calculate BLEU score - take the best score against any reference\n",
    "        bleu_scores = [\n",
    "            sentence_bleu([gold_tok], predicted_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "            for gold_tok in gold_tokens\n",
    "        ]\n",
    "        bleu_score = max(bleu_scores) if bleu_scores else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating BLEU: {e}\")\n",
    "        bleu_score = 0\n",
    "    \n",
    "    return {\n",
    "        'tfidf_similarity': tfidf_score,\n",
    "        'embedding_similarity': embedding_score,\n",
    "        'rouge1': rouge1_score,\n",
    "        'rougeL': rougeL_score,\n",
    "        'bleu': bleu_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully\n",
      "Example dataset item: {'article': 'LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won\\'t cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don\\'t plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don\\'t think I\\'ll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he\\'ll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I\\'ll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe\\'s earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say \\'kid star goes off the rails,\\'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films.  Watch I-Reporter give her review of Potter\\'s latest » . There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer\\'s \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he\\'s legally an adult: \"I just think I\\'m going to be more sort of fair game,\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.', 'highlights': \"Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\\nYoung actor says he has no plans to fritter his cash away .\\nRadcliffe's earnings from first five Potter films have been held in trust fund .\", 'id': '42c027e4ff9730fbb3de84c1af0d2c506e41c3e4'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'article': ['LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won\\'t cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don\\'t plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don\\'t think I\\'ll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he\\'ll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I\\'ll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe\\'s earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say \\'kid star goes off the rails,\\'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films.  Watch I-Reporter give her review of Potter\\'s latest » . There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer\\'s \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he\\'s legally an adult: \"I just think I\\'m going to be more sort of fair game,\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.',\n",
       "  'Editor\\'s note: In our Behind the Scenes series, CNN correspondents share their experiences in covering news and analyze the stories behind the events. Here, Soledad O\\'Brien takes users inside a jail where many of the inmates are mentally ill. An inmate housed on the \"forgotten floor,\" where many mentally ill inmates are housed in Miami before trial. MIAMI, Florida (CNN) -- The ninth floor of the Miami-Dade pretrial detention facility is dubbed the \"forgotten floor.\" Here, inmates with the most severe mental illnesses are incarcerated until they\\'re ready to appear in court. Most often, they face drug charges or charges of assaulting an officer --charges that Judge Steven Leifman says are usually \"avoidable felonies.\" He says the arrests often result from confrontations with police. Mentally ill people often won\\'t do what they\\'re told when police arrive on the scene -- confrontation seems to exacerbate their illness and they become more paranoid, delusional, and less likely to follow directions, according to Leifman. So, they end up on the ninth floor severely mentally disturbed, but not getting any real help because they\\'re in jail. We toured the jail with Leifman. He is well known in Miami as an advocate for justice and the mentally ill. Even though we were not exactly welcomed with open arms by the guards, we were given permission to shoot videotape and tour the floor.  Go inside the \\'forgotten floor\\' » . At first, it\\'s hard to determine where the people are. The prisoners are wearing sleeveless robes. Imagine cutting holes for arms and feet in a heavy wool sleeping bag -- that\\'s kind of what they look like. They\\'re designed to keep the mentally ill patients from injuring themselves. That\\'s also why they have no shoes, laces or mattresses. Leifman says about one-third of all people in Miami-Dade county jails are mentally ill. So, he says, the sheer volume is overwhelming the system, and the result is what we see on the ninth floor. Of course, it is a jail, so it\\'s not supposed to be warm and comforting, but the lights glare, the cells are tiny and it\\'s loud. We see two, sometimes three men -- sometimes in the robes, sometimes naked, lying or sitting in their cells. \"I am the son of the president. You need to get me out of here!\" one man shouts at me. He is absolutely serious, convinced that help is on the way -- if only he could reach the White House. Leifman tells me that these prisoner-patients will often circulate through the system, occasionally stabilizing in a mental hospital, only to return to jail to face their charges. It\\'s brutally unjust, in his mind, and he has become a strong advocate for changing things in Miami. Over a meal later, we talk about how things got this way for mental patients. Leifman says 200 years ago people were considered \"lunatics\" and they were locked up in jails even if they had no charges against them. They were just considered unfit to be in society. Over the years, he says, there was some public outcry, and the mentally ill were moved out of jails and into hospitals. But Leifman says many of these mental hospitals were so horrible they were shut down. Where did the patients go? Nowhere. The streets. They became, in many cases, the homeless, he says. They never got treatment. Leifman says in 1955 there were more than half a million people in state mental hospitals, and today that number has been reduced 90 percent, and 40,000 to 50,000 people are in mental hospitals. The judge says he\\'s working to change this. Starting in 2008, many inmates who would otherwise have been brought to the \"forgotten floor\"  will instead be sent to a new mental health facility -- the first step on a journey toward long-term treatment, not just punishment. Leifman says it\\'s not the complete answer, but it\\'s a start. Leifman says the best part is that it\\'s a win-win solution. The patients win, the families are relieved, and the state saves money by simply not cycling these prisoners through again and again. And, for Leifman, justice is served. E-mail to a friend .',\n",
       "  'MINNEAPOLIS, Minnesota (CNN) -- Drivers who were on the Minneapolis bridge when it collapsed told harrowing tales of survival. \"The whole bridge from one side of the Mississippi to the other just completely gave way, fell all the way down,\" survivor Gary Babineau told CNN. \"I probably had a 30-, 35-foot free fall. And there\\'s cars in the water, there\\'s cars on fire. The whole bridge is down.\" He said his back was injured but he determined he could move around. \"I realized there was a school bus right next to me, and me and a couple of other guys went over and started lifting the kids off the bridge. They were yelling, screaming, bleeding. I think there were some broken bones.\"  Watch a driver describe his narrow escape » . At home when he heard about the disaster, Dr. John Hink, an emergency room physician, jumped into his car and rushed to the scene in 15 minutes. He arrived at the south side of the bridge, stood on the riverbank and saw dozens of people lying dazed on an expansive deck. They were in the middle of the Mississippi River, which was churning fast, and he had no way of getting to them. He went to the north side, where there was easier access to people. Ambulances were also having a hard time driving down to the river to get closer to the scene. Working feverishly, volunteers, EMTs and other officials managed to get 55 people into ambulances in less than two hours. Occasionally, a pickup truck with a medic inside would drive to get an injured person and bring him back up even ground, Hink told CNN. The rescue effort was controlled and organized, he said; the opposite of the lightning-quick collapse. \"I could see the whole bridge as it was going down, as it was falling,\" Babineau said. \"It just gave a rumble real quick, and it all just gave way, and it just fell completely, all the way to the ground. And there was dust everywhere and it was just like everyone has been saying: It was just like out of the movies.\" Babineau said the rear of his pickup truck was dangling over the edge of a broken-off section of the bridge. He said several vehicles slid past him into the water. \"I stayed in my car for one or two seconds. I saw a couple cars fall,\" he said. \"So I stayed in my car until the cars quit falling for a second, then I got out real quick, ran in front of my truck -- because behind my truck was just a hole -- and I helped a woman off of the bridge with me. \"I just wanted off the bridge, and then I ran over to the school bus. I started grabbing kids and handing them down. It was just complete chaos.\" He said most of the children were crying or screaming. He and other rescuers set them on the ground and told them to run to the river bank, but a few needed to be carried because of their injuries.  See rescuers clamber over rubble » . Babineau said he had no rescue training. \"I just knew what I had to do at the moment.\" Melissa Hughes, 32, of Minneapolis, told The Associated Press that she was driving home when the western edge of the bridge collapsed under her. \"You know that free-fall feeling? I felt that twice,\" Hughes said. A pickup landed on top of her car, but she was not hurt. \"I had no idea there was a vehicle on my car,\" she told AP. \"It\\'s really very surreal.\" Babineau told the Minneapolis Star-Tribune: \"On the way down, I thought I was dead. I literally thought I was dead. \"My truck was completely face down, pointed toward the ground, and my truck got ripped in half. It was folded in half, and I can\\'t believe I\\'m alive.\"  See and hear eyewitness accounts » . Bernie Toivonen told CNN\\'s \"American Morning\" that his vehicle was on a part of the bridge that ended up tilted at a 45-degree angle. \"I knew the deck was going down, there was no question about it, and I thought I was going to die,\" he said. After the bridge settled and his car remained upright, \"I just put in park, turned the key off and said, \\'Oh, I\\'m alive,\\' \" he said. E-mail to a friend .'],\n",
       " 'highlights': [\"Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\\nYoung actor says he has no plans to fritter his cash away .\\nRadcliffe's earnings from first five Potter films have been held in trust fund .\",\n",
       "  'Mentally ill inmates in Miami are housed on the \"forgotten floor\"\\nJudge Steven Leifman says most are there as a result of \"avoidable felonies\"\\nWhile CNN tours facility, patient shouts: \"I am the son of the president\"\\nLeifman says the system is unjust and he\\'s fighting for change .',\n",
       "  'NEW: \"I thought I was going to die,\" driver says .\\nMan says pickup truck was folded in half; he just has cut on face .\\nDriver: \"I probably had a 30-, 35-foot free fall\"\\nMinnesota bridge collapsed during rush hour Wednesday .'],\n",
       " 'id': ['42c027e4ff9730fbb3de84c1af0d2c506e41c3e4',\n",
       "  'ee8871b15c50d0db17b0179a6d2beab35065f1e9',\n",
       "  '06352019a19ae31e527f37f7571c6dd7f0c5da37']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_cnn_dataset(3)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(model, tokenizer, text, device):\n",
    "    \"\"\"Calculate the perplexity of a text using a language model\"\"\"\n",
    "    # Tokenize the text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Calculate perplexity\n",
    "    with t.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "    \n",
    "    # Perplexity is the exponential of the average negative log-likelihood\n",
    "    return t.exp(loss).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(num_samples=10):\n",
    "    # Load models and dataset\n",
    "    fo_model, fo_tokenizer, ba_model, ba_tokenizer = load_models()\n",
    "    dataset = load_cnn_dataset(num_samples)\n",
    "    \n",
    "    print(f\"Dataset type: {type(dataset)}\")\n",
    "    print(f\"Dataset keys: {dataset.keys()}\")\n",
    "    \n",
    "    # Determine device\n",
    "    if t.cuda.is_available(): \n",
    "        device = 'cuda' \n",
    "    elif t.backends.mps.is_available():\n",
    "        device = 'mps'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Results\n",
    "    results = []  # Store all results for tabular display\n",
    "    \n",
    "    # Correctly iterate through the dataset by index\n",
    "    num_examples = len(dataset['article']) if 'article' in dataset else 0\n",
    "    print(f\"Processing {num_examples} examples\")\n",
    "    \n",
    "    for idx in tqdm(range(num_examples)):\n",
    "        try:\n",
    "            # Properly construct each example from the dataset\n",
    "            example = {\n",
    "                'article': dataset['article'][idx],\n",
    "                'highlights': dataset['highlights'][idx],\n",
    "                'id': dataset['id'][idx] if 'id' in dataset else f\"example_{idx}\"\n",
    "            }\n",
    "            \n",
    "            # Print article and highlight for visibility (truncated for readability)\n",
    "            print(f\"\\n--- Example {idx+1} ---\")\n",
    "            print(f\"Article (truncated): {example['article'][:150]}...\")\n",
    "            print(f\"Highlight: {example['highlights']}\")\n",
    "            \n",
    "            # Preprocess\n",
    "            processed = preprocess_text(example)\n",
    "            article_sentences = processed['article_sentences']\n",
    "            \n",
    "            # Skip if article is too short\n",
    "            if len(article_sentences) < 3:\n",
    "                print(\"Article too short, skipping\")\n",
    "                continue\n",
    "                \n",
    "            # Evaluate on first highlight sentence (if available)\n",
    "            if processed['highlight_sentences']:\n",
    "                highlight = processed['highlight_sentences'][0]\n",
    "                \n",
    "                # Forward model evaluation\n",
    "                fw_sentence, fw_score = evaluate_forward_model(\n",
    "                    fo_model, fo_tokenizer, article_sentences, highlight, device\n",
    "                )\n",
    "                fw_metrics = calculate_metrics(fw_sentence, processed['highlight_sentences'])\n",
    "                fw_perplexity = calculate_perplexity(fo_model, fo_tokenizer, fw_sentence, device)\n",
    "                fw_metrics['perplexity'] = fw_perplexity\n",
    "                \n",
    "                print(f\"Forward model found: {fw_sentence}\")\n",
    "                \n",
    "                # Backward model evaluation\n",
    "                bw_sentence, bw_score = evaluate_backward_model(\n",
    "                    ba_model, ba_tokenizer, article_sentences, highlight, device\n",
    "                )\n",
    "                bw_metrics = calculate_metrics(bw_sentence, processed['highlight_sentences'])\n",
    "                bw_perplexity = calculate_perplexity(ba_model, ba_tokenizer, bw_sentence, device)\n",
    "                bw_metrics['perplexity'] = bw_perplexity\n",
    "                \n",
    "                print(f\"Backward model found: {bw_sentence}\")\n",
    "                \n",
    "                # Store results for this example\n",
    "                results.append({\n",
    "                    'example_id': example['id'],\n",
    "                    'highlight': highlight,\n",
    "                    'forward_sentence': fw_sentence,\n",
    "                    'backward_sentence': bw_sentence,\n",
    "                    'forward_tfidf': fw_metrics['tfidf_similarity'],\n",
    "                    'forward_embedding': fw_metrics['embedding_similarity'],\n",
    "                    'forward_rouge1': fw_metrics['rouge1'],\n",
    "                    'forward_rougeL': fw_metrics['rougeL'],\n",
    "                    'forward_bleu': fw_metrics['bleu'],  # Add this line\n",
    "                    'forward_perplexity': fw_perplexity,\n",
    "                    'backward_tfidf': bw_metrics['tfidf_similarity'],\n",
    "                    'backward_embedding': bw_metrics['embedding_similarity'],\n",
    "                    'backward_rouge1': bw_metrics['rouge1'],\n",
    "                    'backward_rougeL': bw_metrics['rougeL'],\n",
    "                    'backward_bleu': bw_metrics['bleu'],  # Add this line\n",
    "                    'backward_perplexity': bw_perplexity\n",
    "                })\n",
    "                \n",
    "                # Print metrics for this example\n",
    "                print(\"Forward Model Metrics:\")\n",
    "                for metric, value in fw_metrics.items():\n",
    "                    print(f\"  {metric}: {value:.4f}\")\n",
    "                \n",
    "                print(\"Backward Model Metrics:\")\n",
    "                for metric, value in bw_metrics.items():\n",
    "                    print(f\"  {metric}: {value:.4f}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing example {idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Skip table if no results\n",
    "    if not results:\n",
    "        print(\"No valid results found!\")\n",
    "        return [], []\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Add a row with average metrics\n",
    "    avg_row = {\n",
    "        'example_id': 'AVERAGE',\n",
    "        'highlight': '',\n",
    "        'forward_sentence': '',\n",
    "        'backward_sentence': '',\n",
    "    }\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col not in ['example_id', 'highlight', 'forward_sentence', 'backward_sentence']:\n",
    "            avg_row[col] = df[col].mean()\n",
    "    \n",
    "    # Append average row\n",
    "    df = df._append(avg_row, ignore_index=True)\n",
    "    \n",
    "    # Display the full table\n",
    "    print(\"\\n--- Detailed Results ---\")\n",
    "    # Select just the metric columns for a cleaner display\n",
    "    metrics_df = df[['example_id', 'forward_tfidf', 'forward_embedding', 'forward_rouge1', 'forward_rougeL', 'forward_perplexity',\n",
    "                    'backward_tfidf', 'backward_embedding', 'backward_rouge1', 'backward_rougeL', 'backward_perplexity']]\n",
    "    \n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    print(metrics_df)\n",
    "    \n",
    "    # Create a summary table comparing forward vs backward\n",
    "    print(\"\\n--- Summary: Forward vs Backward ---\")\n",
    "    # In the summary_df creation part of run_evaluation:\n",
    "    summary_df = pd.DataFrame({\n",
    "        'Metric': ['TF-IDF Similarity', 'Embedding Similarity', 'ROUGE-1', 'ROUGE-L', 'BLEU', 'Perplexity'],\n",
    "        'Forward Model': [\n",
    "            df['forward_tfidf'].mean(),\n",
    "            df['forward_embedding'].mean(),\n",
    "            df['forward_rouge1'].mean(),\n",
    "            df['forward_rougeL'].mean(),\n",
    "            df['forward_bleu'].mean(),\n",
    "            df['forward_perplexity'].mean()\n",
    "        ],\n",
    "        'Backward Model': [\n",
    "            df['backward_tfidf'].mean(),\n",
    "            df['backward_embedding'].mean(),\n",
    "            df['backward_rouge1'].mean(),\n",
    "            df['backward_rougeL'].mean(),\n",
    "            df['backward_bleu'].mean(),\n",
    "            df['backward_perplexity'].mean()\n",
    "        ],\n",
    "        'Difference': [\n",
    "            df['backward_tfidf'].mean() - df['forward_tfidf'].mean(),\n",
    "            df['backward_embedding'].mean() - df['forward_embedding'].mean(),\n",
    "            df['backward_rouge1'].mean() - df['forward_rouge1'].mean(),\n",
    "            df['backward_rougeL'].mean() - df['forward_rougeL'].mean(),\n",
    "            df['backward_bleu'].mean() - df['forward_bleu'].mean(),\n",
    "            df['backward_perplexity'].mean() - df['forward_perplexity'].mean()\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    # Add return statement at the end\n",
    "    return summary_df, results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully\n",
      "Example dataset item: {'article': 'LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won\\'t cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don\\'t plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don\\'t think I\\'ll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he\\'ll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I\\'ll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe\\'s earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say \\'kid star goes off the rails,\\'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films.  Watch I-Reporter give her review of Potter\\'s latest » . There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer\\'s \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he\\'s legally an adult: \"I just think I\\'m going to be more sort of fair game,\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.', 'highlights': \"Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\\nYoung actor says he has no plans to fritter his cash away .\\nRadcliffe's earnings from first five Potter films have been held in trust fund .\", 'id': '42c027e4ff9730fbb3de84c1af0d2c506e41c3e4'}\n",
      "Dataset type: <class 'dict'>\n",
      "Dataset keys: dict_keys(['article', 'highlights', 'id'])\n",
      "Using device: mps\n",
      "Processing 3 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 1 ---\n",
      "Article (truncated): LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monda...\n",
      "Highlight: Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\n",
      "Young actor says he has no plans to fritter his cash away .\n",
      "Radcliffe's earnings from first five Potter films have been held in trust fund .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivw/Workspace/Programming_Projects/idls25/idl-project/.venv/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/ivw/Workspace/Programming_Projects/idls25/idl-project/.venv/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/ivw/Workspace/Programming_Projects/idls25/idl-project/.venv/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward model found: LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivw/Workspace/Programming_Projects/idls25/idl-project/.venv/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/ivw/Workspace/Programming_Projects/idls25/idl-project/.venv/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/ivw/Workspace/Programming_Projects/idls25/idl-project/.venv/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      " 33%|███▎      | 1/3 [00:49<01:38, 49.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward model found: LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him.\n",
      "Forward Model Metrics:\n",
      "  tfidf_similarity: 0.4075\n",
      "  embedding_similarity: 0.8524\n",
      "  rouge1: 0.4400\n",
      "  rougeL: 0.4400\n",
      "  bleu: 0.1593\n",
      "  perplexity: 26.3242\n",
      "Backward Model Metrics:\n",
      "  tfidf_similarity: 0.4075\n",
      "  embedding_similarity: 0.8524\n",
      "  rouge1: 0.4400\n",
      "  rougeL: 0.4400\n",
      "  bleu: 0.1593\n",
      "  perplexity: 6018.7441\n",
      "\n",
      "--- Example 2 ---\n",
      "Article (truncated): Editor's note: In our Behind the Scenes series, CNN correspondents share their experiences in covering news and analyze the stories behind the events....\n",
      "Highlight: Mentally ill inmates in Miami are housed on the \"forgotten floor\"\n",
      "Judge Steven Leifman says most are there as a result of \"avoidable felonies\"\n",
      "While CNN tours facility, patient shouts: \"I am the son of the president\"\n",
      "Leifman says the system is unjust and he's fighting for change .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivw/Workspace/Programming_Projects/idls25/idl-project/.venv/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/ivw/Workspace/Programming_Projects/idls25/idl-project/.venv/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward model found: Leifman says in 1955 there were more than half a million people in state mental hospitals, and today that number has been reduced 90 percent, and 40,000 to 50,000 people are in mental hospitals.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:55<00:24, 24.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward model found: MIAMI, Florida (CNN) -- The ninth floor of the Miami-Dade pretrial detention facility is dubbed the \"forgotten floor.\"\n",
      "Forward Model Metrics:\n",
      "  tfidf_similarity: 0.1216\n",
      "  embedding_similarity: 0.4310\n",
      "  rouge1: 0.1882\n",
      "  rougeL: 0.1176\n",
      "  bleu: 0.0000\n",
      "  perplexity: 25.2208\n",
      "Backward Model Metrics:\n",
      "  tfidf_similarity: 0.3690\n",
      "  embedding_similarity: 0.6910\n",
      "  rouge1: 0.2985\n",
      "  rougeL: 0.1791\n",
      "  bleu: 0.0351\n",
      "  perplexity: 11079.6162\n",
      "\n",
      "--- Example 3 ---\n",
      "Article (truncated): MINNEAPOLIS, Minnesota (CNN) -- Drivers who were on the Minneapolis bridge when it collapsed told harrowing tales of survival. \"The whole bridge from ...\n",
      "Highlight: NEW: \"I thought I was going to die,\" driver says .\n",
      "Man says pickup truck was folded in half; he just has cut on face .\n",
      "Driver: \"I probably had a 30-, 35-foot free fall\"\n",
      "Minnesota bridge collapsed during rush hour Wednesday .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivw/Workspace/Programming_Projects/idls25/idl-project/.venv/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/ivw/Workspace/Programming_Projects/idls25/idl-project/.venv/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/ivw/Workspace/Programming_Projects/idls25/idl-project/.venv/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward model found: \"I knew the deck was going down, there was no question about it, and I thought I was going to die,\" he said.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivw/Workspace/Programming_Projects/idls25/idl-project/.venv/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/ivw/Workspace/Programming_Projects/idls25/idl-project/.venv/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/ivw/Workspace/Programming_Projects/idls25/idl-project/.venv/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "100%|██████████| 3/3 [01:01<00:00, 20.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward model found: \"I knew the deck was going down, there was no question about it, and I thought I was going to die,\" he said.\n",
      "Forward Model Metrics:\n",
      "  tfidf_similarity: 0.4311\n",
      "  embedding_similarity: 0.5442\n",
      "  rouge1: 0.4242\n",
      "  rougeL: 0.4242\n",
      "  bleu: 0.2922\n",
      "  perplexity: 12.7295\n",
      "Backward Model Metrics:\n",
      "  tfidf_similarity: 0.4311\n",
      "  embedding_similarity: 0.5442\n",
      "  rouge1: 0.4242\n",
      "  rougeL: 0.4242\n",
      "  bleu: 0.2922\n",
      "  perplexity: 776.8411\n",
      "\n",
      "--- Detailed Results ---\n",
      "                                 example_id  forward_tfidf  forward_embedding  forward_rouge1  forward_rougeL  forward_perplexity  backward_tfidf  backward_embedding  backward_rouge1  backward_rougeL  backward_perplexity\n",
      "0  42c027e4ff9730fbb3de84c1af0d2c506e41c3e4       0.407464           0.852448        0.440000        0.440000           26.324179        0.407464            0.852448         0.440000         0.440000          6018.744141\n",
      "1  ee8871b15c50d0db17b0179a6d2beab35065f1e9       0.121585           0.431025        0.188235        0.117647           25.220823        0.368985            0.690971         0.298507         0.179104         11079.616211\n",
      "2  06352019a19ae31e527f37f7571c6dd7f0c5da37       0.431111           0.544246        0.424242        0.424242           12.729452        0.431111            0.544246         0.424242         0.424242           776.841064\n",
      "3                                   AVERAGE       0.320053           0.609240        0.350826        0.327296           21.424818        0.402520            0.695888         0.387583         0.347782          5958.400472\n",
      "\n",
      "--- Summary: Forward vs Backward ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the evaluation\n",
    "if __name__ == \"__main__\":\n",
    "    summary_df, results = run_evaluation(num_samples=3)  # Start with a small number for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backward_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to html experiment.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
