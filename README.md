# IDL Project

Currently just a repository to share papers, references, etc! 
The descriptions probably aren't really accurate nor helpful, just jotting down thoughts. 

# Ideas and References

## Safety / Interpretability / Alignment
1. Playing around with toy monosemanticity models, it could be cool to gain a bit more insights on how neurons (weights) store information. [Paper](https://transformer-circuits.pub/2022/toy_model/index.html)
2. **Interpretability** Playing around with circuits / neurons / visualizing neurons [Distill](https://distill.pub/2020/circuits/zoom-in/)
3. **Interpretabiltiy** Doing something with interpreting features, like discussed [Anthropic's Paper](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html)
4. **Safety** Reproducing results from removing fine tuning from open source models, such as: [Llama 3](https://arxiv.org/abs/2407.01376), [Llama 2](https://arxiv.org/abs/2311.00117)

## NLP and LLMs  
1. **NLP / LLMs** Titans architecture - Google new architecture that mentioned has large capacity of memory during inference Not sure what can be done yet. [Paper](https://arxiv.org/abs/2501.00663)
2. **NLP / LLMs** Explore how to develop models that 'truthfully' references documents provided and ways to evaluate. See perhaps: [Citations by Anthropic](https://docs.anthropic.com/en/docs/build-with-claude/citations), [Hallucinations benchmark](https://arxiv.org/pdf/2501.08292)
