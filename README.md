# Citing Backward: Reversed LLMs


## Overview

[Part of course project for 11785 S25]

This project explores how Time-Reversed Language Models (TRLMs) can improve citation accuracy in language models. We investigate whether backward token prediction enhances citation attribution and reduces hallucination in high-stakes domains like law, healthcare, and research.

Mainly inspired by "Time-Reversal Provides Unsupervised Feedback to LLMs" (Yerram et al., 2024), we implement smaller-scale models to validate the TRLM approach for citation improvement.

Large Language Models (LLMs) excel at text generation but struggle with accurate citation. We're implementing and evaluating reversed language models that predict queries from responses to enhance citation attribution and reduce hallucination in citations.

## Getting Started
```python
# Clone the repository
git clone https://github.com/strivn/idl-project
cd idl-project

# Install requirements
pip install -r requirements.txt

```

## Team
- Ivan Wiryadi
- Janbol Jangabyl
- Jihu Hwang
- Xuanyi Shen