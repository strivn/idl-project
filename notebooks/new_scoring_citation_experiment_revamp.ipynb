{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring and Citations Testbed\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os \n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "sys.path.append('/ocean/projects/cis250068p/jhwang4/idl-project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/jhwang4/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset, Dataset\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rouge_score import rouge_scorer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import ast\n",
    "import re\n",
    "from scipy.spatial.distance import cosine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from src.model import load_fo_model, load_ba_model, DEVICE\n",
    "from src.data import load_cnn_dataset\n",
    "#from src.utils import *\n",
    "from src.utils_batch_v2 import *\n",
    "#from src.search import *\n",
    "from src.search_batch_v2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 200)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /jet/home/jhwang4/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# Load the models\n",
    "fo_model, fo_tokenizer = load_fo_model()\n",
    "ba_model, ba_tokenizer = load_ba_model()\n",
    "fo_model = fo_model.to(DEVICE)\n",
    "ba_model = ba_model.to(DEVICE)\n",
    "fo_model.half()\n",
    "ba_model.half()\n",
    "# 멀티-GPU 자동 적용\n",
    "if t.cuda.device_count() > 1:\n",
    "    print(t.cuda.device_count())\n",
    "    fo_model = t.nn.DataParallel(fo_model)\n",
    "    ba_model = t.nn.DataParallel(ba_model)\n",
    "\n",
    "# Example Text\n",
    "# article = \"Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him.\"\n",
    "# summary = \"Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday\"\n",
    "# adverse_summary = \"Daniel Craig is recasted as James Bond again\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug = True\n",
    "\n",
    "# # In normal, query is sentence/article, and answer is summary/highlight (S->A direction)\n",
    "# base = calculate_score(summary, article, fo_model, fo_tokenizer, backward=False, query_direction=\"normal\", debug=debug)\n",
    "\n",
    "# # In Fo, query is summary/highlight , and answer is sentence/article(A->S direction)\n",
    "# fo = calculate_score(summary, article, fo_model, fo_tokenizer, backward=False, query_direction=\"reverse\", debug=debug)\n",
    "\n",
    "# # In Ba, query is summary/highlight , and answer is sentence/article(A->S direction)\n",
    "# ba = calculate_score(summary, article, ba_model, ba_tokenizer, backward=True, query_direction=\"reverse\", debug=debug)\n",
    "\n",
    "\n",
    "# adv_base = calculate_score(adverse_summary, article, fo_model, fo_tokenizer, backward=False, query_direction=\"normal\", debug=debug)\n",
    "\n",
    "# adv_fo = calculate_score(adverse_summary, article, fo_model, fo_tokenizer, backward=False, query_direction=\"reverse\", debug=debug)\n",
    "\n",
    "# adv_ba = calculate_score(adverse_summary, article, ba_model, ba_tokenizer, backward=True, query_direction=\"reverse\", debug=debug)\n",
    "\n",
    "\n",
    "# scores_data = {\n",
    "#     'Model Type': ['Base', 'Forward', 'Backward', 'Adv Base', 'Adv Forward', 'Adv Backward'],\n",
    "    \n",
    "#     'Query Direction': ['S->A', 'A->S', 'A->S', 'S->A', 'A->S', 'A->S'],\n",
    "    \n",
    "#     'Sequence Log Prob': [\n",
    "#         base['sequence_log_prob'],\n",
    "#         fo['sequence_log_prob'],\n",
    "#         ba['sequence_log_prob'],\n",
    "        \n",
    "#         adv_base['sequence_log_prob'],\n",
    "#         adv_fo['sequence_log_prob'],\n",
    "#         adv_ba['sequence_log_prob'],\n",
    "#     ],\n",
    "#     'Normalized Log Prob': [\n",
    "#         base['normalized_log_prob'],\n",
    "#         fo['normalized_log_prob'],\n",
    "#         ba['normalized_log_prob'],\n",
    "        \n",
    "#         adv_base['normalized_log_prob'],\n",
    "#         adv_fo['normalized_log_prob'],\n",
    "#         adv_ba['normalized_log_prob'],\n",
    "#     ],\n",
    "#     'Perplexity': [\n",
    "#         base['perplexity'],\n",
    "#         fo['perplexity'],\n",
    "#         ba['perplexity'],\n",
    "        \n",
    "#         adv_base['perplexity'],\n",
    "#         adv_fo['perplexity'],\n",
    "#         adv_ba['perplexity'],\n",
    "#     ]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Type</th>\n",
       "      <th>Query Direction</th>\n",
       "      <th>Sequence Log Prob</th>\n",
       "      <th>Normalized Log Prob</th>\n",
       "      <th>Perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Base</td>\n",
       "      <td>S-&gt;A</td>\n",
       "      <td>-113.7687</td>\n",
       "      <td>-2.7748</td>\n",
       "      <td>16.0362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forward</td>\n",
       "      <td>A-&gt;S</td>\n",
       "      <td>-48.3527</td>\n",
       "      <td>-3.0220</td>\n",
       "      <td>20.5332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Backward</td>\n",
       "      <td>A-&gt;S</td>\n",
       "      <td>-40.4212</td>\n",
       "      <td>-2.5263</td>\n",
       "      <td>12.5074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adv Base</td>\n",
       "      <td>S-&gt;A</td>\n",
       "      <td>-156.6150</td>\n",
       "      <td>-3.8199</td>\n",
       "      <td>45.5986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adv Forward</td>\n",
       "      <td>A-&gt;S</td>\n",
       "      <td>-65.3958</td>\n",
       "      <td>-7.2662</td>\n",
       "      <td>1431.1034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Adv Backward</td>\n",
       "      <td>A-&gt;S</td>\n",
       "      <td>-57.9545</td>\n",
       "      <td>-6.4394</td>\n",
       "      <td>626.0255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Model Type Query Direction  Sequence Log Prob  Normalized Log Prob  \\\n",
       "0          Base            S->A          -113.7687              -2.7748   \n",
       "1       Forward            A->S           -48.3527              -3.0220   \n",
       "2      Backward            A->S           -40.4212              -2.5263   \n",
       "3      Adv Base            S->A          -156.6150              -3.8199   \n",
       "4   Adv Forward            A->S           -65.3958              -7.2662   \n",
       "5  Adv Backward            A->S           -57.9545              -6.4394   \n",
       "\n",
       "   Perplexity  \n",
       "0     16.0362  \n",
       "1     20.5332  \n",
       "2     12.5074  \n",
       "3     45.5986  \n",
       "4   1431.1034  \n",
       "5    626.0255  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result_df = pd.DataFrame(scores_data)\n",
    "# result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded into cache successfully\n",
      "Example dataset item: {'article': 'LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won\\'t cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don\\'t plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don\\'t think I\\'ll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he\\'ll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I\\'ll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe\\'s earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say \\'kid star goes off the rails,\\'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films.  Watch I-Reporter give her review of Potter\\'s latest » . There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer\\'s \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he\\'s legally an adult: \"I just think I\\'m going to be more sort of fair game,\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.', 'highlights': \"Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\\nYoung actor says he has no plans to fritter his cash away .\\nRadcliffe's earnings from first five Potter films have been held in trust fund .\", 'id': '42c027e4ff9730fbb3de84c1af0d2c506e41c3e4'}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_cnn_dataset(num_samples=47852)\n",
    "#dataset = load_cnn_dataset()\n",
    "dataset = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell...</td>\n",
       "      <td>Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\\nYoung actor says he has no plans to fritter his cash away .\\nRadcliffe's earnings from first five Potter films have be...</td>\n",
       "      <td>42c027e4ff9730fbb3de84c1af0d2c506e41c3e4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Editor's note: In our Behind the Scenes series, CNN correspondents share their experiences in covering news and analyze the stories behind the events. Here, Soledad O'Brien takes users inside a ja...</td>\n",
       "      <td>Mentally ill inmates in Miami are housed on the \"forgotten floor\"\\nJudge Steven Leifman says most are there as a result of \"avoidable felonies\"\\nWhile CNN tours facility, patient shouts: \"I am the...</td>\n",
       "      <td>ee8871b15c50d0db17b0179a6d2beab35065f1e9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MINNEAPOLIS, Minnesota (CNN) -- Drivers who were on the Minneapolis bridge when it collapsed told harrowing tales of survival. \"The whole bridge from one side of the Mississippi to the other just ...</td>\n",
       "      <td>NEW: \"I thought I was going to die,\" driver says .\\nMan says pickup truck was folded in half; he just has cut on face .\\nDriver: \"I probably had a 30-, 35-foot free fall\"\\nMinnesota bridge collaps...</td>\n",
       "      <td>06352019a19ae31e527f37f7571c6dd7f0c5da37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WASHINGTON (CNN) -- Doctors removed five small polyps from President Bush's colon on Saturday, and \"none appeared worrisome,\" a White House spokesman said. The polyps were removed and sent to the ...</td>\n",
       "      <td>Five small polyps found during procedure; \"none worrisome,\" spokesman says .\\nPresident reclaims powers transferred to vice president .\\nBush undergoes routine colonoscopy at Camp David .</td>\n",
       "      <td>24521a2abb2e1f5e34e6824e0f9e56904a2b0e88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(CNN)  -- The National Football League has indefinitely suspended Atlanta Falcons quarterback Michael Vick without pay, officials with the league said Friday. NFL star Michael Vick is set to appea...</td>\n",
       "      <td>NEW: NFL chief, Atlanta Falcons owner critical of Michael Vick's conduct .\\nNFL suspends Falcons quarterback indefinitely without pay .\\nVick admits funding dogfighting operation but says he did n...</td>\n",
       "      <td>7fe70cc8b12fab2d0a258fababf7d9c6b5e1262a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                   article  \\\n",
       "0  LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell...   \n",
       "1  Editor's note: In our Behind the Scenes series, CNN correspondents share their experiences in covering news and analyze the stories behind the events. Here, Soledad O'Brien takes users inside a ja...   \n",
       "2  MINNEAPOLIS, Minnesota (CNN) -- Drivers who were on the Minneapolis bridge when it collapsed told harrowing tales of survival. \"The whole bridge from one side of the Mississippi to the other just ...   \n",
       "3  WASHINGTON (CNN) -- Doctors removed five small polyps from President Bush's colon on Saturday, and \"none appeared worrisome,\" a White House spokesman said. The polyps were removed and sent to the ...   \n",
       "4  (CNN)  -- The National Football League has indefinitely suspended Atlanta Falcons quarterback Michael Vick without pay, officials with the league said Friday. NFL star Michael Vick is set to appea...   \n",
       "\n",
       "                                                                                                                                                                                                highlights  \\\n",
       "0  Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\\nYoung actor says he has no plans to fritter his cash away .\\nRadcliffe's earnings from first five Potter films have be...   \n",
       "1  Mentally ill inmates in Miami are housed on the \"forgotten floor\"\\nJudge Steven Leifman says most are there as a result of \"avoidable felonies\"\\nWhile CNN tours facility, patient shouts: \"I am the...   \n",
       "2  NEW: \"I thought I was going to die,\" driver says .\\nMan says pickup truck was folded in half; he just has cut on face .\\nDriver: \"I probably had a 30-, 35-foot free fall\"\\nMinnesota bridge collaps...   \n",
       "3              Five small polyps found during procedure; \"none worrisome,\" spokesman says .\\nPresident reclaims powers transferred to vice president .\\nBush undergoes routine colonoscopy at Camp David .   \n",
       "4  NEW: NFL chief, Atlanta Falcons owner critical of Michael Vick's conduct .\\nNFL suspends Falcons quarterback indefinitely without pay .\\nVick admits funding dogfighting operation but says he did n...   \n",
       "\n",
       "                                         id  \n",
       "0  42c027e4ff9730fbb3de84c1af0d2c506e41c3e4  \n",
       "1  ee8871b15c50d0db17b0179a6d2beab35065f1e9  \n",
       "2  06352019a19ae31e527f37f7571c6dd7f0c5da37  \n",
       "3  24521a2abb2e1f5e34e6824e0f9e56904a2b0e88  \n",
       "4  7fe70cc8b12fab2d0a258fababf7d9c6b5e1262a  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47852\n"
     ]
    }
   ],
   "source": [
    "print(dataset.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                | 0/95704 [00:00<?, ?it/s]/ocean/projects/cis250068p/jhwang4/idl-project/src/utils_batch_v2.py:272: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with t.no_grad(), autocast():\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95704/95704 [6:18:49<00:00,  4.21it/s]\n"
     ]
    }
   ],
   "source": [
    "#linear_results = linear_attribution_search(dataset, fo_model, fo_tokenizer, ba_model, ba_tokenizer) # 50 - 59.4s\n",
    "linear_results = linear_attribution_search(dataset, fo_model, fo_tokenizer, ba_model, ba_tokenizer, sentence_batch_size=500) # 52.4s, 2m30.9s, 6h18m49s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                     | 9/95704 [00:06<17:43:39,  1.50it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#binary_results = binary_search_attribution_search(dataset, fo_model, fo_tokenizer, ba_model, ba_tokenizer) # 50 - 25.7s\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m binary_results \u001b[38;5;241m=\u001b[39m \u001b[43mbinary_search_attribution_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfo_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfo_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mba_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mba_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ocean/projects/cis250068p/jhwang4/idl-project/src/search_batch_v2.py:189\u001b[0m, in \u001b[0;36mbinary_search_attribution_search\u001b[0;34m(dataset, fo_model, fo_tokenizer, ba_model, ba_tokenizer, max_iterations, sentence_batch_size)\u001b[0m\n\u001b[1;32m    186\u001b[0m article \u001b[38;5;241m=\u001b[39m example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# 세 가지 모델 설정으로 이진 탐색 실행\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m baseline_result \u001b[38;5;241m=\u001b[39m \u001b[43mbinary_search_citation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhighlight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marticle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfo_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfo_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackward\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_direction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnormal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43msentence_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msentence_batch_size\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m fo_result \u001b[38;5;241m=\u001b[39m binary_search_citation(\n\u001b[1;32m    197\u001b[0m     highlight, article, fo_model, fo_tokenizer,\n\u001b[1;32m    198\u001b[0m     backward\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, query_direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreverse\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    199\u001b[0m     max_iterations\u001b[38;5;241m=\u001b[39mmax_iterations,\n\u001b[1;32m    200\u001b[0m     sentence_batch_size\u001b[38;5;241m=\u001b[39msentence_batch_size\n\u001b[1;32m    201\u001b[0m )\n\u001b[1;32m    203\u001b[0m ba_result \u001b[38;5;241m=\u001b[39m binary_search_citation(\n\u001b[1;32m    204\u001b[0m     highlight, article, ba_model, ba_tokenizer,\n\u001b[1;32m    205\u001b[0m     backward\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, query_direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreverse\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    206\u001b[0m     max_iterations\u001b[38;5;241m=\u001b[39mmax_iterations,\n\u001b[1;32m    207\u001b[0m     sentence_batch_size\u001b[38;5;241m=\u001b[39msentence_batch_size\n\u001b[1;32m    208\u001b[0m )\n",
      "File \u001b[0;32m/ocean/projects/cis250068p/jhwang4/idl-project/src/search_batch_v2.py:161\u001b[0m, in \u001b[0;36mbinary_search_citation\u001b[0;34m(highlight, article, model, tokenizer, backward, query_direction, max_iterations, sentence_batch_size)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m binary_search_recursive(mid \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, t, iteration \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# 이진 탐색 실행\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m citation, score, perplexity \u001b[38;5;241m=\u001b[39m \u001b[43mbinary_search_recursive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcitation\u001b[39m\u001b[38;5;124m'\u001b[39m: citation,\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m: score,\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperplexity\u001b[39m\u001b[38;5;124m'\u001b[39m: perplexity\n\u001b[1;32m    167\u001b[0m }\n",
      "File \u001b[0;32m/ocean/projects/cis250068p/jhwang4/idl-project/src/search_batch_v2.py:156\u001b[0m, in \u001b[0;36mbinary_search_citation.<locals>.binary_search_recursive\u001b[0;34m(s, t, iteration)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# 더 높은 점수를 가진 구간으로 재귀 호출\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scores[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m scores[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_search_recursive\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m binary_search_recursive(mid \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, t, iteration \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/ocean/projects/cis250068p/jhwang4/idl-project/src/search_batch_v2.py:147\u001b[0m, in \u001b[0;36mbinary_search_citation.<locals>.binary_search_recursive\u001b[0;34m(s, t, iteration)\u001b[0m\n\u001b[1;32m    144\u001b[0m right_ctx, right_tgt \u001b[38;5;241m=\u001b[39m make_ctx_tgt(right_span)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# 점수 계산: 원본과 동일한 방식으로 구간별 점수 계산\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m scores, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_scores_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mleft_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_ctx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mleft_tgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_tgt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackward\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# 점수가 없는 경우 처리\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m scores \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(scores) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m/ocean/projects/cis250068p/jhwang4/idl-project/src/utils_batch_v2.py:273\u001b[0m, in \u001b[0;36mcalculate_scores_batch\u001b[0;34m(contexts, targets, model, tokenizer, backward)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# 3) Forward once in FP16 for memory/speed\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m t\u001b[38;5;241m.\u001b[39mno_grad(), autocast():\n\u001b[0;32m--> 273\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m    274\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlog_softmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    276\u001b[0m \u001b[38;5;66;03m# 4) Extract each sequence’s avg log‑prob and perplexity\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:193\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    192\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[: \u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 193\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:212\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any\n\u001b[1;32m    211\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:118\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    116\u001b[0m         thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m thread \u001b[38;5;129;01min\u001b[39;00m threads:\n\u001b[0;32m--> 118\u001b[0m         \u001b[43mthread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     _worker(\u001b[38;5;241m0\u001b[39m, modules[\u001b[38;5;241m0\u001b[39m], inputs[\u001b[38;5;241m0\u001b[39m], kwargs_tup[\u001b[38;5;241m0\u001b[39m], devices[\u001b[38;5;241m0\u001b[39m], streams[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/opt/packages/AI/pytorch_23.02-1.13.1-py3/lib/python3.10/threading.py:1096\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1096\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/opt/packages/AI/pytorch_23.02-1.13.1-py3/lib/python3.10/threading.py:1116\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1117\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1118\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#binary_results = binary_search_attribution_search(dataset, fo_model, fo_tokenizer, ba_model, ba_tokenizer) # 50 - 25.7s\n",
    "binary_results = binary_search_attribution_search(dataset, fo_model, fo_tokenizer, ba_model, ba_tokenizer, max_iterations=30, sentence_batch_size=1000) # 35.7s, 7m 47.6s, 5m 15.6s, 5m0.8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                | 0/47852 [00:00<?, ?it/s]/ocean/projects/cis250068p/jhwang4/idl-project/src/utils_batch_v2.py:203: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with t.no_grad(), autocast():\n",
      "  0%|                                                                                     | 6/47852 [00:35<79:18:31,  5.97s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#exclusion_results = exclusion_search_attribution_search(dataset, fo_model, fo_tokenizer, ba_model, ba_tokenizer) # 50 - 4m54.5s\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m exclusion_results \u001b[38;5;241m=\u001b[39m \u001b[43mexclusion_search_attribution_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfo_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfo_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mba_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mba_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ocean/projects/cis250068p/jhwang4/idl-project/src/search_batch_v2.py:321\u001b[0m, in \u001b[0;36mexclusion_search_attribution_search\u001b[0;34m(dataset, fo_model, fo_tokenizer, ba_model, ba_tokenizer, sentence_batch_size)\u001b[0m\n\u001b[1;32m    318\u001b[0m article \u001b[38;5;241m=\u001b[39m example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# 세 가지 모델 설정으로 제외 탐색 실행\u001b[39;00m\n\u001b[0;32m--> 321\u001b[0m baseline_result \u001b[38;5;241m=\u001b[39m \u001b[43mexclusion_search_citation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhighlight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marticle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfo_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfo_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackward\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_direction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnormal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43msentence_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msentence_batch_size\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m fo_result \u001b[38;5;241m=\u001b[39m exclusion_search_citation(\n\u001b[1;32m    328\u001b[0m     highlight, article, fo_model, fo_tokenizer,\n\u001b[1;32m    329\u001b[0m     backward\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, query_direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreverse\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    330\u001b[0m     sentence_batch_size\u001b[38;5;241m=\u001b[39msentence_batch_size\n\u001b[1;32m    331\u001b[0m )\n\u001b[1;32m    333\u001b[0m ba_result \u001b[38;5;241m=\u001b[39m exclusion_search_citation(\n\u001b[1;32m    334\u001b[0m     highlight, article, ba_model, ba_tokenizer,\n\u001b[1;32m    335\u001b[0m     backward\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, query_direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreverse\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    336\u001b[0m     sentence_batch_size\u001b[38;5;241m=\u001b[39msentence_batch_size\n\u001b[1;32m    337\u001b[0m )\n",
      "File \u001b[0;32m/ocean/projects/cis250068p/jhwang4/idl-project/src/search_batch_v2.py:271\u001b[0m, in \u001b[0;36mexclusion_search_citation\u001b[0;34m(highlight, article, model, tokenizer, backward, query_direction, sentence_batch_size)\u001b[0m\n\u001b[1;32m    268\u001b[0m     indices\u001b[38;5;241m.\u001b[39mappend(j)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# 배치 점수 계산\u001b[39;00m\n\u001b[0;32m--> 271\u001b[0m scores, ppls \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_scores_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_ctxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_tgts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackward\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# 결과 저장\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, score, ppl \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(indices, scores, ppls):\n",
      "File \u001b[0;32m/ocean/projects/cis250068p/jhwang4/idl-project/src/utils_batch_v2.py:220\u001b[0m, in \u001b[0;36mcalculate_scores_batch\u001b[0;34m(contexts, targets, model, tokenizer, backward)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pos \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(ctx_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, ctx_len \u001b[38;5;241m+\u001b[39m tgt_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    219\u001b[0m     logits_i \u001b[38;5;241m=\u001b[39m logits[i, pos, :]\n\u001b[0;32m--> 220\u001b[0m     next_id  \u001b[38;5;241m=\u001b[39m \u001b[43minput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     lse      \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mlogsumexp(logits_i, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    222\u001b[0m     seq_lp  \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (logits_i[next_id] \u001b[38;5;241m-\u001b[39m lse)\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#exclusion_results = exclusion_search_attribution_search(dataset, fo_model, fo_tokenizer, ba_model, ba_tokenizer) # 50 - 4m54.5s\n",
    "exclusion_results = exclusion_search_attribution_search(dataset, fo_model, fo_tokenizer, ba_model, ba_tokenizer, sentence_batch_size=32) # 3m30.4s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_embedding_similarity(highlight, citation):\n",
    "    \"\"\"Calculate cosine similarity between sentence embeddings.\"\"\"\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Generate embeddings\n",
    "    highlight_embedding = model.encode([highlight])[0]\n",
    "    citation_embedding = model.encode([citation])[0]\n",
    "    \n",
    "    # Calculate cosine similarity (1 - cosine distance)\n",
    "    similarity = 1 - cosine(highlight_embedding, citation_embedding)\n",
    "    return similarity\n",
    "\n",
    "def calculate_rouge_score(highlight, citation):\n",
    "    \"\"\"Calculate ROUGE-L F-measure score.\"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(highlight, citation)\n",
    "    return scores['rougeL'].fmeasure\n",
    "\n",
    "def calculate_tfidf_score(highlight, citation):\n",
    "    \"\"\"Calculate TF-IDF similarity score.\"\"\"\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([highlight, citation])\n",
    "    \n",
    "    # Convert sparse matrix to dense array for cosine similarity calculation\n",
    "    dense_matrix = tfidf_matrix.toarray()\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity = 1 - cosine(dense_matrix[0], dense_matrix[1])\n",
    "    return similarity\n",
    "\n",
    "def process_data(data):\n",
    "    \"\"\"Process the data and reformat to show scores by citation type in columns.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for item in data:\n",
    "        highlight = item['highlight']\n",
    "        result_entry = {'id': item['id'], 'highlight': highlight}\n",
    "        \n",
    "        # Process each citation type\n",
    "        for citation_type in ['base_citation', 'ba_citation', 'fo_citation']:\n",
    "            prefix = citation_type.split('_')[0]  # Extract 'base', 'ba', or 'fo'\n",
    "            \n",
    "            if citation_type in item and item[citation_type]:\n",
    "                citation = item[citation_type]\n",
    "                \n",
    "                # Calculate scores\n",
    "                emb_similarity = calculate_embedding_similarity(highlight, citation)\n",
    "                rouge_score = calculate_rouge_score(highlight, citation)\n",
    "                tfidf_score = calculate_tfidf_score(highlight, citation)\n",
    "                \n",
    "                # Add scores as columns with prefix\n",
    "                result_entry[f'{prefix}_emb_similarity'] = emb_similarity\n",
    "                result_entry[f'{prefix}_rouge_score'] = rouge_score\n",
    "                result_entry[f'{prefix}_tfidf_score'] = tfidf_score\n",
    "                \n",
    "            else:\n",
    "                # Set default values if citation doesn't exist\n",
    "                result_entry[f'{prefix}_emb_similarity'] = None\n",
    "                result_entry[f'{prefix}_rouge_score'] = None\n",
    "                result_entry[f'{prefix}_tfidf_score'] = None\n",
    "                \n",
    "        results.append(result_entry)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Process the data\n",
    "linear_final_results = process_data(linear_results)\n",
    "binary_final_results = process_data(binary_results)\n",
    "#exclusion_final_results = process_data(exclusion_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_MODEL    = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "ROUGE_SCORER = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "TFIDF_VEC    = TfidfVectorizer(\n",
    "    token_pattern=r\"(?u)\\b\\w+\\b\",  # 좀 더 느슨한 토큰화\n",
    "    stop_words=None               # 불용어 필터링 끄기\n",
    ")\n",
    "\n",
    "def calculate_embedding_similarity(highlight, citation):\n",
    "    he = EMB_MODEL.encode([highlight])[0]\n",
    "    ce = EMB_MODEL.encode([citation])[0]\n",
    "    return 1 - cosine(he, ce)\n",
    "\n",
    "def calculate_rouge_score(highlight, citation):\n",
    "    return ROUGE_SCORER.score(highlight, citation)['rougeL'].fmeasure\n",
    "\n",
    "def calculate_tfidf_score(highlight, citation):\n",
    "    try:\n",
    "        mat = TFIDF_VEC.fit_transform([highlight, citation]).toarray()\n",
    "        return 1 - cosine(mat[0], mat[1])\n",
    "    except ValueError:\n",
    "        # 어휘가 하나도 없으면 0.0\n",
    "        return 0.0\n",
    "\n",
    "def process_data(data):\n",
    "    results = []\n",
    "    for item in data:\n",
    "        hl = item['highlight']\n",
    "        rec = {'id': item['id'], 'highlight': hl}\n",
    "        for citation_type in ['base_citation','fo_citation','ba_citation']:\n",
    "            prefix = citation_type.split('_')[0]\n",
    "            cit = item.get(citation_type, None)\n",
    "            if cit:\n",
    "                rec[f'{prefix}_emb_similarity'] = calculate_embedding_similarity(hl, cit)\n",
    "                rec[f'{prefix}_rouge_score']     = calculate_rouge_score(hl, cit)\n",
    "                rec[f'{prefix}_tfidf_score']     = calculate_tfidf_score(hl, cit)\n",
    "            else:\n",
    "                rec[f'{prefix}_emb_similarity'] = None\n",
    "                rec[f'{prefix}_rouge_score']     = None\n",
    "                rec[f'{prefix}_tfidf_score']     = None\n",
    "        results.append(rec)\n",
    "    return results\n",
    "\n",
    "#linear_final_results    = process_data(linear_results)\n",
    "#binary_final_results    = process_data(binary_results)\n",
    "exclusion_final_results = process_data(exclusion_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pd.DataFrame(linear_final_results) \n",
    "r.drop(['id', 'highlight'], axis=1).mean()\n",
    "r.to_csv(\"/ocean/projects/cis250068p/jhwang4/idl-project/notebooks/linear_result_500.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "base_emb_similarity   0.4234\n",
       "base_rouge_score      0.1674\n",
       "base_tfidf_score      0.1590\n",
       "fo_emb_similarity     0.6176\n",
       "fo_rouge_score        0.3265\n",
       "fo_tfidf_score        0.2921\n",
       "ba_emb_similarity     0.6285\n",
       "ba_rouge_score        0.3406\n",
       "ba_tfidf_score        0.3070\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = pd.DataFrame(binary_final_results) \n",
    "r.drop(['id', 'highlight'], axis=1).mean()\n",
    "#r.to_csv(\"binary_500_results.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "base_emb_similarity   0.3841\n",
       "base_rouge_score      0.1555\n",
       "base_tfidf_score      0.1545\n",
       "fo_emb_similarity     0.5921\n",
       "fo_rouge_score        0.3252\n",
       "fo_tfidf_score        0.2929\n",
       "ba_emb_similarity     0.6068\n",
       "ba_rouge_score        0.3247\n",
       "ba_tfidf_score        0.2893\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = pd.DataFrame(exclusion_final_results) \n",
    "r.drop(['id', 'highlight'], axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Base_linear</th>\n",
       "      <th>Fo_linear</th>\n",
       "      <th>Ba_linear</th>\n",
       "      <th>Base_binary</th>\n",
       "      <th>Fo_binary</th>\n",
       "      <th>Ba_binary</th>\n",
       "      <th>Base_exclusion</th>\n",
       "      <th>Fo_exclusion</th>\n",
       "      <th>Ba_exclusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Embedding</th>\n",
       "      <td>0.3962</td>\n",
       "      <td>0.6088</td>\n",
       "      <td>0.6060</td>\n",
       "      <td>0.4320</td>\n",
       "      <td>0.6165</td>\n",
       "      <td>0.6287</td>\n",
       "      <td>0.3695</td>\n",
       "      <td>0.5898</td>\n",
       "      <td>0.6187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rouge</th>\n",
       "      <td>0.1727</td>\n",
       "      <td>0.2888</td>\n",
       "      <td>0.3040</td>\n",
       "      <td>0.1759</td>\n",
       "      <td>0.3384</td>\n",
       "      <td>0.3401</td>\n",
       "      <td>0.1505</td>\n",
       "      <td>0.3306</td>\n",
       "      <td>0.3371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf</th>\n",
       "      <td>0.1586</td>\n",
       "      <td>0.2646</td>\n",
       "      <td>0.2646</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.3067</td>\n",
       "      <td>0.2975</td>\n",
       "      <td>0.1419</td>\n",
       "      <td>0.2926</td>\n",
       "      <td>0.2943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Base_linear  Fo_linear  Ba_linear  Base_binary  Fo_binary  \\\n",
       "Embedding       0.3962     0.6088     0.6060       0.4320     0.6165   \n",
       "Rouge           0.1727     0.2888     0.3040       0.1759     0.3384   \n",
       "Tfidf           0.1586     0.2646     0.2646       0.1622     0.3067   \n",
       "\n",
       "           Ba_binary  Base_exclusion  Fo_exclusion  Ba_exclusion  \n",
       "Embedding     0.6287          0.3695        0.5898        0.6187  \n",
       "Rouge         0.3401          0.1505        0.3306        0.3371  \n",
       "Tfidf         0.2975          0.1419        0.2926        0.2943  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_linear = pd.DataFrame(linear_final_results)\n",
    "df_binary = pd.DataFrame(binary_final_results)\n",
    "df_exclusion = pd.DataFrame(exclusion_final_results)\n",
    "\n",
    "df_merged = pd.merge(df_linear, df_binary, on=['id', 'highlight'],suffixes=('_linear', '_binary'))\n",
    "\n",
    "df_exclusion = df_exclusion.add_suffix('_exclusion')\n",
    "df_exclusion = df_exclusion.rename(columns={'id_exclusion': 'id', 'highlight_exclusion': 'highlight'})\n",
    "\n",
    "df_merged = pd.merge(df_merged, df_exclusion, on=['id', 'highlight'])\n",
    "\n",
    "mean_series = df_merged.drop(['id', 'highlight'], axis=1).mean()\n",
    "\n",
    "data = {\n",
    "    'Base_linear': [\n",
    "        mean_series['base_emb_similarity_linear'],\n",
    "        mean_series['base_rouge_score_linear'],\n",
    "        mean_series['base_tfidf_score_linear']\n",
    "    ],\n",
    "    'Fo_linear': [\n",
    "        mean_series['fo_emb_similarity_linear'],\n",
    "        mean_series['fo_rouge_score_linear'],\n",
    "        mean_series['fo_tfidf_score_linear']\n",
    "    ],\n",
    "    'Ba_linear': [\n",
    "        mean_series['ba_emb_similarity_linear'],\n",
    "        mean_series['ba_rouge_score_linear'],\n",
    "        mean_series['ba_tfidf_score_linear']\n",
    "    ],\n",
    "    'Base_binary': [\n",
    "        mean_series['base_emb_similarity_binary'],\n",
    "        mean_series['base_rouge_score_binary'],\n",
    "        mean_series['base_tfidf_score_binary']\n",
    "    ],\n",
    "    'Fo_binary': [\n",
    "        mean_series['fo_emb_similarity_binary'],\n",
    "        mean_series['fo_rouge_score_binary'],\n",
    "        mean_series['fo_tfidf_score_binary']\n",
    "    ],\n",
    "    'Ba_binary': [\n",
    "        mean_series['ba_emb_similarity_binary'],\n",
    "        mean_series['ba_rouge_score_binary'],\n",
    "        mean_series['ba_tfidf_score_binary']\n",
    "    ],\n",
    "    'Base_exclusion': [\n",
    "        mean_series['base_emb_similarity_exclusion'],\n",
    "        mean_series['base_rouge_score_exclusion'],\n",
    "        mean_series['base_tfidf_score_exclusion']\n",
    "    ],\n",
    "    'Fo_exclusion': [\n",
    "        mean_series['fo_emb_similarity_exclusion'],\n",
    "        mean_series['fo_rouge_score_exclusion'],\n",
    "        mean_series['fo_tfidf_score_exclusion']\n",
    "    ],\n",
    "    'Ba_exclusion': [\n",
    "        mean_series['ba_emb_similarity_exclusion'],\n",
    "        mean_series['ba_rouge_score_exclusion'],\n",
    "        mean_series['ba_tfidf_score_exclusion']\n",
    "    ],\n",
    "}\n",
    "\n",
    "table_df = pd.DataFrame(data, index=['Embedding', 'Rouge', 'Tfidf'])\n",
    "table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_linear = pd.DataFrame(linear_final_results)\n",
    "mean_series = df_linear.drop(['id', 'highlight'], axis=1).mean()\n",
    "\n",
    "data = {\n",
    "    'Base_linear': [\n",
    "        mean_series['base_emb_similarity'],\n",
    "        mean_series['base_rouge_score'],\n",
    "        mean_series['base_tfidf_score']\n",
    "    ],\n",
    "    'Fo_linear': [\n",
    "        mean_series['fo_emb_similarity'],\n",
    "        mean_series['fo_rouge_score'],\n",
    "        mean_series['fo_tfidf_score']\n",
    "    ],\n",
    "    'Ba_linear': [\n",
    "        mean_series['ba_emb_similarity'],\n",
    "        mean_series['ba_rouge_score'],\n",
    "        mean_series['ba_tfidf_score']\n",
    "    ]\n",
    "}\n",
    "\n",
    "table_df = pd.DataFrame(data, index=['Embedding', 'Rouge', 'Tfidf'])\n",
    "table_df.to_csv(\"/ocean/projects/cis250068p/jhwang4/idl-project/notebooks/linear_result.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Base_linear</th>\n",
       "      <th>Fo_linear</th>\n",
       "      <th>Ba_linear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Embedding</th>\n",
       "      <td>0.4792</td>\n",
       "      <td>0.6525</td>\n",
       "      <td>0.6494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rouge</th>\n",
       "      <td>0.2208</td>\n",
       "      <td>0.3357</td>\n",
       "      <td>0.3419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf</th>\n",
       "      <td>0.1988</td>\n",
       "      <td>0.2957</td>\n",
       "      <td>0.3028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Base_linear  Fo_linear  Ba_linear\n",
       "Embedding       0.4792     0.6525     0.6494\n",
       "Rouge           0.2208     0.3357     0.3419\n",
       "Tfidf           0.1988     0.2957     0.3028"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_df.to_csv(\"/ocean/projects/cis250068p/jhwang4/idl-project/notebooks/linear_result.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
