{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune with full scale dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append paths for the src folder\n",
    "import sys\n",
    "import os \n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.getcwd(), 'idl-project')))\n",
    "\n",
    "# Additional imports \n",
    "from src.utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from src.model import load_fo_model\n",
    "from src.data import load_flan_dataset, load_summarization_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from peft import (LoraConfig, PeftModel, get_peft_model,\n",
    "                  prepare_model_for_kbit_training)\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (AutoTokenizer, BitsAndBytesConfig,\n",
    "                          DataCollatorForLanguageModeling, GPTNeoXForCausalLM,\n",
    "                          Trainer, TrainingArguments, TrainerCallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Working in '/ocean/projects/cis250068p/iwiryadi/idl-project'\n",
      "✓ Directory contains 'idl-project'\n"
     ]
    }
   ],
   "source": [
    "current_dir = os.path.abspath(os.getcwd())\n",
    "\n",
    "# Check if 'idl-project' is in the path\n",
    "if 'idl-project' not in current_dir:\n",
    "    raise Exception(\"Current directory '{current_dir}' is not within 'idl-project'\")\n",
    "\n",
    "print(f\"✓ Working in '{current_dir}'\")\n",
    "print(f\"✓ Directory contains 'idl-project'\")\n",
    "\n",
    "OUTPUT_DIR = \"output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.finetune_lora_config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'full-tuning-10-more-data-twice'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RUN_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/ocean/projects/cis250068p/shared/caches'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CACHE_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miwiryadi\u001b[0m (\u001b[33midl-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/ocean/projects/cis250068p/iwiryadi/idl-project/wandb/run-20250418_121511-w3nm6epq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/11785_finetuning/ivan-testing-team/runs/w3nm6epq' target=\"_blank\">full-tuning-10-more-data-twice</a></strong> to <a href='https://wandb.ai/11785_finetuning/ivan-testing-team' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/11785_finetuning/ivan-testing-team' target=\"_blank\">https://wandb.ai/11785_finetuning/ivan-testing-team</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/11785_finetuning/ivan-testing-team/runs/w3nm6epq' target=\"_blank\">https://wandb.ai/11785_finetuning/ivan-testing-team/runs/w3nm6epq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['/ocean/projects/cis250068p/iwiryadi/idl-project/wandb/run-20250418_121511-w3nm6epq/files/notebooks/finetune_lora_config.py']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(entity=\"11785_finetuning\", project='ivan-testing-team', name=RUN_NAME, reinit=True)\n",
    "wandb.save(\"notebooks/finetune_lora_config.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA H100 80GB HBM3\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test prompts\n",
    "test_prompts = [\n",
    "    \"Who is Barack Obama?\",\n",
    "    \"What is Carnegie Mellon University?\",\n",
    "    \"Classify this restaurant review sentiment: 'The food was absolutely delicious but the service was extremely slow and the waiter seemed uninterested in helping us.'\",\n",
    "    \"Compare and contrast Carnegie Mellon University's Computer Science and Information Systems programs in terms of research focus and career outcomes.\",\n",
    "    \"Summarize in one sentence: Dr. Sarah Chen, lead scientist on the mission, called it 'the most significant discovery in the history of space exploration.' The finding suggests that Mars once had a much more hospitable environment with liquid water and possibly a thicker atmosphere. The agency plans to send a sample return mission within the next five years to bring these fossils back to Earth for more detailed analysis. This discovery has profound implications for our understanding of how life might develop throughout the universe.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de753a0980b24a16a401b80c7b268d25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing niv2_zsopt_data...\n",
      "Downloading data to /ocean/projects/cis250068p/shared/caches\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e20274f194d4a7aabcb60b01fb7851c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cef19a3037964fa5a2300885278d3076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully: niv2_zsopt_data\n",
      "Original niv2_zsopt_data size: 5030900\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d239811ede654b42b1de66557157891a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=20):   0%|          | 0/5030900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found summarization examples in niv2_zsopt_data\n",
      "Processing flan_zsopt_data...\n",
      "Downloading data to /ocean/projects/cis250068p/shared/caches\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58eedbf2ddc145a0b6f045fcab901152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a251b23dcff4d159222eff6a73c456b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b24ab7d07e14978a90db57f2c0cb0b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully: flan_zsopt_data\n",
      "Original flan_zsopt_data size: 38970972\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f384a4499c70498fb73841e6e9439c18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=20):   0%|          | 0/38970972 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found summarization examples in flan_zsopt_data\n",
      "Processing t0_zsopt_data...\n",
      "Downloading data to /ocean/projects/cis250068p/shared/caches\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23637e2dba68442ea9285405c8ab932d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae8b8d500a94b98ac1971755b0e78cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9277d7a06cb4eb6a6977276a11bd364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully: t0_zsopt_data\n",
      "Original t0_zsopt_data size: 41652381\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b82aee95cf844fb084f7abc55f144783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=20):   0%|          | 0/41652381 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found summarization examples in t0_zsopt_data\n",
      "Processing cot_zsopt_data...\n",
      "Downloading data to /ocean/projects/cis250068p/shared/caches\n",
      "Dataset loaded successfully: cot_zsopt_data\n",
      "Original cot_zsopt_data size: 95570\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5367830c47e42c29277ee651f03745c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=20):   0%|          | 0/95570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found summarization examples in cot_zsopt_data\n",
      "Processing dialog_zsopt_data...\n",
      "Downloading data to /ocean/projects/cis250068p/shared/caches\n",
      "Dataset loaded successfully: dialog_zsopt_data\n",
      "Original dialog_zsopt_data size: 2715160\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaaf52b10f5e4480a45f2a4ec5db5534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=20):   0%|          | 0/2715160 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found summarization examples in dialog_zsopt_data\n",
      "Combined dataset with 92303 summarization examples\n"
     ]
    }
   ],
   "source": [
    "# dataset = load_summarization_datasets(subset_names=['niv2_zsopt_data', 'cot_zsopt_data', 'dialog_zsopt_data'])\n",
    "dataset = load_summarization_datasets(\n",
    "    subset_names=FLAN_SUBSET, subset_frac=SUBSET_FRAC, p=TASK_DIVERSITY_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "task510_reddit_tifu_title_summarization\n",
      "================================================================================\n",
      "1\n",
      "task618_amazonreview_summary_text_generation\n",
      "================================================================================\n",
      "2\n",
      "task1579_gigaword_incorrect_summarization\n",
      "================================================================================\n",
      "3\n",
      "task1540_parsed_pdfs_summarization\n",
      "================================================================================\n",
      "4\n",
      "task590_amazonfood_summary_correction_classification\n",
      "================================================================================\n",
      "5\n",
      "task1658_billsum_summarization\n",
      "================================================================================\n",
      "6\n",
      "task1290_xsum_summarization\n",
      "================================================================================\n",
      "7\n",
      "task618_amazonreview_summary_text_generation\n",
      "================================================================================\n",
      "8\n",
      "task510_reddit_tifu_title_summarization\n",
      "================================================================================\n",
      "9\n",
      "task511_reddit_tifu_long_text_summarization\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(i)\n",
    "    # print(dataset[i]['inputs'])\n",
    "    # print(dataset[i]['targets'])\n",
    "    print(dataset[i]['_task_name'])\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Instructions: In this task, you are given a Reddit post as a text. Your task is to generate a short summary for this text. The summary must include a situation which caused humor. The summary should be one or two sentences long.\\nInput: Text: tifu (it was actually a few weeks ago) and i can never see my favorite friend or beverage in the same way.\\ni was just hanging out with a friend - she's really awesome and one of my favorite people to spend time with, so we were having a great time playing pokémon stadium on her n64. she's my type of girl, so there is an endless stream of dr. pepper filling our cups which we kept on the floor by our feet. \\ni take my last drink of the night, not knowing that it would change my life forever.\\ni notice something solid enter my mouth. at this point, apparently, i am optimistic. i use my tongue to move the object around and attempt to identify it by pressing it against my teeth, cheek, etc. is it a mostly melted ice cube? maybe some sort of condensed flavor syrup? had i swallowed indiscriminately, i would have probably brushed it off as nothing and moved on.\\nbut that isn't what i did. that isn't what i did at all...\\nmy curiosity quickly transforms to shock and disgust after i spit the object back into my cup. the reality stares back at me from the bottom of my almost empty glass. the only thing i can see is a cockroach, officially the only roach that has ever been in my mouth!!!\\ni couldn't fully accept my situation, so it took about a minute of blinking in disbelief at my new, little friend to realize this was real life. my state of shock lasted long enough that i couldn't even fully freak out when i finally accepted my unfortunate circumstances.\\ni told my friend what happened and it basically crushed her soul, though. she felt terrible, as though she put the roach there herself, and apologized about 100 times. there was no salvaging the night, so i went home and reevaluated my entire life.\\nall i can think about now, when i go over there, is that little guy and all the areas of my mouth that he touched... and his family that have the potential to follow in his footsteps. i have never felt as comfortable in that friend's house since my roach experience.\\nto my knowledge, swallowing a roach isn't any worse for you than putting one in your mouth and spitting it out. if i had just swallowed him without investigation, i would be so much happier right now.\\nOutput:\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[i]['inputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'left my soda on the floor, last drink had a roach in it, said roach basically ruined my life.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[i]['targets']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  LORA\n",
    "\n",
    "### Configure BitsAndBytes for 4-bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure BitsAndBytes for 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",           # Use 4-bit NormalFloat quantization\n",
    "    bnb_4bit_use_double_quant=True,      # Use double quantization for additional memory savings\n",
    "    bnb_4bit_compute_dtype=torch.float32  # Compute in float32 (can also use torch.bfloat16 if available)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID       = \"EleutherAI/pythia-160m-deduped\"\n",
    "MODEL_REVISION = \"step143000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    revision=MODEL_REVISION,\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "\n",
    "# Ensure the tokenizer has padding token set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "if FT_TYPE == 'full':\n",
    "    model = GPTNeoXForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        revision=MODEL_REVISION,\n",
    "        cache_dir=CACHE_DIR,\n",
    "        # quantization_config=bnb_config,\n",
    "        device_map=\"auto\"  # Automatically distribute layers across available GPUs\n",
    "    )\n",
    "else:\n",
    "    # Load the model with quantization\n",
    "    model = GPTNeoXForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        revision=MODEL_REVISION,\n",
    "        cache_dir=CACHE_DIR,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\"  # Automatically distribute layers across available GPUs\n",
    "    )\n",
    "\n",
    "# Prepare the model for k-bit training\n",
    "# model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print trainable parameters information\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FT_TYPE == 'lora': \n",
    "    # Define the LoRA configuration\n",
    "    # For Pythia models, the target module is \"query_key_value\" for attention layers\n",
    "    lora_config = LoraConfig(\n",
    "        r=LORA_RANK,                    # Rank dimension\n",
    "        lora_alpha=LORA_ALPHA,          # LoRA scaling factor\n",
    "        target_modules=TARGET_MODULES,  # Target specific attention modules\n",
    "        lora_dropout=LORA_DROPOUT,      # Dropout probability for LoRA layers\n",
    "        bias=\"none\",            # Don't apply LoRA to bias terms\n",
    "        task_type=\"CAUSAL_LM\"   # Task type for causal language modeling\n",
    "    )\n",
    "\n",
    "    # Apply LoRA to the model\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Dataset Formatting and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format the dataset for causal language modeling\n",
    "def format_for_clm(examples):\n",
    "    # Format as: \"Instruction: {instruction} Input: {input} Output: {output}\"\n",
    "    # Adjust this format based on your specific dataset structure\n",
    "    if 'inputs' in examples and 'targets' in examples:\n",
    "        texts = [\n",
    "            f\"{inp}\\n{target}{tokenizer.eos_token}\"\n",
    "            for inp, target in zip(examples['inputs'], examples['targets'])\n",
    "        ]\n",
    "    else:\n",
    "        # Fallback for other dataset structures\n",
    "        texts = examples['text'] if 'text' in examples else []\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Apply formatting \n",
    "if isinstance(dataset, Dataset):\n",
    "    # For non-streaming datasets\n",
    "    if 'inputs' in dataset.column_names and 'targets' in dataset.column_names:\n",
    "        dataset = dataset.map(format_for_clm, batched=True, num_proc=16)\n",
    "else:\n",
    "    # For streaming datasets, we need to format each example as it comes\n",
    "    dataset = dataset.map(lambda example: {\n",
    "        'text': f\"Instruction: {example['inputs']}\\nOutput: {example['targets']}\" \n",
    "        if 'inputs' in example and 'targets' in example \n",
    "        else example.get('text', '')\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize function for the dataset\n",
    "def tokenize_function(example):\n",
    "    # Handle single examples for streaming datasets\n",
    "    text = example[\"text\"] if \"text\" in example else \"\"\n",
    "    \n",
    "    # Tokenize with padding and truncation\n",
    "    outputs = tokenizer(\n",
    "        text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=1024,  # Adjust based on your needs and GPU memory\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Remove the batch dimension for single examples\n",
    "    for key in outputs:\n",
    "        if isinstance(outputs[key], torch.Tensor) and outputs[key].ndim > 1:\n",
    "            outputs[key] = outputs[key].squeeze(0)\n",
    "    \n",
    "    # Set labels equal to input_ids for causal language modeling\n",
    "    outputs[\"labels\"] = outputs[\"input_ids\"].clone()\n",
    "    return outputs\n",
    "\n",
    "# Apply tokenization to the dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, num_proc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = tokenized_dataset.train_test_split(test_size=TEST_SIZE)\n",
    "train_dataset = split_dataset['train']\n",
    "val_dataset   = split_dataset['test']  # Note: called 'test' by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Since we're using streaming datasets, convert to list for training\n",
    "# # This is needed because Trainer expects a non-streaming dataset\n",
    "# # We'll create a buffer of examples for training\n",
    "# buffer_size = 25000  # Adjust based on your memory constraints\n",
    "# tokenized_examples = []\n",
    "# for example in tqdm(tokenized_dataset, total=buffer_size):\n",
    "#     tokenized_examples.append(example)\n",
    "#     if len(tokenized_examples) >= buffer_size:\n",
    "#         break\n",
    "\n",
    "# print(f\"Collected {len(tokenized_examples)} examples for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert to Dataset object for training\n",
    "# from datasets import Dataset as HFDataset\n",
    "# train_dataset = HFDataset.from_list(tokenized_examples)\n",
    "\n",
    "# print(f\"Training dataset created with columns: {train_dataset.column_names}\")\n",
    "# print(f\"Number of examples: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation test callback\n",
    "class GenerationTestCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Callback to generate text samples at evaluation steps.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, test_prompts, max_length=100, do_sample=True, \n",
    "                 num_beams=2, temperature=0.1, device=\"cuda\"):\n",
    "        \"\"\"\n",
    "        Initialize with tokenizer and test prompts.\n",
    "        \"\"\"\n",
    "        self.tokenizer    = tokenizer\n",
    "        self.test_prompts = test_prompts\n",
    "        self.device       = device\n",
    "        self.max_length   = max_length\n",
    "        self.do_sample    = do_sample\n",
    "        self.num_beams    = num_beams\n",
    "        self.temperature  = temperature\n",
    "            \n",
    "            \n",
    "    def on_evaluate(self, args, state, control, model, **kwargs):\n",
    "        \"\"\"\n",
    "        Run after each evaluation to generate two types of samples:\n",
    "        1. Free-form completion with sampling\n",
    "        2. Greedy decoding for deterministic output\n",
    "        \"\"\"\n",
    "        print(\"GenerationTestCallback\")\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"Generating samples at step {state.global_step}:\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for prompt in self.test_prompts:\n",
    "                inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "                \n",
    "                # 1. Generate with sampling\n",
    "                sample_output_ids = model.generate(\n",
    "                    inputs[\"input_ids\"],\n",
    "                    attention_mask    = inputs[\"attention_mask\"],\n",
    "                    max_length        = self.max_length,\n",
    "                    do_sample         = True,\n",
    "                    temperature       = self.temperature,\n",
    "                    pad_token_id      = self.tokenizer.eos_token_id,\n",
    "                    # eos_token_id      = self.tokenizer.eos_token_id\n",
    "                )\n",
    "                \n",
    "                # 2. Generate with greedy decoding (deterministic) and no sampling\n",
    "                greedy_output_ids = model.generate(\n",
    "                    inputs[\"input_ids\"],\n",
    "                    attention_mask    = inputs[\"attention_mask\"],\n",
    "                    max_length        = self.max_length,\n",
    "                    do_sample         = True,\n",
    "                    pad_token_id      = self.tokenizer.eos_token_id,\n",
    "                    # pad_token_id      = self.tokenizer.pad_token_id,\n",
    "                    # eos_token_id      = self.tokenizer.eos_token_id\n",
    "                )\n",
    "                \n",
    "                # Decode both outputs\n",
    "                sampled_text = self.tokenizer.decode(sample_output_ids[0], skip_special_tokens=True)\n",
    "                greedy_text  = self.tokenizer.decode(greedy_output_ids[0], skip_special_tokens=True)\n",
    "                \n",
    "                # Print the results\n",
    "                print(f\"\\nPrompt: {prompt}\")\n",
    "                print(f\"Greedy: {greedy_text}\")\n",
    "                print(\"-\"*50)\n",
    "                \n",
    "                # Log to W&B if you're using it\n",
    "                if args.report_to == \"wandb\":\n",
    "                    import wandb\n",
    "                    wandb.log({\n",
    "                        f\"generation/{prompt}/sampled\": wandb.Html(\n",
    "                            f\"<b>Step {state.global_step}</b><br>\"\n",
    "                            f\"<p><b>Prompt:</b> {prompt}</p>\"\n",
    "                            f\"<p><b>Sampled:</b> {sampled_text}</p>\"\n",
    "                        ),\n",
    "                        f\"generation/{prompt}/greedy\": wandb.Html(\n",
    "                            f\"<b>Step {state.global_step}</b><br>\"\n",
    "                            f\"<p><b>Prompt:</b> {prompt}</p>\"\n",
    "                            f\"<p><b>Greedy:</b> {greedy_text}</p>\"\n",
    "                        )\n",
    "                    }, step=state.global_step)\n",
    "        \n",
    "        return control\n",
    "\n",
    "# Create generation callback\n",
    "generation_callback = GenerationTestCallback(\n",
    "    tokenizer     = tokenizer,\n",
    "    test_prompts  = test_prompts,\n",
    "    max_length    = 250,\n",
    "    num_beams     = 3,\n",
    "    temperature   = 0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OUTPUT_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 19\u001b[0m\n\u001b[1;32m      3\u001b[0m val_dataset\u001b[38;5;241m.\u001b[39mset_format(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Create training arguments with parameters\u001b[39;00m\n\u001b[1;32m      6\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      7\u001b[0m     per_device_train_batch_size  \u001b[38;5;241m=\u001b[39m BATCH_SIZE,\n\u001b[1;32m      8\u001b[0m     per_device_eval_batch_size   \u001b[38;5;241m=\u001b[39m BATCH_SIZE,\n\u001b[1;32m      9\u001b[0m     gradient_accumulation_steps  \u001b[38;5;241m=\u001b[39m GRADIENT_ACCUMULATION_STEPS,\n\u001b[1;32m     10\u001b[0m     warmup_ratio                 \u001b[38;5;241m=\u001b[39m WARMUP_RATIO,\n\u001b[1;32m     11\u001b[0m     num_train_epochs             \u001b[38;5;241m=\u001b[39m NUM_EPOCHS,\n\u001b[1;32m     12\u001b[0m     learning_rate                \u001b[38;5;241m=\u001b[39m LEARNING_RATE,\n\u001b[1;32m     13\u001b[0m     lr_scheduler_type            \u001b[38;5;241m=\u001b[39m LR_SCHEDULER_TYPE,\n\u001b[1;32m     14\u001b[0m     fp16                         \u001b[38;5;241m=\u001b[39m FP16,\n\u001b[1;32m     15\u001b[0m     logging_steps                \u001b[38;5;241m=\u001b[39m LOGGING_STEPS,\n\u001b[1;32m     16\u001b[0m     save_steps                   \u001b[38;5;241m=\u001b[39m SAVE_STEPS,\n\u001b[1;32m     17\u001b[0m     eval_strategy                \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m     eval_steps                   \u001b[38;5;241m=\u001b[39m EVAL_STEPS,\n\u001b[0;32m---> 19\u001b[0m     output_dir                   \u001b[38;5;241m=\u001b[39m \u001b[43mOUTPUT_DIR\u001b[49m,\n\u001b[1;32m     20\u001b[0m     optim                        \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpaged_adamw_8bit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     21\u001b[0m     save_total_limit             \u001b[38;5;241m=\u001b[39m SAVE_TOTAL_LIMIT,\n\u001b[1;32m     22\u001b[0m     \n\u001b[1;32m     23\u001b[0m     report_to                    \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwandb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     24\u001b[0m     weight_decay                 \u001b[38;5;241m=\u001b[39m WEIGHT_DECAY,\n\u001b[1;32m     25\u001b[0m     \n\u001b[1;32m     26\u001b[0m     \n\u001b[1;32m     27\u001b[0m     logging_first_step           \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,  \n\u001b[1;32m     28\u001b[0m     max_grad_norm                \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m     29\u001b[0m     dataloader_num_workers       \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     30\u001b[0m     \n\u001b[1;32m     31\u001b[0m     load_best_model_at_end       \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# metric_for_best_model        = \"eval_loss\",\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# greater_is_better            = False,\u001b[39;00m\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Set up the trainer with validation\u001b[39;00m\n\u001b[1;32m     38\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     39\u001b[0m     model                     \u001b[38;5;241m=\u001b[39m model,\n\u001b[1;32m     40\u001b[0m     args                      \u001b[38;5;241m=\u001b[39m training_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m \n\u001b[1;32m     47\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'OUTPUT_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "# Set format to PyTorch\n",
    "train_dataset.set_format(type=\"torch\")\n",
    "val_dataset.set_format(type=\"torch\")\n",
    "\n",
    "# Create training arguments with parameters\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size  = BATCH_SIZE,\n",
    "    per_device_eval_batch_size   = BATCH_SIZE,\n",
    "    gradient_accumulation_steps  = GRADIENT_ACCUMULATION_STEPS,\n",
    "    warmup_ratio                 = WARMUP_RATIO,\n",
    "    num_train_epochs             = NUM_EPOCHS,\n",
    "    learning_rate                = LEARNING_RATE,\n",
    "    lr_scheduler_type            = LR_SCHEDULER_TYPE,\n",
    "    fp16                         = FP16,\n",
    "    logging_steps                = LOGGING_STEPS,\n",
    "    save_steps                   = SAVE_STEPS,\n",
    "    eval_strategy                = \"steps\",\n",
    "    eval_steps                   = EVAL_STEPS,\n",
    "    output_dir                   = OUTPUT_DIR,\n",
    "    optim                        = \"paged_adamw_8bit\", \n",
    "    save_total_limit             = SAVE_TOTAL_LIMIT,\n",
    "    \n",
    "    report_to                    = \"wandb\",\n",
    "    weight_decay                 = WEIGHT_DECAY,\n",
    "    \n",
    "    \n",
    "    logging_first_step           = True,  \n",
    "    max_grad_norm                = 1.0,\n",
    "    dataloader_num_workers       = 4,\n",
    "    \n",
    "    load_best_model_at_end       = True,\n",
    "    # metric_for_best_model        = \"eval_loss\",\n",
    "    # greater_is_better            = False,\n",
    ")\n",
    "\n",
    "\n",
    "# Set up the trainer with validation\n",
    "trainer = Trainer(\n",
    "    model                     = model,\n",
    "    args                      = training_args,\n",
    "    train_dataset             = train_dataset,\n",
    "    eval_dataset              = val_dataset,\n",
    "    data_collator             = DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    compute_metrics           = None,  \n",
    "    callbacks                 = [generation_callback],  \n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "# Disable caching during training to avoid memory issues\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "model.save_pretrained(f\"{OUTPUT_DIR}/{RUN_NAME}\")\n",
    "tokenizer.save_pretrained(f\"{OUTPUT_DIR}/{RUN_NAME}\")\n",
    "\n",
    "print(\"Training complete and model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_test_model():\n",
    "    print(\"\\nTesting fine-tuned model:\")\n",
    "    \n",
    "    # Load the base model and LoRA adapter\n",
    "    # base_model = GPTNeoXForCausalLM.from_pretrained(\n",
    "    #     MODEL_ID,\n",
    "    #     revision=MODEL_REVISION,\n",
    "    #     cache_dir=CACHE_DIR,\n",
    "    #     device_map=\"auto\"\n",
    "    # )\n",
    "    \n",
    "    # # Load and apply the fine-tuned LoRA weights\n",
    "    # fine_tuned_model = PeftModel.from_pretrained(\n",
    "    #     base_model, \n",
    "    #     f\"{OUTPUT_DIR}/lora_model\",\n",
    "    #     device_map=\"auto\"\n",
    "    # )\n",
    "    \n",
    "    fine_tuned_model = model\n",
    "    \n",
    "    # Test the model with the prompts\n",
    "    for test_input_string in test_prompts:\n",
    "        inputs = tokenizer(test_input_string, return_tensors=\"pt\").to(DEVICE)\n",
    "        # print(tokens[0])\n",
    "        tokens = fine_tuned_model.generate(\n",
    "            **inputs, \n",
    "            max_length=100, \n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        print(\"Without sampling: \" + tokenizer.decode(tokens[0], skip_special_tokens=True))\n",
    "        \n",
    "        print(\"---------------\")\n",
    "        tokens = fine_tuned_model.generate(\n",
    "            **inputs, \n",
    "            max_length=100, \n",
    "            pad_token_id=tokenizer.eos_token_id, \n",
    "            do_sample=True,\n",
    "        )\n",
    "        print(\"With sampling   : \" + tokenizer.decode(tokens[0], skip_special_tokens=True))\n",
    "        \n",
    "        print(\"\\n===============\")\n",
    "    \n",
    "load_and_test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import * \n",
    "\n",
    "article = \"Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him.\"\n",
    "summary = \"Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday\"\n",
    "adverse_summary = \"Daniel Craig is recasted as James Bond again\"\n",
    "# In normal, query is sentence/article, and answer is summary/highlight (S->A direction)\n",
    "base = calculate_score(summary, article, model, tokenizer, backward=False, query_direction=\"normal\", debug=True)\n",
    "\n",
    "print(base['normalized_log_prob'], base['perplexity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = calculate_score(summary, article, model, tokenizer, backward=False, query_direction=\"reverse\", debug=True)\n",
    "\n",
    "print(base['normalized_log_prob'], base['perplexity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
