{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune with full scale dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append paths for the src folder\n",
    "import sys\n",
    "import os \n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.getcwd(), 'idl-project')))\n",
    "\n",
    "# Additional imports \n",
    "from src.utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import load_fo_model\n",
    "from src.data import load_flan_dataset, load_summarization_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from peft import (LoraConfig, PeftModel, get_peft_model,\n",
    "                  prepare_model_for_kbit_training)\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (AutoTokenizer, BitsAndBytesConfig,\n",
    "                          DataCollatorForLanguageModeling, GPTNeoXForCausalLM,\n",
    "                          Trainer, TrainingArguments, TrainerCallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.path.abspath(os.getcwd())\n",
    "\n",
    "# Check if 'idl-project' is in the path\n",
    "if 'idl-project' not in current_dir:\n",
    "    raise Exception(\"Current directory '{current_dir}' is not within 'idl-project'\")\n",
    "\n",
    "print(f\"✓ Working in '{current_dir}'\")\n",
    "print(f\"✓ Directory contains 'idl-project'\")\n",
    "\n",
    "OUTPUT_DIR = \"output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.finetune_lora_config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init(entity=\"11785_finetuning\", project='ivan-testing-team', name=RUN_NAME, reinit=True)\n",
    "wandb.save(\"notebooks/finetune_lora_config.py\", policy=\"now\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test prompts\n",
    "test_prompts = [\n",
    "    \"Who is Barack Obama?\",\n",
    "    \"What is Carnegie Mellon University?\",\n",
    "    \"Classify this restaurant review sentiment: 'The food was absolutely delicious but the service was extremely slow and the waiter seemed uninterested in helping us.'\",\n",
    "    \"Create a summary of the following incident: \\nAt just after 1 a.m., three CMU students were walking in the 3600 block of Fifth Avenue approximately one mile from central campus when someone in a white Toyota Tacoma fired a BB pellet gun at them, striking two of the students in the arm and the third in the back, none of whom reported injuries. They did not see who was inside the vehicle and were unable to see the vehicle’s license plate.\",\n",
    "    \"Summarize the following text: \\nDr. Sarah Chen, lead scientist on the mission, called it 'the most significant discovery in the history of space exploration.' The finding suggests that Mars once had a much more hospitable environment with liquid water and possibly a thicker atmosphere. The agency plans to send a sample return mission within the next five years to bring these fossils back to Earth for more detailed analysis. This discovery has profound implications for our understanding of how life might develop throughout the universe.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_summarization_datasets(subset_names=['niv2_zsopt_data', 'cot_zsopt_data', 'dialog_zsopt_data'])\n",
    "dataset = load_summarization_datasets(\n",
    "    subset_names=FLAN_SUBSET, subset_frac=SUBSET_FRAC, p=TASK_DIVERSITY_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    print(i, dataset[i]['_task_name'])\n",
    "    # print(dataset[i]['inputs'])\n",
    "    # print(dataset[i]['targets'])\n",
    "    # print()\n",
    "    # print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[i]['inputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[i]['targets']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  LORA\n",
    "\n",
    "### Configure BitsAndBytes for 4-bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure BitsAndBytes for 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",           # Use 4-bit NormalFloat quantization\n",
    "    bnb_4bit_use_double_quant=True,      # Use double quantization for additional memory savings\n",
    "    bnb_4bit_compute_dtype=torch.float32  # Compute in float32 (can also use torch.bfloat16 if available)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID       = \"EleutherAI/pythia-160m-deduped\"\n",
    "MODEL_REVISION = \"step143000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    revision=MODEL_REVISION,\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "\n",
    "# Ensure the tokenizer has padding token set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "if FT_TYPE == 'full':\n",
    "    model = GPTNeoXForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        revision=MODEL_REVISION,\n",
    "        cache_dir=CACHE_DIR,\n",
    "        # quantization_config=bnb_config,\n",
    "        device_map=\"auto\"  # Automatically distribute layers across available GPUs\n",
    "    )\n",
    "else:\n",
    "    # Load the model with quantization\n",
    "    model = GPTNeoXForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        revision=MODEL_REVISION,\n",
    "        cache_dir=CACHE_DIR,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\"  # Automatically distribute layers across available GPUs\n",
    "    )\n",
    "\n",
    "# Prepare the model for k-bit training\n",
    "# model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print trainable parameters information\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FT_TYPE == 'lora': \n",
    "    # Define the LoRA configuration\n",
    "    # For Pythia models, the target module is \"query_key_value\" for attention layers\n",
    "    lora_config = LoraConfig(\n",
    "        r=LORA_RANK,                    # Rank dimension\n",
    "        lora_alpha=LORA_ALPHA,          # LoRA scaling factor\n",
    "        target_modules=TARGET_MODULES,  # Target specific attention modules\n",
    "        lora_dropout=LORA_DROPOUT,      # Dropout probability for LoRA layers\n",
    "        bias=\"none\",            # Don't apply LoRA to bias terms\n",
    "        task_type=\"CAUSAL_LM\"   # Task type for causal language modeling\n",
    "    )\n",
    "\n",
    "    # Apply LoRA to the model\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Dataset Formatting and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format the dataset for causal language modeling\n",
    "def format_for_clm(examples):\n",
    "    # Format as: \"Instruction: {instruction} Input: {input} Output: {output}\"\n",
    "    # Adjust this format based on your specific dataset structure\n",
    "    if 'inputs' in examples and 'targets' in examples:\n",
    "        texts = [\n",
    "            f\"{inp}\\n{target}{tokenizer.eos_token}\"\n",
    "            for inp, target in zip(examples['inputs'], examples['targets'])\n",
    "        ]\n",
    "    else:\n",
    "        # Fallback for other dataset structures\n",
    "        texts = examples['text'] if 'text' in examples else []\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Apply formatting \n",
    "if isinstance(dataset, Dataset):\n",
    "    # For non-streaming datasets\n",
    "    if 'inputs' in dataset.column_names and 'targets' in dataset.column_names:\n",
    "        dataset = dataset.map(format_for_clm, batched=True, num_proc=16)\n",
    "else:\n",
    "    # For streaming datasets, we need to format each example as it comes\n",
    "    dataset = dataset.map(lambda example: {\n",
    "        'text': f\"Instruction: {example['inputs']}\\nOutput: {example['targets']}\" \n",
    "        if 'inputs' in example and 'targets' in example \n",
    "        else example.get('text', '')\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize function for the dataset\n",
    "def tokenize_function(example):\n",
    "    # Handle single examples for streaming datasets\n",
    "    text = example[\"text\"] if \"text\" in example else \"\"\n",
    "    \n",
    "    # Tokenize with padding and truncation\n",
    "    outputs = tokenizer(\n",
    "        text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=1024,  # Adjust based on your needs and GPU memory\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Remove the batch dimension for single examples\n",
    "    for key in outputs:\n",
    "        if isinstance(outputs[key], torch.Tensor) and outputs[key].ndim > 1:\n",
    "            outputs[key] = outputs[key].squeeze(0)\n",
    "    \n",
    "    # Set labels equal to input_ids for causal language modeling\n",
    "    outputs[\"labels\"] = outputs[\"input_ids\"].clone()\n",
    "    return outputs\n",
    "\n",
    "# Apply tokenization to the dataset\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(tokenize_function, num_proc=8) # Can't use tokenization batched here for some reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length of training dataset: {len(tokenized_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = tokenized_dataset.train_test_split(test_size=TEST_SIZE)\n",
    "train_dataset = split_dataset['train']\n",
    "val_dataset   = split_dataset['test']  # Note: called 'test' by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset summary\")\n",
    "print(\"-\"*80)\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(f\"   Length of training dataset    : {len(train_dataset)}\")\n",
    "print(f\"   Length of validation dataset  : {len(val_dataset)}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(f\"   Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"   Accumulation Steps: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(f\"   Expected total training steps : {len(train_dataset) // BATCH_SIZE // GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"   Expected validation steps     : {len(val_dataset) // BATCH_SIZE // GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Since we're using streaming datasets, convert to list for training\n",
    "# # This is needed because Trainer expects a non-streaming dataset\n",
    "# # We'll create a buffer of examples for training\n",
    "# buffer_size = 25000  # Adjust based on your memory constraints\n",
    "# tokenized_examples = []\n",
    "# for example in tqdm(tokenized_dataset, total=buffer_size):\n",
    "#     tokenized_examples.append(example)\n",
    "#     if len(tokenized_examples) >= buffer_size:\n",
    "#         break\n",
    "\n",
    "# print(f\"Collected {len(tokenized_examples)} examples for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert to Dataset object for training\n",
    "# from datasets import Dataset as HFDataset\n",
    "# train_dataset = HFDataset.from_list(tokenized_examples)\n",
    "\n",
    "# print(f\"Training dataset created with columns: {train_dataset.column_names}\")\n",
    "# print(f\"Number of examples: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation test callback\n",
    "class GenerationTestCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Callback to generate text samples at evaluation steps.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, test_prompts, max_length=250, do_sample=True, \n",
    "                 num_beams=2, temperature=0.0, repetition_penalty=1.001, device=\"cuda\"):\n",
    "        \"\"\"\n",
    "        Initialize with tokenizer and test prompts.\n",
    "        \"\"\"\n",
    "        self.tokenizer    = tokenizer\n",
    "        self.test_prompts = test_prompts\n",
    "        self.device       = device\n",
    "        self.max_length   = max_length\n",
    "        self.do_sample    = do_sample\n",
    "        self.num_beams    = num_beams\n",
    "        self.temperature  = temperature\n",
    "        self.repetition_penalty = repetition_penalty\n",
    "            \n",
    "            \n",
    "    def on_evaluate(self, args, state, control, model, **kwargs):\n",
    "        \"\"\"\n",
    "        Run after each evaluation to generate two types of samples:\n",
    "        1. Free-form completion with sampling\n",
    "        2. Greedy decoding for deterministic output\n",
    "        \"\"\"\n",
    "        print(\"GenerationTestCallback\")\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"Generating samples at step {state.global_step}:\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for prompt in self.test_prompts:\n",
    "                inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "                \n",
    "                # 1. Generate with sampling\n",
    "                sample_output_ids = model.generate(\n",
    "                    inputs[\"input_ids\"],\n",
    "                    attention_mask    = inputs[\"attention_mask\"],\n",
    "                    max_length        = self.max_length,\n",
    "                    do_sample         = True,\n",
    "                    repetition_penalty = self.repetition_penalty,\n",
    "                    no_repeat_ngram_size = 3,\n",
    "                    # pad_token_id      = self.tokenizer.pad_token_id,\n",
    "                    # eos_token_id      = self.tokenizer.eos_token_id\n",
    "                )\n",
    "                \n",
    "                # 2. Generate with greedy decoding (deterministic) and no sampling\n",
    "                greedy_output_ids = model.generate(\n",
    "                    inputs[\"input_ids\"],\n",
    "                    attention_mask    = inputs[\"attention_mask\"],\n",
    "                    max_length        = self.max_length,\n",
    "                    do_sample         = False,\n",
    "                    temperature       = self.temperature,\n",
    "                    repetition_penalty = self.repetition_penalty,\n",
    "                    no_repeat_ngram_size = 3,\n",
    "                    # pad_token_id      = self.tokenizer.eos_token_id,\n",
    "                    # pad_token_id      = self.tokenizer.pad_token_id,\n",
    "                    # eos_token_id      = self.tokenizer.eos_token_id\n",
    "                )\n",
    "                \n",
    "                # Decode both outputs\n",
    "                sampled_text = self.tokenizer.decode(sample_output_ids[0], skip_special_tokens=True)\n",
    "                greedy_text  = self.tokenizer.decode(greedy_output_ids[0], skip_special_tokens=True)\n",
    "                \n",
    "                # Print the results\n",
    "                print(f\"\\nPrompt: {prompt}\")\n",
    "                print(f\"Greedy: {greedy_text}\")\n",
    "                print(\"-\"*50)\n",
    "                \n",
    "                # Log to W&B if you're using it\n",
    "                if args.report_to == \"wandb\":\n",
    "                    import wandb\n",
    "                    wandb.log({\n",
    "                        f\"generation/{prompt}/sampled\": wandb.Html(\n",
    "                            f\"<b>Step {state.global_step}</b><br>\"\n",
    "                            f\"<p><b>Prompt:</b> {prompt}</p>\"\n",
    "                            f\"<p><b>Sampled:</b> {sampled_text}</p>\"\n",
    "                        ),\n",
    "                        f\"generation/{prompt}/greedy\": wandb.Html(\n",
    "                            f\"<b>Step {state.global_step}</b><br>\"\n",
    "                            f\"<p><b>Prompt:</b> {prompt}</p>\"\n",
    "                            f\"<p><b>Greedy:</b> {greedy_text}</p>\"\n",
    "                        )\n",
    "                    }, step=state.global_step)\n",
    "        \n",
    "        return control\n",
    "\n",
    "# Create generation callback\n",
    "generation_callback = GenerationTestCallback(\n",
    "    tokenizer     = tokenizer,\n",
    "    test_prompts  = test_prompts,\n",
    "    max_length    = 250,\n",
    "    num_beams     = 3,\n",
    "    temperature   = 0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"eos_token_id: {tokenizer.eos_token_id}, eos_token = {tokenizer.eos_token}\")\n",
    "print(f\"pad_token_id: {tokenizer.pad_token_id}, pad_token = {tokenizer.pad_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_test_model():\n",
    "    print(\"\\nTesting model:\")\n",
    "    \n",
    "    # Load the base model and LoRA adapter\n",
    "    # base_model = GPTNeoXForCausalLM.from_pretrained(\n",
    "    #     MODEL_ID,\n",
    "    #     revision=MODEL_REVISION,\n",
    "    #     cache_dir=CACHE_DIR,\n",
    "    #     device_map=\"auto\"\n",
    "    # )\n",
    "    \n",
    "    # # Load and apply the fine-tuned LoRA weights\n",
    "    # fine_tuned_model = PeftModel.from_pretrained(\n",
    "    #     base_model, \n",
    "    #     f\"{OUTPUT_DIR}/lora_model\",\n",
    "    #     device_map=\"auto\"\n",
    "    # )\n",
    "    \n",
    "    fine_tuned_model = model\n",
    "    \n",
    "    # Test the model with the prompts\n",
    "    for test_input_string in test_prompts:\n",
    "        inputs = tokenizer(test_input_string, return_tensors=\"pt\").to(DEVICE)\n",
    "        # print(tokens[0])\n",
    "        tokens = fine_tuned_model.generate(\n",
    "            **inputs, \n",
    "            repetition_penalty=1.001,\n",
    "            max_length=250, \n",
    "            no_repeat_ngram_size = 3,\n",
    "            # eos_token_id=tokenizer.eos_token_id,\n",
    "            # pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "        print(\"Without sampling: \" + tokenizer.decode(tokens[0], skip_special_tokens=True))\n",
    "        \n",
    "        print(\"---------------\")\n",
    "        tokens = fine_tuned_model.generate(\n",
    "            **inputs, \n",
    "            repetition_penalty=1.001,\n",
    "            max_length=250, \n",
    "            no_repeat_ngram_size = 3,\n",
    "            do_sample=True,\n",
    "            # eos_token_id=tokenizer.eos_token_id,\n",
    "            # pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "        print(\"With sampling   : \" + tokenizer.decode(tokens[0], skip_special_tokens=True))\n",
    "        \n",
    "        print(\"\\n===============\")\n",
    "    \n",
    "load_and_test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set format to PyTorch\n",
    "train_dataset.set_format(type=\"torch\")\n",
    "val_dataset.set_format(type=\"torch\")\n",
    "\n",
    "# Create training arguments with parameters\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size  = BATCH_SIZE,\n",
    "    per_device_eval_batch_size   = BATCH_SIZE,\n",
    "    gradient_accumulation_steps  = GRADIENT_ACCUMULATION_STEPS,\n",
    "    warmup_ratio                 = WARMUP_RATIO,\n",
    "    num_train_epochs             = NUM_EPOCHS,\n",
    "    learning_rate                = LEARNING_RATE,\n",
    "    lr_scheduler_type            = LR_SCHEDULER_TYPE,\n",
    "    fp16                         = FP16,\n",
    "    logging_steps                = LOGGING_STEPS,\n",
    "    save_steps                   = SAVE_STEPS,\n",
    "    eval_strategy                = \"steps\",\n",
    "    eval_steps                   = EVAL_STEPS,\n",
    "    output_dir                   = OUTPUT_DIR,\n",
    "    optim                        = \"paged_adamw_8bit\", \n",
    "    save_total_limit             = SAVE_TOTAL_LIMIT,\n",
    "    \n",
    "    report_to                    = \"wandb\",\n",
    "    weight_decay                 = WEIGHT_DECAY,\n",
    "    \n",
    "    \n",
    "    logging_first_step           = True,  \n",
    "    max_grad_norm                = 1.0,\n",
    "    dataloader_num_workers       = 4,\n",
    "    \n",
    "    load_best_model_at_end       = True,\n",
    "    # metric_for_best_model        = \"eval_loss\",\n",
    "    # greater_is_better            = False,\n",
    ")\n",
    "\n",
    "\n",
    "# Set up the trainer with validation\n",
    "trainer = Trainer(\n",
    "    model                     = model,\n",
    "    args                      = training_args,\n",
    "    train_dataset             = train_dataset,\n",
    "    eval_dataset              = val_dataset,\n",
    "    data_collator             = DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    compute_metrics           = None,  \n",
    "    callbacks                 = [generation_callback],  \n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "# Disable caching during training to avoid memory issues\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "model.save_pretrained(f\"{OUTPUT_DIR}/{RUN_NAME}\")\n",
    "tokenizer.save_pretrained(f\"{OUTPUT_DIR}/{RUN_NAME}\")\n",
    "\n",
    "print(\"Training complete and model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import * \n",
    "\n",
    "article = \"Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him.\"\n",
    "summary = \"Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday\"\n",
    "adverse_summary = \"Daniel Craig is recasted as James Bond again\"\n",
    "# In normal, query is sentence/article, and answer is summary/highlight (S->A direction)\n",
    "base = calculate_score(summary, article, model, tokenizer, backward=False, query_direction=\"normal\", debug=True)\n",
    "\n",
    "print(base['normalized_log_prob'], base['perplexity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = calculate_score(summary, article, model, tokenizer, backward=False, query_direction=\"reverse\", debug=True)\n",
    "\n",
    "print(base['normalized_log_prob'], base['perplexity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
