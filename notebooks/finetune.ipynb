{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune with full scale dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import wandb\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer, Trainer,\n",
    "                          TrainingArguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os \n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "from src.model import load_fo_model\n",
    "from src.data import load_flan_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"output/\"\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.mkdir(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"chiayewken/flan-v2\", split=\"train\", streaming=True)\n",
    "\n",
    "# model_name = \"EleutherAI/pythia-160m\"  #select the lm model\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf2af00b5b314d42a53faf074c9cd047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/6.83k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b287bc31267c47a89433af86514423e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/2167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_fo_model()\n",
    "dataset = load_flan_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token)\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 1: AddedToken(\"<|padding|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 50254: AddedToken(\"                        \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50255: AddedToken(\"                       \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50256: AddedToken(\"                      \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50257: AddedToken(\"                     \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50258: AddedToken(\"                    \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50259: AddedToken(\"                   \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50260: AddedToken(\"                  \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50261: AddedToken(\"                 \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50262: AddedToken(\"                \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50263: AddedToken(\"               \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50264: AddedToken(\"              \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50265: AddedToken(\"             \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50266: AddedToken(\"            \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50267: AddedToken(\"           \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50268: AddedToken(\"          \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50269: AddedToken(\"         \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50270: AddedToken(\"        \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50271: AddedToken(\"       \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50272: AddedToken(\"      \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50273: AddedToken(\"     \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50274: AddedToken(\"    \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50275: AddedToken(\"   \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50276: AddedToken(\"  \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure tokenizer properly \n",
    "\n",
    "# - https://github.com/EleutherAI/pythia/issues/156 \n",
    "#   mentioned it's okay to set it to eos\n",
    "\n",
    "# - https://huggingface.co/EleutherAI/pythia-14m/discussions/4, \n",
    "#   we can see the tokenizer pad token from tokenizer.added_tokens_decoder \n",
    "tokenizer.added_tokens_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without custom pad token set to zero the convergence behaves weirdly\n",
    "tokenizer.pad_token    = \"<|padding|>\"\n",
    "tokenizer.pad_token_id = 1     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_forward(example):\n",
    "    combined_text = f\"{example['source']}\\n{example['target']}\"\n",
    "    tokenized_output = tokenizer(combined_text, truncation=True, padding=\"max_length\", max_length=768, return_tensors=\"pt\")\n",
    "\n",
    "    input_ids = tokenized_output[\"input_ids\"].squeeze(0)\n",
    "    labels = input_ids.clone()\n",
    "    labels[:-1] = input_ids[1:]  # Shift left\n",
    "    labels[-1] = -100  # Ignore loss for last token\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_forward(example):\n",
    "    # combine input, output, and eos token - (I:... Q:... A:... <eos>)\n",
    "    combined_text = f\"{example['inputs']}\\n{example['targets']}{tokenizer.eos_token}\"\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        combined_text,\n",
    "        truncation=True,\n",
    "        max_length=768,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    input_ids = tokenized[\"input_ids\"][0]\n",
    "    labels = input_ids.clone()\n",
    "    \n",
    "    # shift left \n",
    "    labels[:-1] = input_ids[1:]\n",
    "    \n",
    "    # mask the pad and eos token\n",
    "    labels[input_ids == tokenizer.pad_token_id] = -100\n",
    "    labels[input_ids == tokenizer.eos_token_id] = -100\n",
    "    \n",
    "    return {\"input_ids\": input_ids, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n",
      "Input IDs: tensor([  510,   637,   320,  4645,   521, 23908,   281,  7747,   285,   417,\n",
      "          816,  5753,   537,  1893,   752,   812,   320,   253,  1953,    32,\n",
      "          187, 23433,  3560,   407,  3662,    27,  1310,   346,    34,   637,\n",
      "         9398,  3644,  4759, 14133,   285, 17052,    81,  1103,   310,  4645,\n",
      "          745,   247, 30040, 20953,   449,  1057,   326,  1599,   326,   346,\n",
      "           34,   637,   310,  4645,   521, 30040, 20953,   281,   512,   253,\n",
      "         5753,   449,    32,   187, 10976,    27,   187,    14,  4754,   187,\n",
      "           14,   352,   310,   417,  1896,   281,  2028,   187,    14,   642,\n",
      "          187,   262,   310,   417,  1896,   281,  2028,   187,   187,   510,\n",
      "        13131,  2550,   320, 17800,   285, 17390,   387,   253,  1072,   673,\n",
      "           15,   187,   510,  1953,   285,  3662,   403,  2708,    15,   187,\n",
      "         5804,   359,  7525,   432,   346,  7910,  2872,  8290,  2186,   247,\n",
      "        17800, 13131,  1223,  1097,   403,  7063,   275,   253,  1072,  6951,\n",
      "          449,   326,   346,   510, 13131,   310,  3809, 17390,   449,    32,\n",
      "          187, 10976,    27,   187,    14,  4754,   187,    14,   642,   187,\n",
      "           14,   352,   310,   417,  1896,   281,  2028,   187,  2369,   187,\n",
      "          187,  3404,   247, 46309,  8288,  1057,   417,  7933,  1599,  8288,\n",
      "          562,   670,  9836,  7217,    15,   187,   510,  1953,   285,  3662,\n",
      "          403,  2708,    15,   187, 43217,   885,    27,   346,    34,  3416,\n",
      "          275,  2806,   342,   247, 23136,  4176, 18553,   310,  6306,   387,\n",
      "          247, 46309,  8288,   281,   271,  8446,   449,   187, 39355,  4521,\n",
      "          261,    27,   346,    34,  3416,   310,  8288,   562,   670,  9836,\n",
      "         7217,   281,   271,  8446,   273,  2143,  5753,   449,   187,  4045,\n",
      "          359,   871,   326,   253,  9079,   994,  7193,   407,   253, 26536,\n",
      "           32,   187,   262,   310,   417,  1896,   281,  2028,   187,   187,\n",
      "         1147,   434, 11543,   253,   637, 12453,   247, 26540,   407, 16115,\n",
      "          281,   779,   537,  1893,   752,   812,   320,   253,  1953,    32,\n",
      "          187, 23433,  3560,   407,  3662,    27,  2615,   359,  7525,   432,\n",
      "          346,    34,   637,   275,   247,  7619, 14133,   285, 13898, 12453,\n",
      "          247, 26540,   275,   247,  8576,   449,   326,   346,   510,   637,\n",
      "          310, 16115,   449,    32,   187, 10976,    27,   187,    14,  4754,\n",
      "          187,    14,   642,   187,    14,   352,   310,   417,  1896,   281,\n",
      "         2028,   187,  2369,   187,   187,    34,  2872,   637,  4882,   253,\n",
      "        12609,   275,   247,  3317,   314,  6195, 11803,    15,   187,   510,\n",
      "         1953,   285,  3662,   403,  2708,    15,   187,  2042,   346,    34,\n",
      "         2872,   637, 14186,   275,  2806,   310,  4882,   253, 12609,   275,\n",
      "          247,  3317,   314,  6195, 11803,   449,  1057,   326,  1599,   326,\n",
      "          346,    34,  2872,  1436, 14186,   275,  2806,   310,  4882,   253,\n",
      "        12609,   275,   247,  3317,   314,  6195, 11803,   449,    32,   187,\n",
      "        10976,    27,   187,    14,  4754,   187,    14,   352,   310,   417,\n",
      "         1896,   281,  2028,   187,    14,   642,   187,  9820,   187,   187,\n",
      "        22737,  2550, 37931,   275, 10023,   273, 46402,   285,  4677, 37931,\n",
      "        10486,    15,   187,   510,  1953,   285,  3662,   403,  2708,    15,\n",
      "          187,   187, 43217,   885,    27,   346,    56,  4998,  1629,   684,\n",
      "          275, 10023,   273, 46402,   449,   187, 15545,   327,   436, 26536,\n",
      "           13,   476,   359,  7525,   326,   253,  9079,   346,   510,  3416,\n",
      "          310,  4677, 46851,   449,   310,  2032,    32,   187, 10976,    27,\n",
      "          187,    14,  4754,   187,    14,   352,   310,   417,  1896,   281,\n",
      "         2028,   187,    14,   642,   187,  2369,     0,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1])\n",
      "Decoded Tokens: The man be showing his toys to adults and not just kids.. So what could be the question?\n",
      "Question followed by answer: If \"A man wearing dark green shirt and sweatpants is showing off a stuffed toy.\" does that mean that \"A man is showing his stuffed toy to all the kids.\"?\n",
      "Options:\n",
      "- yes\n",
      "- it is not possible to tell\n",
      "- no\n",
      "it is not possible to tell\n",
      "\n",
      "The infant cannot be crying and asleep at the same time.\n",
      "The question and answer are below.\n",
      "Can we conclude from \"Two young boys hold a crying infant while both are sitting in the same chair.\" that \"The infant is fast asleep.\"?\n",
      "Options:\n",
      "- yes\n",
      "- no\n",
      "- it is not possible to tell\n",
      "no\n",
      "\n",
      "At a podium speaking does not necessarily mean speaking out about domestic violence.\n",
      "The question and answer are below.\n",
      "Premise: \"A woman in black with a tan suit jacket is standing at a podium speaking to an audience.\"\n",
      "Hypothesis: \"A woman is speaking out about domestic violence to an audience of school kids.\"\n",
      "Do we know that the hypothesis entailed by the premise?\n",
      "it is not possible to tell\n",
      "\n",
      "It's unlikely the man addresses a chef by singing to him.. So what could be the question?\n",
      "Question followed by answer: Can we conclude from \"A man in a dress shirt and tie addresses a chef in a kitchen.\" that \"The man is singing.\"?\n",
      "Options:\n",
      "- yes\n",
      "- no\n",
      "- it is not possible to tell\n",
      "no\n",
      "\n",
      "A young man playing the guitar in a dimly lit studio.\n",
      "The question and answer are below.\n",
      "If \"A young man dressed in black is playing the guitar in a dimly lit studio.\" does that mean that \"A young person dressed in black is playing the guitar in a dimly lit studio.\"?\n",
      "Options:\n",
      "- yes\n",
      "- it is not possible to tell\n",
      "- no\n",
      "yes\n",
      "\n",
      "Women cannot skate in possession of puck and figure skate simultaneously.\n",
      "The question and answer are below.\n",
      "\n",
      "Premise: \"Woman skates in possession of puck.\"\n",
      "Based on this premise, can we conclude that the hypothesis \"The woman is figure skating.\" is true?\n",
      "Options:\n",
      "- yes\n",
      "- it is not possible to tell\n",
      "- no\n",
      "no\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Example 2:\n",
      "Input IDs: tensor([   60, 26310,  2449,    62,  1310,   346,    34,  8516,  4370,   310,\n",
      "          327,   253,  3216,  1756,  1981,   449,  1057,   326,  1599,   326,\n",
      "          346,    34,  4370,   310,  9734,   247, 20218,   449,    32,   187,\n",
      "        10976,    27,   187,    14,  4754,   187,    14,   352,   310,   417,\n",
      "         1896,   281,  2028,   187,    14,   642,   187,  2302,  1039,   281,\n",
      "         2028,   604,   253,  4370,   310,  9734,   247, 20218,    15,   187,\n",
      "          510,  3662,   310,   352,   310,   417,  1896,   281,  2028,    15,\n",
      "          187,   187,    50,    27,  1310,   346,    34,   637,   310,  7063,\n",
      "          275,   247,  8516,  6951,   342,   247,  3416,  7063,   327,   521,\n",
      "        13427, 18727,   323,   253,  6568,   449,  1057,   326,  1599,   326,\n",
      "          346,    34,  4564,   403,  3192,   271, 13226,  5406,   449,    32,\n",
      "          187, 10976,    27,   187,    14,  4754,   187,    14,   352,   310,\n",
      "          417,  1896,   281,  2028,   187,    14,   642,   187,    34,    27,\n",
      "         5761,   952, 18727,   323,   247,  6568,   513,   417,  1599,   326,\n",
      "          597,   403, 13226,  7968,    15,   187,   510,  3662,   310,   352,\n",
      "          310,   417,  1896,   281,  2028,    15,   187,   187, 26310,  2449,\n",
      "           27, 13343,   885,    27,   346,  7910,  1821,  8862,   403, 15133,\n",
      "          432,   247,  1355,  2502,  3817,   449,   187, 15545,   327,   436,\n",
      "        26536,    13,   476,   359,  7525,   326,   253,  9079,   346,    34,\n",
      "         3392,   285,  3347,  1790,   327,   247,  3817, 15133,   449,   310,\n",
      "         2032,    32,   187, 10976,    27,   187,    14,  4754,   187,    14,\n",
      "          352,   310,   417,  1896,   281,  2028,   187,    14,   642,   187,\n",
      "          187,  1466,   434,  8415,   352,  7808,    27,   380,   767,  1821,\n",
      "          513,   417,   452,   281,   320,  3392,   285,  3347,    15,   187,\n",
      "          510,  3662,   310,   352,   310,   417,  1896,   281,  2028,    15,\n",
      "          187,   187,    60, 26310,  2449,    62, 10300,   253,  6197,   346,\n",
      "        17185, 14370,    73,   517, 16893,   347,  4370,  1180,   337,   449,\n",
      "          476,   359,  7525,   326,   346,    34,  4370, 16893,   449,    32,\n",
      "          187, 10976,    27,   187,    14,  4754,   187,    14,   352,   310,\n",
      "          417,  1896,   281,  2028,   187,    14,   642,   187,    34,  3168,\n",
      "        14370,    73,   517,   310,   247,  4370,    28,   326,   352,   310,\n",
      "        16893,   310, 18648,  4232,    15,   187,   510,  3662,   310,  4754,\n",
      "           15,   187,   187,    50,    27,  1310,   346,    34,  3416,  9398,\n",
      "          247,  4759, 14133,   285,   247,  5006,  9398,   247,  2502, 14133,\n",
      "          403, 17392, 10015,   275,   247, 38879,  1223,  7063,   327,   247,\n",
      "         2412,   449,  1057,   326,  1599,   326,   346,   510,  3416,   285,\n",
      "         5006,   403, 17392, 10015,   275,   253, 38879,   449,    32,   187,\n",
      "        10976,    27,   187,    14,  4754,   187,    14,   352,   310,   417,\n",
      "         1896,   281,  2028,   187,    14,   642,   187,    34,    27,   329,\n",
      "         3416,   285,   247,  5006,   403, 17392, 10015,   275,   247, 38879,\n",
      "           15,   187,   510,  3662,   310,  4754,    15,   187,   187,    60,\n",
      "        26310,  2449,    62,  2615,   359,  7525,   432,   346,    34,   637,\n",
      "          275,   247,  3168, 14133,   285, 27929,   449,   326,   346, 30389,\n",
      "           69,  6816,   275,   247,  2846,  6406,   342,   247,  9539,  7487,\n",
      "          449,    32,   187, 10976,    27,   187,    14,  4754,   187,    14,\n",
      "          642,   187,    14,   352,   310,   417,  1896,   281,  2028,   187,\n",
      "          187,    34,   637,   310,  2740, 18792,   285, 17055,   247,  2257,\n",
      "          273,  2583,   432,   247,  9539,    15,   187,   510,  3662,   310,\n",
      "          352,   310,   417,  1896,   281,  2028,    15,     0,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1])\n",
      "Decoded Tokens: [QUESTION] If \"A brown dog is on the ground growling.\" does that mean that \"A dog is warning a stranger.\"?\n",
      "Options:\n",
      "- yes\n",
      "- it is not possible to tell\n",
      "- no\n",
      "No way to tell if the dog is warning a stranger.\n",
      "The answer is it is not possible to tell.\n",
      "\n",
      "Q: If \"A man is sitting in a brown chair with a woman sitting on his lap smiling for the camera.\" does that mean that \"A couple are taking an engagement picture.\"?\n",
      "Options:\n",
      "- yes\n",
      "- it is not possible to tell\n",
      "- no\n",
      "A: Two people smiling for a camera do not mean that they are engagement pictures.\n",
      "The answer is it is not possible to tell.\n",
      "\n",
      "QUESTION: Premise: \"Two men yellow are fishing from a small red box.\"\n",
      "Based on this premise, can we conclude that the hypothesis \"A father and son sit on a box fishing.\" is true?\n",
      "Options:\n",
      "- yes\n",
      "- it is not possible to tell\n",
      "- no\n",
      "\n",
      "Let's solve it slowly: The two men do not have to be father and son.\n",
      "The answer is it is not possible to tell.\n",
      "\n",
      "[QUESTION] Given the sentence \"White greyhound racing as dog number 1.\" can we conclude that \"A dog racing.\"?\n",
      "Options:\n",
      "- yes\n",
      "- it is not possible to tell\n",
      "- no\n",
      "A white greyhound is a dog; that it is racing is firmly established.\n",
      "The answer is yes.\n",
      "\n",
      "Q: If \"A woman wearing a green shirt and a boy wearing a red shirt are washing clothes in a creek while sitting on a log.\" does that mean that \"The woman and boy are washing clothes in the creek.\"?\n",
      "Options:\n",
      "- yes\n",
      "- it is not possible to tell\n",
      "- no\n",
      "A: A woman and a boy are washing clothes in a creek.\n",
      "The answer is yes.\n",
      "\n",
      "[QUESTION] Can we conclude from \"A man in a white shirt and jeans.\" that \"Breakdancing in a city street with a crowd watching.\"?\n",
      "Options:\n",
      "- yes\n",
      "- no\n",
      "- it is not possible to tell\n",
      "\n",
      "A man is break dancing and collecting a lot of money from a crowd.\n",
      "The answer is it is not possible to tell.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Example 3:\n",
      "Input IDs: tensor([   43,   991,    27,  6758,   273,   253,  1563, 14683,   310, 14122,\n",
      "          561,   474,    32,   187, 10976,    27,   187,    14, 20580,   566,\n",
      "          329,    27,   346,  1328, 11210,   626, 10464,   742,   275,   247,\n",
      "         2129,   594,   344,   434, 16076,     3,   187,    14, 20580,   566,\n",
      "          378,    27,   346,  1328, 11210,   626, 10464,   742,   275,   247,\n",
      "         2129,   594,   344,   434,  4076,     3,   187,   187, 18552,    27,\n",
      "        37239,   273,  1869,    27,   329,  1436,   665,   556,   417,   921,\n",
      "         2122,   323,   247,  2129,   476,   760,   320, 16076,    15,   380,\n",
      "         3662,   310, 20580,   566,   378,    15,   187,   187,    43,   991,\n",
      "           27, 20745,   534,  6197,   310,   417, 13760,    15,   187, 10976,\n",
      "           27,   187,    14, 20580,   566,   329,    27,   346,   510,  5101,\n",
      "         4566,  1475,   253,  6149,     3,   187,    14, 20580,   566,   378,\n",
      "           27,   346,   510,  6149,  4566,  1475,   253,  5101,     3,   187,\n",
      "          187, 18552,    27, 37239,   273,  1869,    27,   380,  5101,  1057,\n",
      "          417,  2118,    13,   253,  6149, 45933,    15,   380,  3662,   310,\n",
      "        20580,   566,   329,    15,   187,   187,    43,   991,    27,  4683,\n",
      "          253,  2708, 14683,    13,   534,   581,  1057,   475,  1439,    11,\n",
      "         1056,  3282,    32,   187, 10976,    27,   187,    14, 20580,   566,\n",
      "          329,    27,   346,  1328, 14782,  3698,   327,   521, 20051,     3,\n",
      "          187,    14, 20580,   566,   378,    27,   346,   248, 14782,  3698,\n",
      "          327,   521, 11480,     3,   187,   187, 18552,    27,   187, 26268,\n",
      "          273,  1869,    27,   329,  3698,   310,   417, 16332,   327,   581,\n",
      "          434, 11480,    15,   380,  3662,   310, 20580,   566,   378,    15,\n",
      "            0,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1])\n",
      "Decoded Tokens: Jax: Which of the following sentences is nonsensical?\n",
      "Options:\n",
      "- Sentence A: \"He hasn't bathed in a week so he's dirty\"\n",
      "- Sentence B: \"He hasn't bathed in a week so he's clean\"\n",
      "\n",
      "Alex: Chain of thought: A person who has not showered for a week can only be dirty. The answer is Sentence B.\n",
      "\n",
      "Jax: Pick which sentence is not logical.\n",
      "Options:\n",
      "- Sentence A: \"The sun goes around the earth\"\n",
      "- Sentence B: \"The earth goes around the sun\"\n",
      "\n",
      "Alex: Chain of thought: The sun does not move, the earth rotates. The answer is Sentence A.\n",
      "\n",
      "Jax: Of the below sentences, which one does *not* make sense?\n",
      "Options:\n",
      "- Sentence A: \"He wore watch on his wrist\"\n",
      "- Sentence B: \"he wore watch on his nose\"\n",
      "\n",
      "Alex:\n",
      "Chain of thought: A watch is not worn on one's nose. The answer is Sentence B.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, example in enumerate(dataset):\n",
    "    tokenized_example = preprocess_forward(example)\n",
    "    print(f\"Example {i + 1}:\")\n",
    "    print(f\"Input IDs: {tokenized_example['input_ids']}\")\n",
    "    print(f\"Decoded Tokens: {tokenizer.decode(tokenized_example['input_ids'], skip_special_tokens=True)}\")\n",
    "    print(\"-\" * 80 + '\\n\\n')\n",
    "\n",
    "    if i == 2:  # Show only 3 examples\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "batch_size = 8\n",
    "total_examples = 378_000_000  # Total examples in FLAN dataset\n",
    "train_ratio = 0.9\n",
    "val_ratio = 0.05\n",
    "test_ratio = 0.05\n",
    "\n",
    "def preprocess_forward(example):\n",
    "    # combine input, output, and eos token - (I:... Q:... A:... <eos>)\n",
    "    combined_text = f\"{example['inputs']}\\n{example['targets']}{tokenizer.eos_token}\"\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        combined_text,\n",
    "        truncation=True,\n",
    "        max_length=768,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    input_ids = tokenized[\"input_ids\"][0]\n",
    "    labels = input_ids.clone()\n",
    "    \n",
    "    # shift left \n",
    "    labels[:-1] = input_ids[1:]\n",
    "    \n",
    "    # mask the pad and eos token\n",
    "    labels[input_ids == tokenizer.pad_token_id] = -100\n",
    "    labels[input_ids == tokenizer.eos_token_id] = -100\n",
    "    #TODO: Do we need to mask the prompt tokens?\n",
    "    \n",
    "    return {\"input_ids\": input_ids, \"labels\": labels}\n",
    "\n",
    "\n",
    "# Shuffle and split before preprocessing\n",
    "shuffled = dataset.shuffle(seed=42, buffer_size=100_000)\n",
    "\n",
    "# Calculate split sizes\n",
    "train_size = int(total_examples * train_ratio)\n",
    "val_size = int(total_examples * val_ratio)\n",
    "test_size = int(total_examples * test_ratio)\n",
    "\n",
    "# Create splits\n",
    "train_raw = shuffled.take(train_size)\n",
    "remaining = shuffled.skip(train_size)\n",
    "val_raw = remaining.take(val_size)\n",
    "test_raw = remaining.skip(val_size).take(test_size)\n",
    "\n",
    "# Preprocess each split\n",
    "tokenized_train = train_raw.map(preprocess_forward, batched=True)\n",
    "tokenized_val = val_raw.map(preprocess_forward, batched=True)\n",
    "tokenized_test = test_raw.map(preprocess_forward, batched=True)\n",
    "#TODO: Was there any reason we use batched=false?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{OUTPUT_DIR}/pythia-finetuned\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    max_steps=10_000,  # Required for streaming datasets\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    save_steps=1000,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    gradient_accumulation_steps=2,\n",
    "    fp16=True,\n",
    "    report_to=\"wandb\",\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save final model\n",
    "trainer.save_model(f\"{OUTPUT_DIR}/pythia-finetuned-final\")\n",
    "tokenizer.save_pretrained(f\"{OUTPUT_DIR}/pythia-finetuned-final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reversed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"chiayewken/flan-v2\", split=\"train\", streaming=True)\n",
    "\n",
    "model_name = \"afterless/reverse-pythia-160m\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.pad_token = \"<pad>\"\n",
    "tokenizer.pad_token_id = 0\n",
    "\n",
    "def preprocess_reverse(example):\n",
    "    combined_text = f\"{example['source']}\\n{example['target']}\"\n",
    "\n",
    "    tokenized_output = tokenizer(\n",
    "        combined_text,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=768,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    input_ids = tokenized_output[\"input_ids\"].squeeze(0)  # Remove batch dim\n",
    "    reversed_input_ids = input_ids.flip(dims=[0])  # Reverse sequence\n",
    "\n",
    "    labels = reversed_input_ids.clone()\n",
    "    labels = torch.roll(labels, shifts=1, dims=0)  # Shift right by 1\n",
    "    labels[0] = -100  # Ignore loss for first token\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": reversed_input_ids,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "for i, example in enumerate(dataset):\n",
    "    tokenized_example = preprocess_reverse(example)\n",
    "    print(f\"Example {i + 1}:\")\n",
    "    print(f\"Input IDs: {tokenized_example['input_ids']}\")\n",
    "    print(f\"Decoded Tokens: {tokenizer.decode(tokenized_example['input_ids'], skip_special_tokens=True)}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    if i == 2:  # Show only 3 examples\n",
    "        break\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
