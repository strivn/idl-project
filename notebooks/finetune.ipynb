{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune with full scale dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import wandb\n",
    "import datasets\n",
    "from transformers import TrainerCallback\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.enable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Working in '/ocean/projects/cis250068p/iwiryadi/idl-project'\n",
      "✓ Directory contains 'idl-project'\n"
     ]
    }
   ],
   "source": [
    "current_dir = os.path.abspath(os.getcwd())\n",
    "\n",
    "# Check if 'idl-project' is in the path\n",
    "if 'idl-project' not in current_dir:\n",
    "    raise Exception(\"Current directory '{current_dir}' is not within 'idl-project'\")\n",
    "\n",
    "print(f\"✓ Working in '{current_dir}'\")\n",
    "print(f\"✓ Directory contains 'idl-project'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Tesla V100-SXM2-32GB\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append paths for the src folder\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.getcwd(), 'idl-project')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Additional imports \n",
    "from src.model import load_fo_model\n",
    "from src.data import load_flan_dataset\n",
    "from src.utils import DEVICE, CACHE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"output/\"\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.mkdir(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_fo_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without custom pad token set to zero the convergence behaves weirdly\n",
    "# tokenizer.pad_token    = \"<|padding|>\"\n",
    "# tokenizer.pad_token_id = 1     \n",
    "\n",
    "# Use EOS token as padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without sampling: Who is Barack Obama?\n",
      "\n",
      "The question is, is he a man who has been in office for a long time?\n",
      "\n",
      "The answer is, yes.\n",
      "\n",
      "He is a man who has been in office for a long time.\n",
      "\n",
      "He is a man who has been in office for a long time.\n",
      "\n",
      "He is a man who has been in office for a long time.\n",
      "\n",
      "He is a man who has been in office for a long time.\n",
      "\n",
      "He is a man who has been in office for a long time.\n",
      "\n",
      "He is a man who has been in office for a long time.\n",
      "\n",
      "He is a man who has been in office for a long time.\n",
      "\n",
      "He is a\n",
      "---------------\n",
      "With sampling   : Who is Barack Obama?\n",
      "\n",
      "What we can learn from that discussion is that Obama is a much more conservative politician than Barack. As he gets down on the floor (even with some of the best comments on the debates before he was in office) we will learn that Obama is as capable as anyone of holding up the House of Representatives and the Senate.\n",
      "\n",
      "As for Obama’s role in the Republican Party and his lack of success of making his nomination appear a legitimate contender for the nomination, he will need to be at least a moderate presidential candidate to win over voters. Most of the candidates who have held on the national stage in the past are moderate, but a very moderate politician for president is probably a good candidate.\n",
      "\n",
      "To win\n",
      "\n",
      " ===============\n",
      "Without sampling: What is Carnegie Mellon University?\n",
      "\n",
      "The Carnegie Mellon University is a private, non-profit, non-profit, non-profit, non-profit, non-profit, non-profit, non-profit, non-profit, non-profit, non-profit, non-profit, non-profit, non-profit, non-profit, non-profit, non-profit, non-profit, non-profit, non-profit, non-profit, non-profit, non-profit, non-profit, non-profit, non-profit, non-profit, non-profit, non-profit, non-profit, non-profit, non-profit, non-profit, non-profit,\n",
      "---------------\n",
      "With sampling   : What is Carnegie Mellon University?\n",
      "\n",
      "Carnegie Mellon University (CMCU) is a public institution that teaches courses on financial management and financial analysis to undergraduate and graduate students, administrators, administrators, professors, researchers and other professional personnel. It provides an academic and practical college education throughout the United States. Its academic campuses serve approximately 2 million students annually, including 15,000 graduate students, 20,000 faculty and over 2,000 administrators. CCMCU’s educational programs are focused on teaching and research, research in finance, and management of student debts. Additionally, CCMCU’s courses offer courses in both finance and management, including courses on the design, construction, interpretation, and valuation of portfolio investments.\n",
      "\n",
      "Students can\n",
      "\n",
      " ===============\n",
      "Without sampling: Classify this restaurant review sentiment: 'The food was absolutely delicious but the service was extremely slow and the waiter seemed uninterested in helping us.'\n",
      "\n",
      "'I'm sure he was very helpful,' said the owner.\n",
      "\n",
      "'I'm sure he was very helpful,' said the manager.\n",
      "\n",
      "'I'm sure he was very helpful,' said the owner.\n",
      "\n",
      "'I'm sure he was very helpful,' said the manager.\n",
      "\n",
      "'I'm sure he was very helpful,' said the owner.\n",
      "\n",
      "'I'm sure he was very helpful,' said the manager.\n",
      "\n",
      "'I'm sure he was very helpful,' said the owner.\n",
      "\n",
      "'I'm sure he was very helpful,' said the manager.\n",
      "\n",
      "---------------\n",
      "With sampling   : Classify this restaurant review sentiment: 'The food was absolutely delicious but the service was extremely slow and the waiter seemed uninterested in helping us.'\n",
      "\n",
      "Kyle (pictured, with Toma) was able to book their tour of the city-cum-villiers.\n",
      "\n",
      "Bolton & Company\n",
      "\n",
      "Mileage: 28\n",
      "\n",
      "Duration: 7-8 hours\n",
      "\n",
      "Nutrition:\n",
      "\n",
      "Kyle had his meals at Balmoral; however, the food could be ordered more frequently in between meals, which, like food from a restaurant, did not come cheap.\n",
      "\n",
      "He also said they were more prepared in the morning, with fresh fruit and vegetables on the menu, which he wanted to\n",
      "\n",
      " ===============\n",
      "Without sampling: Compare and contrast Carnegie Mellon University's Computer Science and Information Systems programs in terms of research focus and career outcomes.\n",
      "\n",
      "The University of Pittsburgh's Computer Science and Information Systems program is a program of the University of Pittsburgh's College of Information Systems and Information Sciences. The program is designed to provide students with the skills and knowledge necessary to be successful in the field of information systems and information systems engineering.\n",
      "\n",
      "The University of Pittsburgh's Computer Science and Information Systems program is a program of the University of Pittsburgh's College of Information Systems and Information Sciences. The program is designed to provide students with the skills and knowledge necessary to be successful in the field of information systems and information systems engineering.\n",
      "\n",
      "The University of Pittsburgh's Computer Science and Information Systems program\n",
      "---------------\n",
      "With sampling   : Compare and contrast Carnegie Mellon University's Computer Science and Information Systems programs in terms of research focus and career outcomes.\n",
      "\n",
      "Introduction {#s1}\n",
      "============\n",
      "\n",
      "Computer programming techniques have been applied to a wide variety of tasks, including communication, information processing and visualization. Many of these problems are hard in nature, but are easily solved by mathematical solvers. Although the use of tools for their solving are not well known, they are common in physics and engineering and will be discussed in a later part of this paper.\n",
      "\n",
      "A new type of technique that provides a natural solution to computer programming problems that have been in practice very often is called [*computational solvers*]{}. This type of technique includes solving some specific problems in real-time computational\n",
      "\n",
      " ===============\n",
      "Without sampling: Summarize in one sentence: Dr. Sarah Chen, lead scientist on the mission, called it 'the most significant discovery in the history of space exploration.' The finding suggests that Mars once had a much more hospitable environment with liquid water and possibly a thicker atmosphere. The agency plans to send a sample return mission within the next five years to bring these fossils back to Earth for more detailed analysis. This discovery has profound implications for our understanding of how life might develop throughout the universe.\n",
      "\n",
      "The discovery of Mars is a major milestone in the history of space exploration. The discovery of Mars is a major milestone in the history of space exploration. The discovery of Mars is a major milestone in the history of space exploration. The discovery of Mars is\n",
      "---------------\n",
      "With sampling   : Summarize in one sentence: Dr. Sarah Chen, lead scientist on the mission, called it 'the most significant discovery in the history of space exploration.' The finding suggests that Mars once had a much more hospitable environment with liquid water and possibly a thicker atmosphere. The agency plans to send a sample return mission within the next five years to bring these fossils back to Earth for more detailed analysis. This discovery has profound implications for our understanding of how life might develop throughout the universe.\n",
      "\n",
      "But this will be too complicated. Once Earth's gravity is stopped, the spacecraft's gravitational field will return us to our ordinary selves, where life has been invented by our forefathers. In those days, when it came to living, we\n",
      "\n",
      " ===============\n"
     ]
    }
   ],
   "source": [
    "# Define test prompts\n",
    "test_prompts = [\n",
    "    \"Who is Barack Obama?\",\n",
    "    \"What is Carnegie Mellon University?\",\n",
    "    \"Classify this restaurant review sentiment: 'The food was absolutely delicious but the service was extremely slow and the waiter seemed uninterested in helping us.'\",\n",
    "    \"Compare and contrast Carnegie Mellon University's Computer Science and Information Systems programs in terms of research focus and career outcomes.\",\n",
    "    \"Summarize in one sentence: Dr. Sarah Chen, lead scientist on the mission, called it 'the most significant discovery in the history of space exploration.' The finding suggests that Mars once had a much more hospitable environment with liquid water and possibly a thicker atmosphere. The agency plans to send a sample return mission within the next five years to bring these fossils back to Earth for more detailed analysis. This discovery has profound implications for our understanding of how life might develop throughout the universe.\"\n",
    "]\n",
    "\n",
    "\n",
    "for test_input_string in test_prompts:\n",
    "    inputs = tokenizer(test_input_string, return_tensors=\"pt\").to(DEVICE)\n",
    "    # print(tokens[0])\n",
    "    tokens = model.generate(**inputs, max_length=150, pad_token_id=tokenizer.eos_token_id)\n",
    "    print(\"Without sampling: \" + tokenizer.decode(tokens[0]))\n",
    "    \n",
    "    print(\"---------------\")\n",
    "    tokens = model.generate(**inputs, max_length=150, pad_token_id=tokenizer.eos_token_id, do_sample=True)\n",
    "    print(\"With sampling   : \" + tokenizer.decode(tokens[0]))\n",
    "    \n",
    "    print(\"\\n===============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a2ef20aeed34d62a6802435931b809e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/2167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "dataset = load_flan_dataset().batch(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>\n",
      "0\n",
      "<|endoftext|>\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token)\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.eos_token)\n",
    "print(tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 1: AddedToken(\"<|padding|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 50254: AddedToken(\"                        \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50255: AddedToken(\"                       \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50256: AddedToken(\"                      \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50257: AddedToken(\"                     \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50258: AddedToken(\"                    \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50259: AddedToken(\"                   \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50260: AddedToken(\"                  \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50261: AddedToken(\"                 \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50262: AddedToken(\"                \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50263: AddedToken(\"               \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50264: AddedToken(\"              \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50265: AddedToken(\"             \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50266: AddedToken(\"            \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50267: AddedToken(\"           \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50268: AddedToken(\"          \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50269: AddedToken(\"         \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50270: AddedToken(\"        \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50271: AddedToken(\"       \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50272: AddedToken(\"      \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50273: AddedToken(\"     \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50274: AddedToken(\"    \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50275: AddedToken(\"   \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " 50276: AddedToken(\"  \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure tokenizer properly \n",
    "\n",
    "# - https://github.com/EleutherAI/pythia/issues/156 \n",
    "#   mentioned it's okay to set it to eos\n",
    "\n",
    "# - https://huggingface.co/EleutherAI/pythia-14m/discussions/4, \n",
    "#   we can see the tokenizer pad token from tokenizer.added_tokens_decoder \n",
    "tokenizer.added_tokens_decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['inputs', 'targets', '_template_idx', '_task_source', '_task_name', '_template_type'])\n"
     ]
    }
   ],
   "source": [
    "examples = next(iter(dataset))\n",
    "print(examples.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QUESTION] If \"A brown dog is on the ground growling.\" does that mean that \"A dog is warning a stranger.\"?\n",
      "Options:\n",
      "- yes\n",
      "- it is not possible to tell\n",
      "- no\n",
      "No way to tell if the dog is warning a stranger.\n",
      "The answer is it is not possible to tell.\n",
      "\n",
      "Q: If \"A man is sitting in a brown chair with a woman sitting on his lap smiling for the camera.\" does that mean that \"A couple are taking an engagement picture.\"?\n",
      "Options:\n",
      "- yes\n",
      "- it is not possible to tell\n",
      "- no\n",
      "A: Two people smiling for a camera do not mean that they are engagement pictures.\n",
      "The answer is it is not possible to tell.\n",
      "\n",
      "QUESTION: Premise: \"Two men yellow are fishing from a small red box.\"\n",
      "Based on this premise, can we conclude that the hypothesis \"A father and son sit on a box fishing.\" is true?\n",
      "Options:\n",
      "- yes\n",
      "- it is not possible to tell\n",
      "- no\n",
      "\n",
      "Let's solve it slowly: The two men do not have to be father and son.\n",
      "The answer is it is not possible to tell.\n",
      "\n",
      "[QUESTION] Given the sentence \"White greyhound racing as dog number 1.\" can we conclude that \"A dog racing.\"?\n",
      "Options:\n",
      "- yes\n",
      "- it is not possible to tell\n",
      "- no\n",
      "A white greyhound is a dog; that it is racing is firmly established.\n",
      "The answer is yes.\n",
      "\n",
      "Q: If \"A woman wearing a green shirt and a boy wearing a red shirt are washing clothes in a creek while sitting on a log.\" does that mean that \"The woman and boy are washing clothes in the creek.\"?\n",
      "Options:\n",
      "- yes\n",
      "- it is not possible to tell\n",
      "- no\n",
      "A: A woman and a boy are washing clothes in a creek.\n",
      "The answer is yes.\n",
      "\n",
      "[QUESTION] Can we conclude from \"A man in a white shirt and jeans.\" that \"Breakdancing in a city street with a crowd watching.\"?\n",
      "Options:\n",
      "- yes\n",
      "- no\n",
      "- it is not possible to tell\n",
      "\n",
      "======\n",
      "A man is break dancing and collecting a lot of money from a crowd.\n",
      "The answer is it is not possible to tell.\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "combined_texts = []\n",
    "for inp, tgt in zip(examples['inputs'], examples['targets']):\n",
    "    # Handle potential None values or empty strings\n",
    "    inp_text = inp if inp is not None else \"\"\n",
    "    tgt_text = tgt if tgt is not None else \"\"\n",
    "    combined_texts.append(f\"{inp_text}\\n======\\n{tgt_text}\\n{tokenizer.eos_token}\")\n",
    "\n",
    "print(combined_texts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_forward(examples):\n",
    "    # Combine inputs, outputs, and eos token for each example in the batch\n",
    "    combined_texts = []\n",
    "    \n",
    "    for inp, tgt in zip(examples['inputs'], examples['targets']):\n",
    "        \n",
    "        # Handle potential None values or empty strings\n",
    "        inp_text = inp if inp is not None else \"\"\n",
    "        tgt_text = tgt if tgt is not None else \"\"\n",
    "        \n",
    "        # Use separator between input and target\n",
    "        combined_texts.append(f\"{inp_text}\\n{tgt_text}{tokenizer.eos_token}\")\n",
    "    \n",
    "    # Tokenize the entire batch at once\n",
    "    tokenized = tokenizer(\n",
    "        combined_texts,\n",
    "        truncation=True,\n",
    "        max_length=768,\n",
    "        padding=\"longest\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    input_ids = tokenized[\"input_ids\"]\n",
    "    attention_mask = tokenized[\"attention_mask\"]\n",
    "    labels = input_ids.clone()\n",
    "    \n",
    "    # Shift labels to the left (for each sequence in the batch)\n",
    "    labels[:, :-1] = input_ids[:, 1:]\n",
    "    # Add padding token as the last prediction target\n",
    "    # labels[:, -1] = tokenizer.pad_token_id\n",
    "    labels[:, -1] = -100  \n",
    "\n",
    "\n",
    "    # Mask pad tokens in labels\n",
    "    labels[input_ids == tokenizer.pad_token_id] = -100\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids, \n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "    \n",
    "# QUESTION: Do we need to mask / weight the prompt tokens? Discussion: https://towardsdatascience.com/to-mask-or-not-to-mask-the-effect-of-prompt-tokens-on-instruction-tuning-016f85fd67f4/\n",
    "# Didn't find guidance through skimming the FLAN papers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  510,   637,   320,  ...,     0,     0,     0],\n",
       "         [   60, 26310,  2449,  ...,     0,     0,     0],\n",
       "         [   43,   991,    27,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [   50,    27, 11271,  ...,    15,   432,   253],\n",
       "         [ 9301, 19782,    27,  ...,     0,     0,     0],\n",
       "         [19751,   275,  1984,  ...,     0,     0,     0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'labels': tensor([[  637,   320,  4645,  ...,  -100,  -100,  -100],\n",
       "         [26310,  2449,    62,  ...,  -100,  -100,  -100],\n",
       "         [  991,    27,  6758,  ...,  -100,  -100,  -100],\n",
       "         ...,\n",
       "         [   27, 11271,   486,  ...,   432,   253,  -100],\n",
       "         [19782,    27, 13343,  ...,  -100,  -100,  -100],\n",
       "         [  275,  1984,    27,  ...,  -100,  -100,  -100]])}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_forward(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([16, 768])\n",
      "attention_mask shape: torch.Size([16, 768])\n",
      "labels shape: torch.Size([16, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  510,   637,   320,  ...,     0,     0,     0],\n",
       "         [   60, 26310,  2449,  ...,     0,     0,     0],\n",
       "         [   43,   991,    27,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [   50,    27, 11271,  ...,    15,   432,   253],\n",
       "         [ 9301, 19782,    27,  ...,     0,     0,     0],\n",
       "         [19751,   275,  1984,  ...,     0,     0,     0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'labels': tensor([[  637,   320,  4645,  ...,  -100,  -100,  -100],\n",
       "         [26310,  2449,    62,  ...,  -100,  -100,  -100],\n",
       "         [  991,    27,  6758,  ...,  -100,  -100,  -100],\n",
       "         ...,\n",
       "         [   27, 11271,   486,  ...,   432,   253,  -100],\n",
       "         [19782,    27, 13343,  ...,  -100,  -100,  -100],\n",
       "         [  275,  1984,    27,  ...,  -100,  -100,  -100]])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed = preprocess_forward(examples)\n",
    "print(\"input_ids shape: \" + str(preprocessed['input_ids'].shape))\n",
    "print(\"attention_mask shape: \" + str(preprocessed['attention_mask'].shape))\n",
    "print(\"labels shape: \" + str(preprocessed['labels'].shape))\n",
    "\n",
    "preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jax: Which of the following sentences is nonsensical?\n",
      "Options:\n",
      "- Sentence A: \"He hasn't bathed in a week so he's dirty\"\n",
      "- Sentence B: \"He hasn't bathed in a week so he's clean\"\n",
      "\n",
      "Alex: Chain of thought: A person who has not showered for a week can only be dirty. The answer is Sentence B.\n",
      "\n",
      "Jax: Pick which sentence is not logical.\n",
      "Options:\n",
      "- Sentence A: \"The sun goes around the earth\"\n",
      "- Sentence B: \"The earth goes around the sun\"\n",
      "\n",
      "Alex: Chain of thought: The sun does not move, the earth rotates. The answer is Sentence A.\n",
      "\n",
      "Jax: Of the below sentences, which one does *not* make sense?\n",
      "Options:\n",
      "- Sentence A: \"He wore watch on his wrist\"\n",
      "- Sentence B: \"he wore watch on his nose\"\n",
      "\n",
      "Alex:\n",
      "Chain of thought: A watch is not worn on one's nose. The answer is Sentence B.\n",
      "Index  | Token                | ID       | Mask   | Label   \n",
      "------------------------------------------------------------\n",
      "0      | J                    | 43       | 1      | 991     \n",
      "1      | ax                   | 991      | 1      | 27      \n",
      "2      | :                    | 27       | 1      | 6758    \n",
      "3      |  Which               | 6758     | 1      | 273     \n",
      "4      |  of                  | 273      | 1      | 253     \n",
      "5      |  the                 | 253      | 1      | 1563    \n",
      "6      |  following           | 1563     | 1      | 14683   \n",
      "7      |  sentences           | 14683    | 1      | 310     \n",
      "8      |  is                  | 310      | 1      | 14122   \n",
      "9      |  nons                | 14122    | 1      | 561     \n",
      "10     | ens                  | 561      | 1      | 474     \n",
      "11     | ical                 | 474      | 1      | 32      \n",
      "12     | ?                    | 32       | 1      | 187     \n",
      "13     | \\n                   | 187      | 1      | 10976   \n",
      "14     | Options              | 10976    | 1      | 27      \n",
      "15     | :                    | 27       | 1      | 187     \n",
      "16     | \\n                   | 187      | 1      | 14      \n",
      "17     | -                    | 14       | 1      | 20580   \n",
      "18     |  Sent                | 20580    | 1      | 566     \n",
      "19     | ence                 | 566      | 1      | 329     \n",
      "20     |  A                   | 329      | 1      | 27      \n",
      "21     | :                    | 27       | 1      | 346     \n",
      "22     |  \"                   | 346      | 1      | 1328    \n",
      "23     | He                   | 1328     | 1      | 11210   \n",
      "24     |  hasn                | 11210    | 1      | 626     \n",
      "25     | 't                   | 626      | 1      | 10464   \n",
      "26     |  bat                 | 10464    | 1      | 742     \n",
      "27     | hed                  | 742      | 1      | 275     \n",
      "28     |  in                  | 275      | 1      | 247     \n",
      "29     |  a                   | 247      | 1      | 2129    \n",
      "30     |  week                | 2129     | 1      | 594     \n",
      "31     |  so                  | 594      | 1      | 344     \n",
      "32     |  he                  | 344      | 1      | 434     \n",
      "33     | 's                   | 434      | 1      | 16076   \n",
      "34     |  dirty               | 16076    | 1      | 3       \n",
      "35     | \"                    | 3        | 1      | 187     \n",
      "36     | \\n                   | 187      | 1      | 14      \n",
      "37     | -                    | 14       | 1      | 20580   \n",
      "38     |  Sent                | 20580    | 1      | 566     \n",
      "39     | ence                 | 566      | 1      | 378     \n",
      "40     |  B                   | 378      | 1      | 27      \n",
      "41     | :                    | 27       | 1      | 346     \n",
      "42     |  \"                   | 346      | 1      | 1328    \n",
      "43     | He                   | 1328     | 1      | 11210   \n",
      "44     |  hasn                | 11210    | 1      | 626     \n",
      "45     | 't                   | 626      | 1      | 10464   \n",
      "46     |  bat                 | 10464    | 1      | 742     \n",
      "47     | hed                  | 742      | 1      | 275     \n",
      "48     |  in                  | 275      | 1      | 247     \n",
      "49     |  a                   | 247      | 1      | 2129    \n",
      "50     |  week                | 2129     | 1      | 594     \n",
      "51     |  so                  | 594      | 1      | 344     \n",
      "52     |  he                  | 344      | 1      | 434     \n",
      "53     | 's                   | 434      | 1      | 4076    \n",
      "54     |  clean               | 4076     | 1      | 3       \n",
      "55     | \"                    | 3        | 1      | 187     \n",
      "56     | \\n                   | 187      | 1      | 187     \n",
      "57     | \\n                   | 187      | 1      | 18552   \n",
      "58     | Alex                 | 18552    | 1      | 27      \n",
      "59     | :                    | 27       | 1      | 37239   \n",
      "60     |  Chain               | 37239    | 1      | 273     \n",
      "61     |  of                  | 273      | 1      | 1869    \n",
      "62     |  thought             | 1869     | 1      | 27      \n",
      "63     | :                    | 27       | 1      | 329     \n",
      "64     |  A                   | 329      | 1      | 1436    \n",
      "65     |  person              | 1436     | 1      | 665     \n",
      "66     |  who                 | 665      | 1      | 556     \n",
      "67     |  has                 | 556      | 1      | 417     \n",
      "68     |  not                 | 417      | 1      | 921     \n",
      "69     |  show                | 921      | 1      | 2122    \n",
      "70     | ered                 | 2122     | 1      | 323     \n",
      "71     |  for                 | 323      | 1      | 247     \n",
      "72     |  a                   | 247      | 1      | 2129    \n",
      "73     |  week                | 2129     | 1      | 476     \n",
      "74     |  can                 | 476      | 1      | 760     \n",
      "75     |  only                | 760      | 1      | 320     \n",
      "76     |  be                  | 320      | 1      | 16076   \n",
      "77     |  dirty               | 16076    | 1      | 15      \n",
      "78     | .                    | 15       | 1      | 380     \n",
      "79     |  The                 | 380      | 1      | 3662    \n",
      "80     |  answer              | 3662     | 1      | 310     \n",
      "81     |  is                  | 310      | 1      | 20580   \n",
      "82     |  Sent                | 20580    | 1      | 566     \n",
      "83     | ence                 | 566      | 1      | 378     \n",
      "84     |  B                   | 378      | 1      | 15      \n",
      "85     | .                    | 15       | 1      | 187     \n",
      "86     | \\n                   | 187      | 1      | 187     \n",
      "87     | \\n                   | 187      | 1      | 43      \n",
      "88     | J                    | 43       | 1      | 991     \n",
      "89     | ax                   | 991      | 1      | 27      \n",
      "90     | :                    | 27       | 1      | 20745   \n",
      "91     |  Pick                | 20745    | 1      | 534     \n",
      "92     |  which               | 534      | 1      | 6197    \n",
      "93     |  sentence            | 6197     | 1      | 310     \n",
      "94     |  is                  | 310      | 1      | 417     \n",
      "95     |  not                 | 417      | 1      | 13760   \n",
      "96     |  logical             | 13760    | 1      | 15      \n",
      "97     | .                    | 15       | 1      | 187     \n",
      "98     | \\n                   | 187      | 1      | 10976   \n",
      "99     | Options              | 10976    | 1      | 27      \n",
      "100    | :                    | 27       | 1      | 187     \n",
      "101    | \\n                   | 187      | 1      | 14      \n",
      "102    | -                    | 14       | 1      | 20580   \n",
      "103    |  Sent                | 20580    | 1      | 566     \n",
      "104    | ence                 | 566      | 1      | 329     \n",
      "105    |  A                   | 329      | 1      | 27      \n",
      "106    | :                    | 27       | 1      | 346     \n",
      "107    |  \"                   | 346      | 1      | 510     \n",
      "108    | The                  | 510      | 1      | 5101    \n",
      "109    |  sun                 | 5101     | 1      | 4566    \n",
      "110    |  goes                | 4566     | 1      | 1475    \n",
      "111    |  around              | 1475     | 1      | 253     \n",
      "112    |  the                 | 253      | 1      | 6149    \n",
      "113    |  earth               | 6149     | 1      | 3       \n",
      "114    | \"                    | 3        | 1      | 187     \n",
      "115    | \\n                   | 187      | 1      | 14      \n",
      "116    | -                    | 14       | 1      | 20580   \n",
      "117    |  Sent                | 20580    | 1      | 566     \n",
      "118    | ence                 | 566      | 1      | 378     \n",
      "119    |  B                   | 378      | 1      | 27      \n",
      "120    | :                    | 27       | 1      | 346     \n",
      "121    |  \"                   | 346      | 1      | 510     \n",
      "122    | The                  | 510      | 1      | 6149    \n",
      "123    |  earth               | 6149     | 1      | 4566    \n",
      "124    |  goes                | 4566     | 1      | 1475    \n",
      "125    |  around              | 1475     | 1      | 253     \n",
      "126    |  the                 | 253      | 1      | 5101    \n",
      "127    |  sun                 | 5101     | 1      | 3       \n",
      "128    | \"                    | 3        | 1      | 187     \n",
      "129    | \\n                   | 187      | 1      | 187     \n",
      "130    | \\n                   | 187      | 1      | 18552   \n",
      "131    | Alex                 | 18552    | 1      | 27      \n",
      "132    | :                    | 27       | 1      | 37239   \n",
      "133    |  Chain               | 37239    | 1      | 273     \n",
      "134    |  of                  | 273      | 1      | 1869    \n",
      "135    |  thought             | 1869     | 1      | 27      \n",
      "136    | :                    | 27       | 1      | 380     \n",
      "137    |  The                 | 380      | 1      | 5101    \n",
      "138    |  sun                 | 5101     | 1      | 1057    \n",
      "139    |  does                | 1057     | 1      | 417     \n",
      "140    |  not                 | 417      | 1      | 2118    \n",
      "141    |  move                | 2118     | 1      | 13      \n",
      "142    | ,                    | 13       | 1      | 253     \n",
      "143    |  the                 | 253      | 1      | 6149    \n",
      "144    |  earth               | 6149     | 1      | 45933   \n",
      "145    |  rotates             | 45933    | 1      | 15      \n",
      "146    | .                    | 15       | 1      | 380     \n",
      "147    |  The                 | 380      | 1      | 3662    \n",
      "148    |  answer              | 3662     | 1      | 310     \n",
      "149    |  is                  | 310      | 1      | 20580   \n",
      "150    |  Sent                | 20580    | 1      | 566     \n",
      "151    | ence                 | 566      | 1      | 329     \n",
      "152    |  A                   | 329      | 1      | 15      \n",
      "153    | .                    | 15       | 1      | 187     \n",
      "154    | \\n                   | 187      | 1      | 187     \n",
      "155    | \\n                   | 187      | 1      | 43      \n",
      "156    | J                    | 43       | 1      | 991     \n",
      "157    | ax                   | 991      | 1      | 27      \n",
      "158    | :                    | 27       | 1      | 4683    \n",
      "159    |  Of                  | 4683     | 1      | 253     \n",
      "160    |  the                 | 253      | 1      | 2708    \n",
      "161    |  below               | 2708     | 1      | 14683   \n",
      "162    |  sentences           | 14683    | 1      | 13      \n",
      "163    | ,                    | 13       | 1      | 534     \n",
      "164    |  which               | 534      | 1      | 581     \n",
      "165    |  one                 | 581      | 1      | 1057    \n",
      "166    |  does                | 1057     | 1      | 475     \n",
      "167    |  *                   | 475      | 1      | 1439    \n",
      "168    | not                  | 1439     | 1      | 11      \n",
      "169    | *                    | 11       | 1      | 1056    \n",
      "170    |  make                | 1056     | 1      | 3282    \n",
      "171    |  sense               | 3282     | 1      | 32      \n",
      "172    | ?                    | 32       | 1      | 187     \n",
      "173    | \\n                   | 187      | 1      | 10976   \n",
      "174    | Options              | 10976    | 1      | 27      \n",
      "175    | :                    | 27       | 1      | 187     \n",
      "176    | \\n                   | 187      | 1      | 14      \n",
      "177    | -                    | 14       | 1      | 20580   \n",
      "178    |  Sent                | 20580    | 1      | 566     \n",
      "179    | ence                 | 566      | 1      | 329     \n",
      "180    |  A                   | 329      | 1      | 27      \n",
      "181    | :                    | 27       | 1      | 346     \n",
      "182    |  \"                   | 346      | 1      | 1328    \n",
      "183    | He                   | 1328     | 1      | 14782   \n",
      "184    |  wore                | 14782    | 1      | 3698    \n",
      "185    |  watch               | 3698     | 1      | 327     \n",
      "186    |  on                  | 327      | 1      | 521     \n",
      "187    |  his                 | 521      | 1      | 20051   \n",
      "188    |  wrist               | 20051    | 1      | 3       \n",
      "189    | \"                    | 3        | 1      | 187     \n",
      "190    | \\n                   | 187      | 1      | 14      \n",
      "191    | -                    | 14       | 1      | 20580   \n",
      "192    |  Sent                | 20580    | 1      | 566     \n",
      "193    | ence                 | 566      | 1      | 378     \n",
      "194    |  B                   | 378      | 1      | 27      \n",
      "195    | :                    | 27       | 1      | 346     \n",
      "196    |  \"                   | 346      | 1      | 248     \n",
      "197    | he                   | 248      | 1      | 14782   \n",
      "198    |  wore                | 14782    | 1      | 3698    \n",
      "199    |  watch               | 3698     | 1      | 327     \n",
      "200    |  on                  | 327      | 1      | 521     \n",
      "201    |  his                 | 521      | 1      | 11480   \n",
      "202    |  nose                | 11480    | 1      | 3       \n",
      "203    | \"                    | 3        | 1      | 187     \n",
      "204    | \\n                   | 187      | 1      | 187     \n",
      "205    | \\n                   | 187      | 1      | 18552   \n",
      "206    | Alex                 | 18552    | 1      | 27      \n",
      "207    | :                    | 27       | 1      | 187     \n",
      "208    | \\n                   | 187      | 1      | 26268   \n",
      "209    | Chain                | 26268    | 1      | 273     \n",
      "210    |  of                  | 273      | 1      | 1869    \n",
      "211    |  thought             | 1869     | 1      | 27      \n",
      "212    | :                    | 27       | 1      | 329     \n",
      "213    |  A                   | 329      | 1      | 3698    \n",
      "214    |  watch               | 3698     | 1      | 310     \n",
      "215    |  is                  | 310      | 1      | 417     \n",
      "216    |  not                 | 417      | 1      | 16332   \n",
      "217    |  worn                | 16332    | 1      | 327     \n",
      "218    |  on                  | 327      | 1      | 581     \n",
      "219    |  one                 | 581      | 1      | 434     \n",
      "220    | 's                   | 434      | 1      | 11480   \n",
      "221    |  nose                | 11480    | 1      | 15      \n",
      "222    | .                    | 15       | 1      | 380     \n",
      "223    |  The                 | 380      | 1      | 3662    \n",
      "224    |  answer              | 3662     | 1      | 310     \n",
      "225    |  is                  | 310      | 1      | 20580   \n",
      "226    |  Sent                | 20580    | 1      | 566     \n",
      "227    | ence                 | 566      | 1      | 378     \n",
      "228    |  B                   | 378      | 1      | 15      \n",
      "229    | .                    | 15       | 1      | 0       \n",
      "230    | <|endoftext|>        | 0        | 1      | ignored \n",
      "231    | <|endoftext|>        | 0        | 0      | ignored \n",
      "232    | <|endoftext|>        | 0        | 0      | ignored \n",
      "233    | <|endoftext|>        | 0        | 0      | ignored \n",
      "234    | <|endoftext|>        | 0        | 0      | ignored \n",
      "235    | <|endoftext|>        | 0        | 0      | ignored \n",
      "236    | <|endoftext|>        | 0        | 0      | ignored \n",
      "237    | <|endoftext|>        | 0        | 0      | ignored \n",
      "238    | <|endoftext|>        | 0        | 0      | ignored \n",
      "239    | <|endoftext|>        | 0        | 0      | ignored \n",
      "240    | <|endoftext|>        | 0        | 0      | ignored \n",
      "241    | <|endoftext|>        | 0        | 0      | ignored \n",
      "242    | <|endoftext|>        | 0        | 0      | ignored \n",
      "243    | <|endoftext|>        | 0        | 0      | ignored \n",
      "244    | <|endoftext|>        | 0        | 0      | ignored \n",
      "245    | <|endoftext|>        | 0        | 0      | ignored \n",
      "246    | <|endoftext|>        | 0        | 0      | ignored \n",
      "247    | <|endoftext|>        | 0        | 0      | ignored \n",
      "248    | <|endoftext|>        | 0        | 0      | ignored \n",
      "249    | <|endoftext|>        | 0        | 0      | ignored \n",
      "250    | <|endoftext|>        | 0        | 0      | ignored \n",
      "251    | <|endoftext|>        | 0        | 0      | ignored \n",
      "252    | <|endoftext|>        | 0        | 0      | ignored \n",
      "253    | <|endoftext|>        | 0        | 0      | ignored \n",
      "254    | <|endoftext|>        | 0        | 0      | ignored \n",
      "255    | <|endoftext|>        | 0        | 0      | ignored \n",
      "256    | <|endoftext|>        | 0        | 0      | ignored \n",
      "257    | <|endoftext|>        | 0        | 0      | ignored \n",
      "258    | <|endoftext|>        | 0        | 0      | ignored \n",
      "259    | <|endoftext|>        | 0        | 0      | ignored \n",
      "260    | <|endoftext|>        | 0        | 0      | ignored \n",
      "261    | <|endoftext|>        | 0        | 0      | ignored \n",
      "262    | <|endoftext|>        | 0        | 0      | ignored \n",
      "263    | <|endoftext|>        | 0        | 0      | ignored \n",
      "264    | <|endoftext|>        | 0        | 0      | ignored \n",
      "265    | <|endoftext|>        | 0        | 0      | ignored \n",
      "266    | <|endoftext|>        | 0        | 0      | ignored \n",
      "267    | <|endoftext|>        | 0        | 0      | ignored \n",
      "268    | <|endoftext|>        | 0        | 0      | ignored \n",
      "269    | <|endoftext|>        | 0        | 0      | ignored \n",
      "270    | <|endoftext|>        | 0        | 0      | ignored \n",
      "271    | <|endoftext|>        | 0        | 0      | ignored \n",
      "272    | <|endoftext|>        | 0        | 0      | ignored \n",
      "273    | <|endoftext|>        | 0        | 0      | ignored \n",
      "274    | <|endoftext|>        | 0        | 0      | ignored \n",
      "275    | <|endoftext|>        | 0        | 0      | ignored \n",
      "276    | <|endoftext|>        | 0        | 0      | ignored \n",
      "277    | <|endoftext|>        | 0        | 0      | ignored \n",
      "278    | <|endoftext|>        | 0        | 0      | ignored \n",
      "279    | <|endoftext|>        | 0        | 0      | ignored \n",
      "280    | <|endoftext|>        | 0        | 0      | ignored \n",
      "281    | <|endoftext|>        | 0        | 0      | ignored \n",
      "282    | <|endoftext|>        | 0        | 0      | ignored \n",
      "283    | <|endoftext|>        | 0        | 0      | ignored \n",
      "284    | <|endoftext|>        | 0        | 0      | ignored \n",
      "285    | <|endoftext|>        | 0        | 0      | ignored \n",
      "286    | <|endoftext|>        | 0        | 0      | ignored \n",
      "287    | <|endoftext|>        | 0        | 0      | ignored \n",
      "288    | <|endoftext|>        | 0        | 0      | ignored \n",
      "289    | <|endoftext|>        | 0        | 0      | ignored \n",
      "290    | <|endoftext|>        | 0        | 0      | ignored \n",
      "291    | <|endoftext|>        | 0        | 0      | ignored \n",
      "292    | <|endoftext|>        | 0        | 0      | ignored \n",
      "293    | <|endoftext|>        | 0        | 0      | ignored \n",
      "294    | <|endoftext|>        | 0        | 0      | ignored \n",
      "295    | <|endoftext|>        | 0        | 0      | ignored \n",
      "296    | <|endoftext|>        | 0        | 0      | ignored \n",
      "297    | <|endoftext|>        | 0        | 0      | ignored \n",
      "298    | <|endoftext|>        | 0        | 0      | ignored \n",
      "299    | <|endoftext|>        | 0        | 0      | ignored \n",
      "300    | <|endoftext|>        | 0        | 0      | ignored \n",
      "301    | <|endoftext|>        | 0        | 0      | ignored \n",
      "302    | <|endoftext|>        | 0        | 0      | ignored \n",
      "303    | <|endoftext|>        | 0        | 0      | ignored \n",
      "304    | <|endoftext|>        | 0        | 0      | ignored \n",
      "305    | <|endoftext|>        | 0        | 0      | ignored \n",
      "306    | <|endoftext|>        | 0        | 0      | ignored \n",
      "307    | <|endoftext|>        | 0        | 0      | ignored \n",
      "308    | <|endoftext|>        | 0        | 0      | ignored \n",
      "309    | <|endoftext|>        | 0        | 0      | ignored \n",
      "310    | <|endoftext|>        | 0        | 0      | ignored \n",
      "311    | <|endoftext|>        | 0        | 0      | ignored \n",
      "312    | <|endoftext|>        | 0        | 0      | ignored \n",
      "313    | <|endoftext|>        | 0        | 0      | ignored \n",
      "314    | <|endoftext|>        | 0        | 0      | ignored \n",
      "315    | <|endoftext|>        | 0        | 0      | ignored \n",
      "316    | <|endoftext|>        | 0        | 0      | ignored \n",
      "317    | <|endoftext|>        | 0        | 0      | ignored \n",
      "318    | <|endoftext|>        | 0        | 0      | ignored \n",
      "319    | <|endoftext|>        | 0        | 0      | ignored \n",
      "320    | <|endoftext|>        | 0        | 0      | ignored \n",
      "321    | <|endoftext|>        | 0        | 0      | ignored \n",
      "322    | <|endoftext|>        | 0        | 0      | ignored \n",
      "323    | <|endoftext|>        | 0        | 0      | ignored \n",
      "324    | <|endoftext|>        | 0        | 0      | ignored \n",
      "325    | <|endoftext|>        | 0        | 0      | ignored \n",
      "326    | <|endoftext|>        | 0        | 0      | ignored \n",
      "327    | <|endoftext|>        | 0        | 0      | ignored \n",
      "328    | <|endoftext|>        | 0        | 0      | ignored \n",
      "329    | <|endoftext|>        | 0        | 0      | ignored \n",
      "330    | <|endoftext|>        | 0        | 0      | ignored \n",
      "331    | <|endoftext|>        | 0        | 0      | ignored \n",
      "332    | <|endoftext|>        | 0        | 0      | ignored \n",
      "333    | <|endoftext|>        | 0        | 0      | ignored \n",
      "334    | <|endoftext|>        | 0        | 0      | ignored \n",
      "335    | <|endoftext|>        | 0        | 0      | ignored \n",
      "336    | <|endoftext|>        | 0        | 0      | ignored \n",
      "337    | <|endoftext|>        | 0        | 0      | ignored \n",
      "338    | <|endoftext|>        | 0        | 0      | ignored \n",
      "339    | <|endoftext|>        | 0        | 0      | ignored \n",
      "340    | <|endoftext|>        | 0        | 0      | ignored \n",
      "341    | <|endoftext|>        | 0        | 0      | ignored \n",
      "342    | <|endoftext|>        | 0        | 0      | ignored \n",
      "343    | <|endoftext|>        | 0        | 0      | ignored \n",
      "344    | <|endoftext|>        | 0        | 0      | ignored \n",
      "345    | <|endoftext|>        | 0        | 0      | ignored \n",
      "346    | <|endoftext|>        | 0        | 0      | ignored \n",
      "347    | <|endoftext|>        | 0        | 0      | ignored \n",
      "348    | <|endoftext|>        | 0        | 0      | ignored \n",
      "349    | <|endoftext|>        | 0        | 0      | ignored \n",
      "350    | <|endoftext|>        | 0        | 0      | ignored \n",
      "351    | <|endoftext|>        | 0        | 0      | ignored \n",
      "352    | <|endoftext|>        | 0        | 0      | ignored \n",
      "353    | <|endoftext|>        | 0        | 0      | ignored \n",
      "354    | <|endoftext|>        | 0        | 0      | ignored \n",
      "355    | <|endoftext|>        | 0        | 0      | ignored \n",
      "356    | <|endoftext|>        | 0        | 0      | ignored \n",
      "357    | <|endoftext|>        | 0        | 0      | ignored \n",
      "358    | <|endoftext|>        | 0        | 0      | ignored \n",
      "359    | <|endoftext|>        | 0        | 0      | ignored \n",
      "360    | <|endoftext|>        | 0        | 0      | ignored \n",
      "361    | <|endoftext|>        | 0        | 0      | ignored \n",
      "362    | <|endoftext|>        | 0        | 0      | ignored \n",
      "363    | <|endoftext|>        | 0        | 0      | ignored \n",
      "364    | <|endoftext|>        | 0        | 0      | ignored \n",
      "365    | <|endoftext|>        | 0        | 0      | ignored \n",
      "366    | <|endoftext|>        | 0        | 0      | ignored \n",
      "367    | <|endoftext|>        | 0        | 0      | ignored \n",
      "368    | <|endoftext|>        | 0        | 0      | ignored \n",
      "369    | <|endoftext|>        | 0        | 0      | ignored \n",
      "370    | <|endoftext|>        | 0        | 0      | ignored \n",
      "371    | <|endoftext|>        | 0        | 0      | ignored \n",
      "372    | <|endoftext|>        | 0        | 0      | ignored \n",
      "373    | <|endoftext|>        | 0        | 0      | ignored \n",
      "374    | <|endoftext|>        | 0        | 0      | ignored \n",
      "375    | <|endoftext|>        | 0        | 0      | ignored \n",
      "376    | <|endoftext|>        | 0        | 0      | ignored \n",
      "377    | <|endoftext|>        | 0        | 0      | ignored \n",
      "378    | <|endoftext|>        | 0        | 0      | ignored \n",
      "379    | <|endoftext|>        | 0        | 0      | ignored \n",
      "380    | <|endoftext|>        | 0        | 0      | ignored \n",
      "381    | <|endoftext|>        | 0        | 0      | ignored \n",
      "382    | <|endoftext|>        | 0        | 0      | ignored \n",
      "383    | <|endoftext|>        | 0        | 0      | ignored \n",
      "384    | <|endoftext|>        | 0        | 0      | ignored \n",
      "385    | <|endoftext|>        | 0        | 0      | ignored \n",
      "386    | <|endoftext|>        | 0        | 0      | ignored \n",
      "387    | <|endoftext|>        | 0        | 0      | ignored \n",
      "388    | <|endoftext|>        | 0        | 0      | ignored \n",
      "389    | <|endoftext|>        | 0        | 0      | ignored \n",
      "390    | <|endoftext|>        | 0        | 0      | ignored \n",
      "391    | <|endoftext|>        | 0        | 0      | ignored \n",
      "392    | <|endoftext|>        | 0        | 0      | ignored \n",
      "393    | <|endoftext|>        | 0        | 0      | ignored \n",
      "394    | <|endoftext|>        | 0        | 0      | ignored \n",
      "395    | <|endoftext|>        | 0        | 0      | ignored \n",
      "396    | <|endoftext|>        | 0        | 0      | ignored \n",
      "397    | <|endoftext|>        | 0        | 0      | ignored \n",
      "398    | <|endoftext|>        | 0        | 0      | ignored \n",
      "399    | <|endoftext|>        | 0        | 0      | ignored \n",
      "400    | <|endoftext|>        | 0        | 0      | ignored \n",
      "401    | <|endoftext|>        | 0        | 0      | ignored \n",
      "402    | <|endoftext|>        | 0        | 0      | ignored \n",
      "403    | <|endoftext|>        | 0        | 0      | ignored \n",
      "404    | <|endoftext|>        | 0        | 0      | ignored \n",
      "405    | <|endoftext|>        | 0        | 0      | ignored \n",
      "406    | <|endoftext|>        | 0        | 0      | ignored \n",
      "407    | <|endoftext|>        | 0        | 0      | ignored \n",
      "408    | <|endoftext|>        | 0        | 0      | ignored \n",
      "409    | <|endoftext|>        | 0        | 0      | ignored \n",
      "410    | <|endoftext|>        | 0        | 0      | ignored \n",
      "411    | <|endoftext|>        | 0        | 0      | ignored \n",
      "412    | <|endoftext|>        | 0        | 0      | ignored \n",
      "413    | <|endoftext|>        | 0        | 0      | ignored \n",
      "414    | <|endoftext|>        | 0        | 0      | ignored \n",
      "415    | <|endoftext|>        | 0        | 0      | ignored \n",
      "416    | <|endoftext|>        | 0        | 0      | ignored \n",
      "417    | <|endoftext|>        | 0        | 0      | ignored \n",
      "418    | <|endoftext|>        | 0        | 0      | ignored \n",
      "419    | <|endoftext|>        | 0        | 0      | ignored \n",
      "420    | <|endoftext|>        | 0        | 0      | ignored \n",
      "421    | <|endoftext|>        | 0        | 0      | ignored \n",
      "422    | <|endoftext|>        | 0        | 0      | ignored \n",
      "423    | <|endoftext|>        | 0        | 0      | ignored \n",
      "424    | <|endoftext|>        | 0        | 0      | ignored \n",
      "425    | <|endoftext|>        | 0        | 0      | ignored \n",
      "426    | <|endoftext|>        | 0        | 0      | ignored \n",
      "427    | <|endoftext|>        | 0        | 0      | ignored \n",
      "428    | <|endoftext|>        | 0        | 0      | ignored \n",
      "429    | <|endoftext|>        | 0        | 0      | ignored \n",
      "430    | <|endoftext|>        | 0        | 0      | ignored \n",
      "431    | <|endoftext|>        | 0        | 0      | ignored \n",
      "432    | <|endoftext|>        | 0        | 0      | ignored \n",
      "433    | <|endoftext|>        | 0        | 0      | ignored \n",
      "434    | <|endoftext|>        | 0        | 0      | ignored \n",
      "435    | <|endoftext|>        | 0        | 0      | ignored \n",
      "436    | <|endoftext|>        | 0        | 0      | ignored \n",
      "437    | <|endoftext|>        | 0        | 0      | ignored \n",
      "438    | <|endoftext|>        | 0        | 0      | ignored \n",
      "439    | <|endoftext|>        | 0        | 0      | ignored \n",
      "440    | <|endoftext|>        | 0        | 0      | ignored \n",
      "441    | <|endoftext|>        | 0        | 0      | ignored \n",
      "442    | <|endoftext|>        | 0        | 0      | ignored \n",
      "443    | <|endoftext|>        | 0        | 0      | ignored \n",
      "444    | <|endoftext|>        | 0        | 0      | ignored \n",
      "445    | <|endoftext|>        | 0        | 0      | ignored \n",
      "446    | <|endoftext|>        | 0        | 0      | ignored \n",
      "447    | <|endoftext|>        | 0        | 0      | ignored \n",
      "448    | <|endoftext|>        | 0        | 0      | ignored \n",
      "449    | <|endoftext|>        | 0        | 0      | ignored \n",
      "450    | <|endoftext|>        | 0        | 0      | ignored \n",
      "451    | <|endoftext|>        | 0        | 0      | ignored \n",
      "452    | <|endoftext|>        | 0        | 0      | ignored \n",
      "453    | <|endoftext|>        | 0        | 0      | ignored \n",
      "454    | <|endoftext|>        | 0        | 0      | ignored \n",
      "455    | <|endoftext|>        | 0        | 0      | ignored \n",
      "456    | <|endoftext|>        | 0        | 0      | ignored \n",
      "457    | <|endoftext|>        | 0        | 0      | ignored \n",
      "458    | <|endoftext|>        | 0        | 0      | ignored \n",
      "459    | <|endoftext|>        | 0        | 0      | ignored \n",
      "460    | <|endoftext|>        | 0        | 0      | ignored \n",
      "461    | <|endoftext|>        | 0        | 0      | ignored \n",
      "462    | <|endoftext|>        | 0        | 0      | ignored \n",
      "463    | <|endoftext|>        | 0        | 0      | ignored \n",
      "464    | <|endoftext|>        | 0        | 0      | ignored \n",
      "465    | <|endoftext|>        | 0        | 0      | ignored \n",
      "466    | <|endoftext|>        | 0        | 0      | ignored \n",
      "467    | <|endoftext|>        | 0        | 0      | ignored \n",
      "468    | <|endoftext|>        | 0        | 0      | ignored \n",
      "469    | <|endoftext|>        | 0        | 0      | ignored \n",
      "470    | <|endoftext|>        | 0        | 0      | ignored \n",
      "471    | <|endoftext|>        | 0        | 0      | ignored \n",
      "472    | <|endoftext|>        | 0        | 0      | ignored \n",
      "473    | <|endoftext|>        | 0        | 0      | ignored \n",
      "474    | <|endoftext|>        | 0        | 0      | ignored \n",
      "475    | <|endoftext|>        | 0        | 0      | ignored \n",
      "476    | <|endoftext|>        | 0        | 0      | ignored \n",
      "477    | <|endoftext|>        | 0        | 0      | ignored \n",
      "478    | <|endoftext|>        | 0        | 0      | ignored \n",
      "479    | <|endoftext|>        | 0        | 0      | ignored \n",
      "480    | <|endoftext|>        | 0        | 0      | ignored \n",
      "481    | <|endoftext|>        | 0        | 0      | ignored \n",
      "482    | <|endoftext|>        | 0        | 0      | ignored \n",
      "483    | <|endoftext|>        | 0        | 0      | ignored \n",
      "484    | <|endoftext|>        | 0        | 0      | ignored \n",
      "485    | <|endoftext|>        | 0        | 0      | ignored \n",
      "486    | <|endoftext|>        | 0        | 0      | ignored \n",
      "487    | <|endoftext|>        | 0        | 0      | ignored \n",
      "488    | <|endoftext|>        | 0        | 0      | ignored \n",
      "489    | <|endoftext|>        | 0        | 0      | ignored \n",
      "490    | <|endoftext|>        | 0        | 0      | ignored \n",
      "491    | <|endoftext|>        | 0        | 0      | ignored \n",
      "492    | <|endoftext|>        | 0        | 0      | ignored \n",
      "493    | <|endoftext|>        | 0        | 0      | ignored \n",
      "494    | <|endoftext|>        | 0        | 0      | ignored \n",
      "495    | <|endoftext|>        | 0        | 0      | ignored \n",
      "496    | <|endoftext|>        | 0        | 0      | ignored \n",
      "497    | <|endoftext|>        | 0        | 0      | ignored \n",
      "498    | <|endoftext|>        | 0        | 0      | ignored \n",
      "499    | <|endoftext|>        | 0        | 0      | ignored \n",
      "500    | <|endoftext|>        | 0        | 0      | ignored \n",
      "501    | <|endoftext|>        | 0        | 0      | ignored \n",
      "502    | <|endoftext|>        | 0        | 0      | ignored \n",
      "503    | <|endoftext|>        | 0        | 0      | ignored \n",
      "504    | <|endoftext|>        | 0        | 0      | ignored \n",
      "505    | <|endoftext|>        | 0        | 0      | ignored \n",
      "506    | <|endoftext|>        | 0        | 0      | ignored \n",
      "507    | <|endoftext|>        | 0        | 0      | ignored \n",
      "508    | <|endoftext|>        | 0        | 0      | ignored \n",
      "509    | <|endoftext|>        | 0        | 0      | ignored \n",
      "510    | <|endoftext|>        | 0        | 0      | ignored \n",
      "511    | <|endoftext|>        | 0        | 0      | ignored \n",
      "512    | <|endoftext|>        | 0        | 0      | ignored \n",
      "513    | <|endoftext|>        | 0        | 0      | ignored \n",
      "514    | <|endoftext|>        | 0        | 0      | ignored \n",
      "515    | <|endoftext|>        | 0        | 0      | ignored \n",
      "516    | <|endoftext|>        | 0        | 0      | ignored \n",
      "517    | <|endoftext|>        | 0        | 0      | ignored \n",
      "518    | <|endoftext|>        | 0        | 0      | ignored \n",
      "519    | <|endoftext|>        | 0        | 0      | ignored \n",
      "520    | <|endoftext|>        | 0        | 0      | ignored \n",
      "521    | <|endoftext|>        | 0        | 0      | ignored \n",
      "522    | <|endoftext|>        | 0        | 0      | ignored \n",
      "523    | <|endoftext|>        | 0        | 0      | ignored \n",
      "524    | <|endoftext|>        | 0        | 0      | ignored \n",
      "525    | <|endoftext|>        | 0        | 0      | ignored \n",
      "526    | <|endoftext|>        | 0        | 0      | ignored \n",
      "527    | <|endoftext|>        | 0        | 0      | ignored \n",
      "528    | <|endoftext|>        | 0        | 0      | ignored \n",
      "529    | <|endoftext|>        | 0        | 0      | ignored \n",
      "530    | <|endoftext|>        | 0        | 0      | ignored \n",
      "531    | <|endoftext|>        | 0        | 0      | ignored \n",
      "532    | <|endoftext|>        | 0        | 0      | ignored \n",
      "533    | <|endoftext|>        | 0        | 0      | ignored \n",
      "534    | <|endoftext|>        | 0        | 0      | ignored \n",
      "535    | <|endoftext|>        | 0        | 0      | ignored \n",
      "536    | <|endoftext|>        | 0        | 0      | ignored \n",
      "537    | <|endoftext|>        | 0        | 0      | ignored \n",
      "538    | <|endoftext|>        | 0        | 0      | ignored \n",
      "539    | <|endoftext|>        | 0        | 0      | ignored \n",
      "540    | <|endoftext|>        | 0        | 0      | ignored \n",
      "541    | <|endoftext|>        | 0        | 0      | ignored \n",
      "542    | <|endoftext|>        | 0        | 0      | ignored \n",
      "543    | <|endoftext|>        | 0        | 0      | ignored \n",
      "544    | <|endoftext|>        | 0        | 0      | ignored \n",
      "545    | <|endoftext|>        | 0        | 0      | ignored \n",
      "546    | <|endoftext|>        | 0        | 0      | ignored \n",
      "547    | <|endoftext|>        | 0        | 0      | ignored \n",
      "548    | <|endoftext|>        | 0        | 0      | ignored \n",
      "549    | <|endoftext|>        | 0        | 0      | ignored \n",
      "550    | <|endoftext|>        | 0        | 0      | ignored \n",
      "551    | <|endoftext|>        | 0        | 0      | ignored \n",
      "552    | <|endoftext|>        | 0        | 0      | ignored \n",
      "553    | <|endoftext|>        | 0        | 0      | ignored \n",
      "554    | <|endoftext|>        | 0        | 0      | ignored \n",
      "555    | <|endoftext|>        | 0        | 0      | ignored \n",
      "556    | <|endoftext|>        | 0        | 0      | ignored \n",
      "557    | <|endoftext|>        | 0        | 0      | ignored \n",
      "558    | <|endoftext|>        | 0        | 0      | ignored \n",
      "559    | <|endoftext|>        | 0        | 0      | ignored \n",
      "560    | <|endoftext|>        | 0        | 0      | ignored \n",
      "561    | <|endoftext|>        | 0        | 0      | ignored \n",
      "562    | <|endoftext|>        | 0        | 0      | ignored \n",
      "563    | <|endoftext|>        | 0        | 0      | ignored \n",
      "564    | <|endoftext|>        | 0        | 0      | ignored \n",
      "565    | <|endoftext|>        | 0        | 0      | ignored \n",
      "566    | <|endoftext|>        | 0        | 0      | ignored \n",
      "567    | <|endoftext|>        | 0        | 0      | ignored \n",
      "568    | <|endoftext|>        | 0        | 0      | ignored \n",
      "569    | <|endoftext|>        | 0        | 0      | ignored \n",
      "570    | <|endoftext|>        | 0        | 0      | ignored \n",
      "571    | <|endoftext|>        | 0        | 0      | ignored \n",
      "572    | <|endoftext|>        | 0        | 0      | ignored \n",
      "573    | <|endoftext|>        | 0        | 0      | ignored \n",
      "574    | <|endoftext|>        | 0        | 0      | ignored \n",
      "575    | <|endoftext|>        | 0        | 0      | ignored \n",
      "576    | <|endoftext|>        | 0        | 0      | ignored \n",
      "577    | <|endoftext|>        | 0        | 0      | ignored \n",
      "578    | <|endoftext|>        | 0        | 0      | ignored \n",
      "579    | <|endoftext|>        | 0        | 0      | ignored \n",
      "580    | <|endoftext|>        | 0        | 0      | ignored \n",
      "581    | <|endoftext|>        | 0        | 0      | ignored \n",
      "582    | <|endoftext|>        | 0        | 0      | ignored \n",
      "583    | <|endoftext|>        | 0        | 0      | ignored \n",
      "584    | <|endoftext|>        | 0        | 0      | ignored \n",
      "585    | <|endoftext|>        | 0        | 0      | ignored \n",
      "586    | <|endoftext|>        | 0        | 0      | ignored \n",
      "587    | <|endoftext|>        | 0        | 0      | ignored \n",
      "588    | <|endoftext|>        | 0        | 0      | ignored \n",
      "589    | <|endoftext|>        | 0        | 0      | ignored \n",
      "590    | <|endoftext|>        | 0        | 0      | ignored \n",
      "591    | <|endoftext|>        | 0        | 0      | ignored \n",
      "592    | <|endoftext|>        | 0        | 0      | ignored \n",
      "593    | <|endoftext|>        | 0        | 0      | ignored \n",
      "594    | <|endoftext|>        | 0        | 0      | ignored \n",
      "595    | <|endoftext|>        | 0        | 0      | ignored \n",
      "596    | <|endoftext|>        | 0        | 0      | ignored \n",
      "597    | <|endoftext|>        | 0        | 0      | ignored \n",
      "598    | <|endoftext|>        | 0        | 0      | ignored \n",
      "599    | <|endoftext|>        | 0        | 0      | ignored \n",
      "600    | <|endoftext|>        | 0        | 0      | ignored \n",
      "601    | <|endoftext|>        | 0        | 0      | ignored \n",
      "602    | <|endoftext|>        | 0        | 0      | ignored \n",
      "603    | <|endoftext|>        | 0        | 0      | ignored \n",
      "604    | <|endoftext|>        | 0        | 0      | ignored \n",
      "605    | <|endoftext|>        | 0        | 0      | ignored \n",
      "606    | <|endoftext|>        | 0        | 0      | ignored \n",
      "607    | <|endoftext|>        | 0        | 0      | ignored \n",
      "608    | <|endoftext|>        | 0        | 0      | ignored \n",
      "609    | <|endoftext|>        | 0        | 0      | ignored \n",
      "610    | <|endoftext|>        | 0        | 0      | ignored \n",
      "611    | <|endoftext|>        | 0        | 0      | ignored \n",
      "612    | <|endoftext|>        | 0        | 0      | ignored \n",
      "613    | <|endoftext|>        | 0        | 0      | ignored \n",
      "614    | <|endoftext|>        | 0        | 0      | ignored \n",
      "615    | <|endoftext|>        | 0        | 0      | ignored \n",
      "616    | <|endoftext|>        | 0        | 0      | ignored \n",
      "617    | <|endoftext|>        | 0        | 0      | ignored \n",
      "618    | <|endoftext|>        | 0        | 0      | ignored \n",
      "619    | <|endoftext|>        | 0        | 0      | ignored \n",
      "620    | <|endoftext|>        | 0        | 0      | ignored \n",
      "621    | <|endoftext|>        | 0        | 0      | ignored \n",
      "622    | <|endoftext|>        | 0        | 0      | ignored \n",
      "623    | <|endoftext|>        | 0        | 0      | ignored \n",
      "624    | <|endoftext|>        | 0        | 0      | ignored \n",
      "625    | <|endoftext|>        | 0        | 0      | ignored \n",
      "626    | <|endoftext|>        | 0        | 0      | ignored \n",
      "627    | <|endoftext|>        | 0        | 0      | ignored \n",
      "628    | <|endoftext|>        | 0        | 0      | ignored \n",
      "629    | <|endoftext|>        | 0        | 0      | ignored \n",
      "630    | <|endoftext|>        | 0        | 0      | ignored \n",
      "631    | <|endoftext|>        | 0        | 0      | ignored \n",
      "632    | <|endoftext|>        | 0        | 0      | ignored \n",
      "633    | <|endoftext|>        | 0        | 0      | ignored \n",
      "634    | <|endoftext|>        | 0        | 0      | ignored \n",
      "635    | <|endoftext|>        | 0        | 0      | ignored \n",
      "636    | <|endoftext|>        | 0        | 0      | ignored \n",
      "637    | <|endoftext|>        | 0        | 0      | ignored \n",
      "638    | <|endoftext|>        | 0        | 0      | ignored \n",
      "639    | <|endoftext|>        | 0        | 0      | ignored \n",
      "640    | <|endoftext|>        | 0        | 0      | ignored \n",
      "641    | <|endoftext|>        | 0        | 0      | ignored \n",
      "642    | <|endoftext|>        | 0        | 0      | ignored \n",
      "643    | <|endoftext|>        | 0        | 0      | ignored \n",
      "644    | <|endoftext|>        | 0        | 0      | ignored \n",
      "645    | <|endoftext|>        | 0        | 0      | ignored \n",
      "646    | <|endoftext|>        | 0        | 0      | ignored \n",
      "647    | <|endoftext|>        | 0        | 0      | ignored \n",
      "648    | <|endoftext|>        | 0        | 0      | ignored \n",
      "649    | <|endoftext|>        | 0        | 0      | ignored \n",
      "650    | <|endoftext|>        | 0        | 0      | ignored \n",
      "651    | <|endoftext|>        | 0        | 0      | ignored \n",
      "652    | <|endoftext|>        | 0        | 0      | ignored \n",
      "653    | <|endoftext|>        | 0        | 0      | ignored \n",
      "654    | <|endoftext|>        | 0        | 0      | ignored \n",
      "655    | <|endoftext|>        | 0        | 0      | ignored \n",
      "656    | <|endoftext|>        | 0        | 0      | ignored \n",
      "657    | <|endoftext|>        | 0        | 0      | ignored \n",
      "658    | <|endoftext|>        | 0        | 0      | ignored \n",
      "659    | <|endoftext|>        | 0        | 0      | ignored \n",
      "660    | <|endoftext|>        | 0        | 0      | ignored \n",
      "661    | <|endoftext|>        | 0        | 0      | ignored \n",
      "662    | <|endoftext|>        | 0        | 0      | ignored \n",
      "663    | <|endoftext|>        | 0        | 0      | ignored \n",
      "664    | <|endoftext|>        | 0        | 0      | ignored \n",
      "665    | <|endoftext|>        | 0        | 0      | ignored \n",
      "666    | <|endoftext|>        | 0        | 0      | ignored \n",
      "667    | <|endoftext|>        | 0        | 0      | ignored \n",
      "668    | <|endoftext|>        | 0        | 0      | ignored \n",
      "669    | <|endoftext|>        | 0        | 0      | ignored \n",
      "670    | <|endoftext|>        | 0        | 0      | ignored \n",
      "671    | <|endoftext|>        | 0        | 0      | ignored \n",
      "672    | <|endoftext|>        | 0        | 0      | ignored \n",
      "673    | <|endoftext|>        | 0        | 0      | ignored \n",
      "674    | <|endoftext|>        | 0        | 0      | ignored \n",
      "675    | <|endoftext|>        | 0        | 0      | ignored \n",
      "676    | <|endoftext|>        | 0        | 0      | ignored \n",
      "677    | <|endoftext|>        | 0        | 0      | ignored \n",
      "678    | <|endoftext|>        | 0        | 0      | ignored \n",
      "679    | <|endoftext|>        | 0        | 0      | ignored \n",
      "680    | <|endoftext|>        | 0        | 0      | ignored \n",
      "681    | <|endoftext|>        | 0        | 0      | ignored \n",
      "682    | <|endoftext|>        | 0        | 0      | ignored \n",
      "683    | <|endoftext|>        | 0        | 0      | ignored \n",
      "684    | <|endoftext|>        | 0        | 0      | ignored \n",
      "685    | <|endoftext|>        | 0        | 0      | ignored \n",
      "686    | <|endoftext|>        | 0        | 0      | ignored \n",
      "687    | <|endoftext|>        | 0        | 0      | ignored \n",
      "688    | <|endoftext|>        | 0        | 0      | ignored \n",
      "689    | <|endoftext|>        | 0        | 0      | ignored \n",
      "690    | <|endoftext|>        | 0        | 0      | ignored \n",
      "691    | <|endoftext|>        | 0        | 0      | ignored \n",
      "692    | <|endoftext|>        | 0        | 0      | ignored \n",
      "693    | <|endoftext|>        | 0        | 0      | ignored \n",
      "694    | <|endoftext|>        | 0        | 0      | ignored \n",
      "695    | <|endoftext|>        | 0        | 0      | ignored \n",
      "696    | <|endoftext|>        | 0        | 0      | ignored \n",
      "697    | <|endoftext|>        | 0        | 0      | ignored \n",
      "698    | <|endoftext|>        | 0        | 0      | ignored \n",
      "699    | <|endoftext|>        | 0        | 0      | ignored \n",
      "700    | <|endoftext|>        | 0        | 0      | ignored \n",
      "701    | <|endoftext|>        | 0        | 0      | ignored \n",
      "702    | <|endoftext|>        | 0        | 0      | ignored \n",
      "703    | <|endoftext|>        | 0        | 0      | ignored \n",
      "704    | <|endoftext|>        | 0        | 0      | ignored \n",
      "705    | <|endoftext|>        | 0        | 0      | ignored \n",
      "706    | <|endoftext|>        | 0        | 0      | ignored \n",
      "707    | <|endoftext|>        | 0        | 0      | ignored \n",
      "708    | <|endoftext|>        | 0        | 0      | ignored \n",
      "709    | <|endoftext|>        | 0        | 0      | ignored \n",
      "710    | <|endoftext|>        | 0        | 0      | ignored \n",
      "711    | <|endoftext|>        | 0        | 0      | ignored \n",
      "712    | <|endoftext|>        | 0        | 0      | ignored \n",
      "713    | <|endoftext|>        | 0        | 0      | ignored \n",
      "714    | <|endoftext|>        | 0        | 0      | ignored \n",
      "715    | <|endoftext|>        | 0        | 0      | ignored \n",
      "716    | <|endoftext|>        | 0        | 0      | ignored \n",
      "717    | <|endoftext|>        | 0        | 0      | ignored \n",
      "718    | <|endoftext|>        | 0        | 0      | ignored \n",
      "719    | <|endoftext|>        | 0        | 0      | ignored \n",
      "720    | <|endoftext|>        | 0        | 0      | ignored \n",
      "721    | <|endoftext|>        | 0        | 0      | ignored \n",
      "722    | <|endoftext|>        | 0        | 0      | ignored \n",
      "723    | <|endoftext|>        | 0        | 0      | ignored \n",
      "724    | <|endoftext|>        | 0        | 0      | ignored \n",
      "725    | <|endoftext|>        | 0        | 0      | ignored \n",
      "726    | <|endoftext|>        | 0        | 0      | ignored \n",
      "727    | <|endoftext|>        | 0        | 0      | ignored \n",
      "728    | <|endoftext|>        | 0        | 0      | ignored \n",
      "729    | <|endoftext|>        | 0        | 0      | ignored \n",
      "730    | <|endoftext|>        | 0        | 0      | ignored \n",
      "731    | <|endoftext|>        | 0        | 0      | ignored \n",
      "732    | <|endoftext|>        | 0        | 0      | ignored \n",
      "733    | <|endoftext|>        | 0        | 0      | ignored \n",
      "734    | <|endoftext|>        | 0        | 0      | ignored \n",
      "735    | <|endoftext|>        | 0        | 0      | ignored \n",
      "736    | <|endoftext|>        | 0        | 0      | ignored \n",
      "737    | <|endoftext|>        | 0        | 0      | ignored \n",
      "738    | <|endoftext|>        | 0        | 0      | ignored \n",
      "739    | <|endoftext|>        | 0        | 0      | ignored \n",
      "740    | <|endoftext|>        | 0        | 0      | ignored \n",
      "741    | <|endoftext|>        | 0        | 0      | ignored \n",
      "742    | <|endoftext|>        | 0        | 0      | ignored \n",
      "743    | <|endoftext|>        | 0        | 0      | ignored \n",
      "744    | <|endoftext|>        | 0        | 0      | ignored \n",
      "745    | <|endoftext|>        | 0        | 0      | ignored \n",
      "746    | <|endoftext|>        | 0        | 0      | ignored \n",
      "747    | <|endoftext|>        | 0        | 0      | ignored \n",
      "748    | <|endoftext|>        | 0        | 0      | ignored \n",
      "749    | <|endoftext|>        | 0        | 0      | ignored \n",
      "750    | <|endoftext|>        | 0        | 0      | ignored \n",
      "751    | <|endoftext|>        | 0        | 0      | ignored \n",
      "752    | <|endoftext|>        | 0        | 0      | ignored \n",
      "753    | <|endoftext|>        | 0        | 0      | ignored \n",
      "754    | <|endoftext|>        | 0        | 0      | ignored \n",
      "755    | <|endoftext|>        | 0        | 0      | ignored \n",
      "756    | <|endoftext|>        | 0        | 0      | ignored \n",
      "757    | <|endoftext|>        | 0        | 0      | ignored \n",
      "758    | <|endoftext|>        | 0        | 0      | ignored \n",
      "759    | <|endoftext|>        | 0        | 0      | ignored \n",
      "760    | <|endoftext|>        | 0        | 0      | ignored \n",
      "761    | <|endoftext|>        | 0        | 0      | ignored \n",
      "762    | <|endoftext|>        | 0        | 0      | ignored \n",
      "763    | <|endoftext|>        | 0        | 0      | ignored \n",
      "764    | <|endoftext|>        | 0        | 0      | ignored \n",
      "765    | <|endoftext|>        | 0        | 0      | ignored \n",
      "766    | <|endoftext|>        | 0        | 0      | ignored \n",
      "767    | <|endoftext|>        | 0        | 0      | ignored \n"
     ]
    }
   ],
   "source": [
    "def visualize_tokenization(tokenizer, input_ids, attention_mask, labels, n):\n",
    "    \"\"\"\n",
    "    Visualize token-level information with their corresponding labels and masks.\n",
    "    \n",
    "    Args:\n",
    "        tokenizer: The tokenizer used\n",
    "        input_ids: The token IDs\n",
    "        attention_mask: The attention mask\n",
    "        labels: The labels for each token\n",
    "    \"\"\"\n",
    "    # Take the first example if we have a batch\n",
    "    if len(input_ids.shape) > 1:\n",
    "        input_ids = input_ids[n]\n",
    "        attention_mask = attention_mask[n]\n",
    "        labels = labels[n]\n",
    "    \n",
    "    # Decode each token individually to see them separately\n",
    "    tokens = [tokenizer.decode([id]) for id in input_ids]\n",
    "    \n",
    "    # Create a table for visualization\n",
    "    print(f\"{'Index':<6} | {'Token':<20} | {'ID':<8} | {'Mask':<6} | {'Label':<8}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i, (token, id, mask, label) in enumerate(zip(tokens, input_ids, attention_mask, labels)):\n",
    "        # Skip padding tokens if needed\n",
    "        # if id == tokenizer.pad_token_id and mask == 0:\n",
    "        #     continue\n",
    "            \n",
    "        # Format special tokens for better readability\n",
    "        token_display = token.replace('\\n', '\\\\n').replace('\\t', '\\\\t')\n",
    "        if len(token_display) > 18:\n",
    "            token_display = token_display[:15] + \"...\"\n",
    "            \n",
    "        print(f\"{i:<6} | {token_display:<20} | {int(id):<8} | {int(mask):<6} | {int(label) if label != -100 else 'ignored':<8}\")\n",
    "\n",
    "\n",
    "print(tokenizer.decode(preprocessed['input_ids'][2], skip_special_tokens=True))\n",
    "visualize_tokenization(\n",
    "    tokenizer, \n",
    "    preprocessed['input_ids'], \n",
    "    preprocessed['attention_mask'], \n",
    "    preprocessed['labels'], \n",
    "    n=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "total_examples = 378_000_000  # Total examples in FLAN dataset\n",
    "\n",
    "small_subset   = 378_000 * 2\n",
    "# small_subset   = 3_780_000\n",
    "\n",
    "train_ratio    = 0.9\n",
    "val_ratio      = 0.05\n",
    "test_ratio     = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try out with smaller set first\n",
    "total_examples = small_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and split before preprocessing\n",
    "shuffled   = dataset.shuffle(seed=42, buffer_size=200_000)\n",
    "\n",
    "# Calculate split sizes\n",
    "train_size = int(total_examples * train_ratio)\n",
    "val_size   = int(total_examples * val_ratio)\n",
    "test_size  = int(total_examples * test_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680400 37800 37800\n"
     ]
    }
   ],
   "source": [
    "print(train_size, val_size, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create splits\n",
    "train_raw = shuffled.take(train_size)\n",
    "remaining = shuffled.skip(train_size)\n",
    "val_raw   = remaining.take(val_size)\n",
    "test_raw  = remaining.skip(val_size).take(test_size)\n",
    "\n",
    "# Preprocess each split\n",
    "tokenized_train = train_raw.map(preprocess_forward, batched=True)\n",
    "tokenized_val   = val_raw.map(preprocess_forward, batched=True)\n",
    "tokenized_test  = test_raw.map(preprocess_forward, batched=True)\n",
    "\n",
    "# QUESTION: How do we know what is its task composition here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5315\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "intended_epochs      = 1\n",
    "gradient_accum_steps = 8\n",
    "\n",
    "max_steps = int(total_examples * train_ratio) // (batch_size * gradient_accum_steps) * intended_epochs\n",
    "\n",
    "print(max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated time: 15185.714285714286s (4.218253968253968) hr\n"
     ]
    }
   ],
   "source": [
    "print(f\"estimated time: {max_steps / 0.35}s ({max_steps / 0.35 / 3600}) hr\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miwiryadi\u001b[0m (\u001b[33midl-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/ocean/projects/cis250068p/iwiryadi/idl-project/wandb/run-20250410_095445-uypk56hf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/11785_finetuning/ivan-testing/runs/uypk56hf' target=\"_blank\">testing</a></strong> to <a href='https://wandb.ai/11785_finetuning/ivan-testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/11785_finetuning/ivan-testing' target=\"_blank\">https://wandb.ai/11785_finetuning/ivan-testing</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/11785_finetuning/ivan-testing/runs/uypk56hf' target=\"_blank\">https://wandb.ai/11785_finetuning/ivan-testing/runs/uypk56hf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/11785_finetuning/ivan-testing/runs/uypk56hf?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1496360766b0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(entity=\"11785_finetuning\", project='ivan-testing', name=\"testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerationTestCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Callback to generate text samples at evaluation steps.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, test_prompts, device=\"cuda\"):\n",
    "        \"\"\"\n",
    "        Initialize with tokenizer and test prompts.\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.test_prompts = test_prompts\n",
    "        self.device = device\n",
    "        \n",
    "    def on_evaluate(self, args, state, control, model, **kwargs):\n",
    "        \"\"\"\n",
    "        Run after each evaluation.\n",
    "        \"\"\"\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"Generating samples at step {state.global_step}:\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for prompt in self.test_prompts:\n",
    "                inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "                \n",
    "                # Generate text\n",
    "                output_ids = model.generate(\n",
    "                    inputs[\"input_ids\"],\n",
    "                    max_length=150,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "                \n",
    "                # Decode the output\n",
    "                generated_text = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "                \n",
    "                # Print the result\n",
    "                print(f\"\\nPrompt: {prompt}\")\n",
    "                print(f\"Generated: {generated_text}\")\n",
    "                print(\"-\"*50)\n",
    "                \n",
    "                # Log to W&B if you're using it\n",
    "                if args.report_to == \"wandb\":\n",
    "                    import wandb\n",
    "                    wandb.log({\n",
    "                        f\"generation/{prompt}\": wandb.Html(\n",
    "                            f\"<b>Step {state.global_step}</b><br>\"\n",
    "                            f\"<p><b>Prompt:</b> {prompt}</p>\"\n",
    "                            f\"<p><b>Generated:</b> {generated_text}</p>\"\n",
    "                        )\n",
    "                    }, step=state.global_step)\n",
    "        \n",
    "        return control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some additional notes / references ;\n",
    "- From [2210.11416v5.pdf](papers/2210.11416v5.pdf): We found that learning rate, batch size and the dropout were the most important hyperparameters for instruction finetuning.\n",
    "- https://www.philschmid.de/fine-tune-flan-t5\n",
    "- https://arxiv.org/html/2401.13586v2 \n",
    "- https://github.com/TheFloatingString/stacking-llms/blob/main/pipeline-fine_tune-instruction_prompt.ipynb and https://arxiv.org/html/2410.15570v1#S3 \n",
    "- https://github.com/huggingface/smol-course/blob/main/1_instruction_tuning/notebooks/sft_finetuning_example.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_94069/1927398677.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    run_name      = \"testing\",\n",
    "    \n",
    "    output_dir    = f\"{OUTPUT_DIR}/pythia-finetuned\",\n",
    "    eval_strategy = \"steps\",\n",
    "    learning_rate = 1e-6, # https://openreview.net/pdf?id=3pDMYjpOxk was using 1e-6 for HHLF Anthropic\n",
    "    per_device_train_batch_size = batch_size,\n",
    "    per_device_eval_batch_size  = batch_size,\n",
    "    max_steps     = max_steps,  # Required for streaming datasets\n",
    "    \n",
    "    \n",
    "    eval_steps    = max_steps // 5, # eval in each 20%\n",
    "    save_steps    = max_steps // 2, # eval in each 50%\n",
    "    logging_steps = gradient_accum_steps,\n",
    "    gradient_accumulation_steps = gradient_accum_steps,\n",
    "    \n",
    "    save_total_limit = 2,\n",
    "    \n",
    "    weight_decay     = 0.01,\n",
    "    fp16             = True,\n",
    "    max_grad_norm    = 1.0,\n",
    "    warmup_steps     = max_steps // 5, # 20% warmup\n",
    "    \n",
    "    logging_dir      = \"./logs\",\n",
    "    report_to        = \"wandb\",\n",
    "    push_to_hub      = False,\n",
    "    disable_tqdm     = False,\n",
    "    \n",
    "    dataloader_num_workers = 4,\n",
    ")\n",
    "\n",
    "# Create the callback\n",
    "generation_callback = GenerationTestCallback(tokenizer, test_prompts, device=DEVICE)\n",
    "\n",
    "# Initialize trainer with the callback\n",
    "trainer = Trainer(\n",
    "    model         = model,\n",
    "    args          = training_args,\n",
    "    train_dataset = tokenized_train,\n",
    "    eval_dataset  = tokenized_val,\n",
    "    tokenizer     = tokenizer,\n",
    "    callbacks     = [generation_callback]  # Add our callback here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1064' max='5315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1064/5315 48:02 < 3:12:20, 0.37 it/s, Epoch 0.20/9223372036854775807]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid 95850) is killed by signal: Killed. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[1;32m      2\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTOKENIZERS_PARALLELISM\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFalse\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2620\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2618\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   2619\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2620\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\n\u001b[1;32m   2622\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2624\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:3093\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time)\u001b[0m\n\u001b[1;32m   3091\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 3093\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3094\u001b[0m     is_new_best_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_determine_best_metric(metrics\u001b[38;5;241m=\u001b[39mmetrics, trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[1;32m   3096\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_strategy \u001b[38;5;241m==\u001b[39m SaveStrategy\u001b[38;5;241m.\u001b[39mBEST:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:3047\u001b[0m, in \u001b[0;36mTrainer._evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   3046\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m-> 3047\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3048\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   3050\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:4136\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4133\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   4135\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 4136\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4137\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4139\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   4140\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   4141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   4142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4144\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4146\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   4147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:4320\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4317\u001b[0m observed_num_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   4319\u001b[0m \u001b[38;5;66;03m# Main evaluation loop\u001b[39;00m\n\u001b[0;32m-> 4320\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m   4321\u001b[0m     \u001b[38;5;66;03m# Update the observed num examples\u001b[39;00m\n\u001b[1;32m   4322\u001b[0m     observed_batch_size \u001b[38;5;241m=\u001b[39m find_batch_size(inputs)\n\u001b[1;32m   4323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m observed_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/data_loader.py:858\u001b[0m, in \u001b[0;36mDataLoaderDispatcher.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop_iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    857\u001b[0m first_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 858\u001b[0m next_batch, next_batch_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    859\u001b[0m batch_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stop_iteration:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/data_loader.py:812\u001b[0m, in \u001b[0;36mDataLoaderDispatcher._fetch_batches\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    810\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mnum_processes):\n\u001b[1;32m    811\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_state_dict()\n\u001b[0;32m--> 812\u001b[0m         batches\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    814\u001b[0m     batch \u001b[38;5;241m=\u001b[39m concatenate(batches, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1458\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1458\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1461\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1410\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1408\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1409\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1410\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1411\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1412\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1251\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1239\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1249\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1251\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1254\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1255\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1256\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/packages/AI/pytorch_23.02-1.13.1-py3/lib/python3.10/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m/opt/packages/AI/pytorch_23.02-1.13.1-py3/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py:73\u001b[0m, in \u001b[0;36m_set_SIGCHLD_handler.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandler\u001b[39m(signum, frame):\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[43m_error_if_any_worker_fails\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m previous_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m callable(previous_handler)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 95850) is killed by signal: Killed. "
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = \"False\"\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('output//pythia-finetuned-last-step-2/tokenizer_config.json',\n",
       " 'output//pythia-finetuned-last-step-2/special_tokens_map.json',\n",
       " 'output//pythia-finetuned-last-step-2/tokenizer.json')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save final model\n",
    "trainer.save_model(f\"{OUTPUT_DIR}/pythia-finetuned-last-step-3\")\n",
    "tokenizer.save_pretrained(f\"{OUTPUT_DIR}/pythia-finetuned-last-step-3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the resulting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTNeoXForCausalLM\n",
    "\n",
    "model = GPTNeoXForCausalLM.from_pretrained(\n",
    "    f\"{OUTPUT_DIR}/pythia-finetuned-last-step-2\",\n",
    "    device_map=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without sampling: Who is Barack Obama? Is Barack Obama?n::::::::: the:::: the::::::: the:: the:: the::: the:: the::: the:: the:: the: the: the: the: the: the: the: the: the: the: the: the: the: the: the: the: the: the: the: the: the: the: the: the: the: the: the: the: the: the: the: the: the: the: the: the: the: the: the: the: the: the: the: the: the: the: the: the\n",
      "---------------\n",
      "With sampling   : Who is Barack Obama? A?n?n?\\x\\xaq\\xa/r-\\\\\\\\\\\\\\xa0\\0\\a\\+\\\\+\\/\\s/:\\xn+\\\\+\\\\?\\\\\\\\xa-xaxa/\\\\xa0\\xan\\xan\\6\\2nn\\nnn-\\4\\4n-\\'\\5b\\2\\0n\\-\\-i\\iai\\ian\\\\0-\\\\1pn\\'?\\-i\\iti\\ist\\\\0-\\+\\0\\-0\\\\0+\\\\nonb\\u;i\\u\\s a\\\n",
      "\n",
      "===============\n",
      "Without sampling: What is Carnegie Mellon University? Carnegie Mellon University is a university located in the University of Pennsylvania. in Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania Pennsylvania\n",
      "---------------\n",
      "With sampling   : What is Carnegie Mellon University? Carnegie Mellon University is an Internationalin Business Economics , Business , , , and educational in countries It The ,- , and have in for. of. universities Its in , , industry educational of ,, the in of , universities research , educational , of , , , the , , and , , an industry institution   Universityn , , , the of , educational , , an educational for ,, the in ,, the research university , \" as the of educational the of , and the , in the of.\\    are in of which universities   academic ,- as of the: university that one   college , with educationals , the of it the , and ,  : that in   educational , \"\n",
      "\n",
      "===============\n",
      "Without sampling: Classify this restaurant review sentiment: 'The food was absolutely delicious but the service was extremely slow and the waiter seemed uninterested in helping us.' Review I this a,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "---------------\n",
      "With sampling   : Classify this restaurant review sentiment: 'The food was absolutely delicious but the service was extremely slow and the waiter seemed uninterested in helping us.' the: to:); \": was into as; is of;;. French ', \" is one;; word form in;; in.\\;.\\ is on with;\\,.\\ a ' ab '.\\ ison ' the is of in ( ' ' is on ' ' be, ison is for .;\\ a.]\\' the; is of for.\\; on; \" this ' not ' a\\\" the be ' on is a that an '\\' the.\\ ' is. of ' ' aal be a to:\\ ': in:) to in;\n",
      "\n",
      "===============\n",
      "Without sampling: Compare and contrast Carnegie Mellon University's Computer Science and Information Systems programs in terms of research focus and career outcomes. in,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, the,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, the,,,,,,,,,,,,,,,,,\n",
      "---------------\n",
      "With sampling   : Compare and contrast Carnegie Mellon University's Computer Science and Information Systems programs in terms of research focus and career outcomes. was a student of at institution., a and a of year of.\\\\\\\\\\\\\\\\\\\\\\A\\\\\\\\\\\\1\\1 answer \\(\\\\b\\\\\\\\b\\\\b\\\\+\\\\b\\b\\+\\w\\-+\\w+\\-+\\--\\+\\-\\+\\+\\-+\\+\\\\)\\-\\\\.\\- A Answer ' Answer',',;',This is the answer \" is this? is this? is answer this? is answer this is!\\I this is answer the is is this is answer this is answer this is it this\n",
      "\n",
      "===============\n",
      "Without sampling: Summarize in one sentence: Dr. Sarah Chen, lead scientist on the mission, called it 'the most significant discovery in the history of space exploration.' The finding suggests that Mars once had a much more hospitable environment with liquid water and possibly a thicker atmosphere. The agency plans to send a sample return mission within the next five years to bring these fossils back to Earth for more detailed analysis. This discovery has profound implications for our understanding of how life might develop throughout the universe. the of the of the of the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "---------------\n",
      "With sampling   : Summarize in one sentence: Dr. Sarah Chen, lead scientist on the mission, called it 'the most significant discovery in the history of space exploration.' The finding suggests that Mars once had a much more hospitable environment with liquid water and possibly a thicker atmosphere. The agency plans to send a sample return mission within the next five years to bring these fossils back to Earth for more detailed analysis. This discovery has profound implications for our understanding of how life might develop throughout the universe. scientists to the the that Earths and are to this time tos to future.:): the wass the were the were had for been to as, that made was, the on, and them on the and thes issn that is\n",
      "\n",
      "===============\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for test_input_string in test_prompts:\n",
    "    inputs = tokenizer(test_input_string, return_tensors=\"pt\").to(DEVICE)\n",
    "    # print(tokens[0])\n",
    "    tokens = model.generate(**inputs, max_length=150, pad_token_id=tokenizer.eos_token_id)\n",
    "    print(\"Without sampling: \" + tokenizer.decode(tokens[0]))\n",
    "    \n",
    "    print(\"---------------\")\n",
    "    tokens = model.generate(**inputs, max_length=150, pad_token_id=tokenizer.eos_token_id, do_sample=True)\n",
    "    print(\"With sampling   : \" + tokenizer.decode(tokens[0]))\n",
    "    \n",
    "    print(\"\\n===============\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1394 steps, rounghly ~190k data \n",
    "\n",
    "\n",
    "test_input_string = \"Who is Barrack Obama?\"\n",
    "Before fine tuning: 'Who is Barrack Obama?\\n\\nThe Obama administration has been a major player in the Obama administration’s efforts to undermine the'\n",
    "After fine tuning:  'Who is Barrack Obama? is a of the of the of the of the of the of the of the of the of the'\n",
    "A                   'Who is Barrack Obama? is the is the is the? is the is the? is the is the is? the is'\n",
    "\n",
    "\n",
    "test_input_string = \"Carnegie Mellon University is known for\"\n",
    "Before fine tuning: 'Carnegie Mellon University is known for its research on the evolution of the human brain.\\n\\nThe Carnegie Mellon University’s research'\n",
    "After fine tuning:  'Carnegie Mellon University is known for its educational, and, the,,,,,,,,,,,,,,'\n",
    "\n",
    "\n",
    "test_input_string = \"What is Carnegie Mellon University?\"\n",
    "Before fine tuning: \"'What is Carnegie Mellon University?\\n\\nThe Carnegie Mellon University is a private, non-profit, non-profit, non'\"\n",
    "After fine tuning:  'What is Carnegie Mellon University? Carnegie Mellon University is a of of of of of of the of of of of of the of'\n",
    "After               'What is Carnegie Mellon University? Carnegie Mellon University is a university located in the United States, the United States, and the of'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
