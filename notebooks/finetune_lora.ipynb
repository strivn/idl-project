{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune with full scale dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from peft import (LoraConfig, PeftModel, get_peft_model,\n",
    "                  prepare_model_for_kbit_training)\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (AutoTokenizer, BitsAndBytesConfig,\n",
    "                          DataCollatorForLanguageModeling, GPTNeoXForCausalLM,\n",
    "                          Trainer, TrainingArguments, TrainerCallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Working in '/ocean/projects/cis250068p/iwiryadi/idl-project'\n",
      "✓ Directory contains 'idl-project'\n"
     ]
    }
   ],
   "source": [
    "current_dir = os.path.abspath(os.getcwd())\n",
    "\n",
    "# Check if 'idl-project' is in the path\n",
    "if 'idl-project' not in current_dir:\n",
    "    raise Exception(\"Current directory '{current_dir}' is not within 'idl-project'\")\n",
    "\n",
    "print(f\"✓ Working in '{current_dir}'\")\n",
    "print(f\"✓ Directory contains 'idl-project'\")\n",
    "\n",
    "OUTPUT_DIR = \"output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Append paths for the src folder\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.getcwd(), 'idl-project')))\n",
    "\n",
    "# Additional imports \n",
    "from src.model import load_fo_model\n",
    "from src.data import load_flan_dataset, load_summarization_datasets\n",
    "from src.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Tesla V100-SXM2-32GB\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_NAME = 'lora-tuning-1'\n",
    "\n",
    "# Data parameters\n",
    "# SUBSET_FRAC                 = 1\n",
    "# FLAN_SUBSET                 = ['cot_zsopt_data']\n",
    "SUBSET_FRAC                 = 1\n",
    "FLAN_SUBSET                 = ['niv2_zsopt_data', 'cot_zsopt_data', 'dialog_zsopt_data']\n",
    "\n",
    "# Training parameters\n",
    "BATCH_SIZE                  = 8\n",
    "GRADIENT_ACCUMULATION_STEPS = 8\n",
    "LEARNING_RATE               = 1e-5\n",
    "NUM_EPOCHS                  = 3\n",
    "MAX_SEQ_LENGTH              = 1024\n",
    "WARMUP_STEPS                = 1000\n",
    "LOGGING_STEPS               = 100\n",
    "SAVE_STEPS                  = 1000\n",
    "EVAL_STEPS                  = 1000\n",
    "SAVE_TOTAL_LIMIT            = 3\n",
    "WEIGHT_DECAY                = 0.01\n",
    "FP16                        = True\n",
    "\n",
    "TEST_SIZE                   = 0.05\n",
    "\n",
    "# BUFFER_SIZE                 = 25000  # For dataset processing\n",
    "\n",
    "\n",
    "# LoRA configuration\n",
    "LORA_RANK                   = 8\n",
    "LORA_ALPHA                  = 32\n",
    "LORA_DROPOUT                = 0.05\n",
    "TARGET_MODULES              = [\"query_key_value\"]  # Target specific attention modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test prompts\n",
    "test_prompts = [\n",
    "    \"Who is Barack Obama?\",\n",
    "    \"What is Carnegie Mellon University?\",\n",
    "    \"Classify this restaurant review sentiment: 'The food was absolutely delicious but the service was extremely slow and the waiter seemed uninterested in helping us.'\",\n",
    "    \"Compare and contrast Carnegie Mellon University's Computer Science and Information Systems programs in terms of research focus and career outcomes.\",\n",
    "    \"Summarize in one sentence: Dr. Sarah Chen, lead scientist on the mission, called it 'the most significant discovery in the history of space exploration.' The finding suggests that Mars once had a much more hospitable environment with liquid water and possibly a thicker atmosphere. The agency plans to send a sample return mission within the next five years to bring these fossils back to Earth for more detailed analysis. This discovery has profound implications for our understanding of how life might develop throughout the universe.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f6ec4940264427aa69c412667a388c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing cot_zsopt_data...\n",
      "Dataset loaded successfully: cot_zsopt_data\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "Found summarization examples in cot_zsopt_data\n",
      "Combined dataset with 248 summarization examples\n"
     ]
    }
   ],
   "source": [
    "# dataset = load_summarization_datasets(subset_names=['niv2_zsopt_data', 'cot_zsopt_data', 'dialog_zsopt_data'])\n",
    "dataset = load_summarization_datasets(subset_names=FLAN_SUBSET, subset_frac=SUBSET_FRAC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  LORA\n",
    "\n",
    "### Configure BitsAndBytes for 4-bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure BitsAndBytes for 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",           # Use 4-bit NormalFloat quantization\n",
    "    bnb_4bit_use_double_quant=True,      # Use double quantization for additional memory savings\n",
    "    bnb_4bit_compute_dtype=torch.float32  # Compute in float32 (can also use torch.bfloat16 if available)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID       = \"EleutherAI/pythia-160m-deduped\"\n",
    "MODEL_REVISION = \"step143000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    revision=MODEL_REVISION,\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "\n",
    "# Ensure the tokenizer has padding token set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the model with quantization\n",
    "model = GPTNeoXForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    revision=MODEL_REVISION,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"  # Automatically distribute layers across available GPUs\n",
    ")\n",
    "\n",
    "# Prepare the model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LoRA configuration\n",
    "# For Pythia models, the target module is \"query_key_value\" for attention layers\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_RANK,                    # Rank dimension\n",
    "    lora_alpha=LORA_ALPHA,          # LoRA scaling factor\n",
    "    target_modules=TARGET_MODULES,  # Target specific attention modules\n",
    "    lora_dropout=LORA_DROPOUT,      # Dropout probability for LoRA layers\n",
    "    bias=\"none\",            # Don't apply LoRA to bias terms\n",
    "    task_type=\"CAUSAL_LM\"   # Task type for causal language modeling\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "peft_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 294912 || all params: 120150528 || trainable%: 0.25\n"
     ]
    }
   ],
   "source": [
    "# Print trainable parameters information\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(peft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Dataset Formatting and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda03f8d1f564989ae3ec4807a3556b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/248 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to format the dataset for causal language modeling\n",
    "def format_for_clm(examples):\n",
    "    # Format as: \"Instruction: {instruction} Input: {input} Output: {output}\"\n",
    "    # Adjust this format based on your specific dataset structure\n",
    "    if 'inputs' in examples and 'targets' in examples:\n",
    "        texts = [\n",
    "            f\"{inp}\\n{target}{tokenizer.eos_token}\"\n",
    "            for inp, target in zip(examples['inputs'], examples['targets'])\n",
    "        ]\n",
    "    else:\n",
    "        # Fallback for other dataset structures\n",
    "        texts = examples['text'] if 'text' in examples else []\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Apply formatting \n",
    "if isinstance(dataset, Dataset):\n",
    "    # For non-streaming datasets\n",
    "    if 'inputs' in dataset.column_names and 'targets' in dataset.column_names:\n",
    "        dataset = dataset.map(format_for_clm, batched=True, num_proc=4)\n",
    "else:\n",
    "    # For streaming datasets, we need to format each example as it comes\n",
    "    dataset = dataset.map(lambda example: {\n",
    "        'text': f\"Instruction: {example['inputs']}\\nOutput: {example['targets']}\" \n",
    "        if 'inputs' in example and 'targets' in example \n",
    "        else example.get('text', '')\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8f93e6d3cb24cb5adab843e7b0fb5d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/248 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Tokenize function for the dataset\n",
    "def tokenize_function(example):\n",
    "    # Handle single examples for streaming datasets\n",
    "    text = example[\"text\"] if \"text\" in example else \"\"\n",
    "    \n",
    "    # Tokenize with padding and truncation\n",
    "    outputs = tokenizer(\n",
    "        text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=1024,  # Adjust based on your needs and GPU memory\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Remove the batch dimension for single examples\n",
    "    for key in outputs:\n",
    "        if isinstance(outputs[key], torch.Tensor) and outputs[key].ndim > 1:\n",
    "            outputs[key] = outputs[key].squeeze(0)\n",
    "    \n",
    "    # Set labels equal to input_ids for causal language modeling\n",
    "    outputs[\"labels\"] = outputs[\"input_ids\"].clone()\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "# Apply tokenization to the dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = tokenized_dataset.train_test_split(test_size=TEST_SIZE)\n",
    "train_dataset = split_dataset['train']\n",
    "val_dataset   = split_dataset['test']  # Note: called 'test' by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Since we're using streaming datasets, convert to list for training\n",
    "# # This is needed because Trainer expects a non-streaming dataset\n",
    "# # We'll create a buffer of examples for training\n",
    "# buffer_size = 25000  # Adjust based on your memory constraints\n",
    "# tokenized_examples = []\n",
    "# for example in tqdm(tokenized_dataset, total=buffer_size):\n",
    "#     tokenized_examples.append(example)\n",
    "#     if len(tokenized_examples) >= buffer_size:\n",
    "#         break\n",
    "\n",
    "# print(f\"Collected {len(tokenized_examples)} examples for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert to Dataset object for training\n",
    "# from datasets import Dataset as HFDataset\n",
    "# train_dataset = HFDataset.from_list(tokenized_examples)\n",
    "\n",
    "# print(f\"Training dataset created with columns: {train_dataset.column_names}\")\n",
    "# print(f\"Number of examples: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation test callback\n",
    "class GenerationTestCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Callback to generate text samples at evaluation steps.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, test_prompts, max_length=100, do_sample=True, \n",
    "                 num_beams=2, temperature=0.1, device=\"cuda\"):\n",
    "        \"\"\"\n",
    "        Initialize with tokenizer and test prompts.\n",
    "        \"\"\"\n",
    "        self.tokenizer    = tokenizer\n",
    "        self.test_prompts = test_prompts\n",
    "        self.device       = device\n",
    "        self.max_length   = max_length\n",
    "        self.do_sample    = do_sample\n",
    "        self.num_beams    = num_beams\n",
    "        self.temperature  = temperature\n",
    "            \n",
    "            \n",
    "    def on_evaluate(self, args, state, control, model, **kwargs):\n",
    "        \"\"\"\n",
    "        Run after each evaluation to generate two types of samples:\n",
    "        1. Free-form completion with sampling\n",
    "        2. Greedy decoding for deterministic output\n",
    "        \"\"\"\n",
    "        print(\"GenerationTestCallback\")\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"Generating samples at step {state.global_step}:\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for prompt in self.test_prompts:\n",
    "                inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "                \n",
    "                # 1. Generate with sampling\n",
    "                sample_output_ids = model.generate(\n",
    "                    inputs[\"input_ids\"],\n",
    "                    attention_mask    = inputs[\"attention_mask\"],\n",
    "                    max_length        = self.max_length,\n",
    "                    do_sample         = True,\n",
    "                    temperature       = self.temperature,\n",
    "                    pad_token_id      = self.tokenizer.eos_token_id,\n",
    "                    # eos_token_id      = self.tokenizer.eos_token_id\n",
    "                )\n",
    "                \n",
    "                # 2. Generate with greedy decoding (deterministic) and no sampling\n",
    "                greedy_output_ids = model.generate(\n",
    "                    inputs[\"input_ids\"],\n",
    "                    attention_mask    = inputs[\"attention_mask\"],\n",
    "                    max_length        = self.max_length,\n",
    "                    do_sample         = True,\n",
    "                    pad_token_id      = self.tokenizer.eos_token_id,\n",
    "                    # pad_token_id      = self.tokenizer.pad_token_id,\n",
    "                    # eos_token_id      = self.tokenizer.eos_token_id\n",
    "                )\n",
    "                \n",
    "                # Decode both outputs\n",
    "                sampled_text = self.tokenizer.decode(sample_output_ids[0], skip_special_tokens=True)\n",
    "                greedy_text  = self.tokenizer.decode(greedy_output_ids[0], skip_special_tokens=True)\n",
    "                \n",
    "                # Print the results\n",
    "                print(f\"\\nPrompt: {prompt}\")\n",
    "                print(f\"Greedy: {greedy_text}\")\n",
    "                print(\"-\"*50)\n",
    "                \n",
    "                # Log to W&B if you're using it\n",
    "                if args.report_to == \"wandb\":\n",
    "                    import wandb\n",
    "                    wandb.log({\n",
    "                        f\"generation/{prompt}/sampled\": wandb.Html(\n",
    "                            f\"<b>Step {state.global_step}</b><br>\"\n",
    "                            f\"<p><b>Prompt:</b> {prompt}</p>\"\n",
    "                            f\"<p><b>Sampled:</b> {sampled_text}</p>\"\n",
    "                        ),\n",
    "                        f\"generation/{prompt}/greedy\": wandb.Html(\n",
    "                            f\"<b>Step {state.global_step}</b><br>\"\n",
    "                            f\"<p><b>Prompt:</b> {prompt}</p>\"\n",
    "                            f\"<p><b>Greedy:</b> {greedy_text}</p>\"\n",
    "                        )\n",
    "                    }, step=state.global_step)\n",
    "        \n",
    "        return control\n",
    "\n",
    "# Create generation callback\n",
    "generation_callback = GenerationTestCallback(\n",
    "    tokenizer     = tokenizer,\n",
    "    test_prompts  = test_prompts,\n",
    "    max_length    = 250,\n",
    "    num_beams     = 3,\n",
    "    temperature   = 0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>█▁</td></tr><tr><td>eval/samples_per_second</td><td>▁█</td></tr><tr><td>eval/steps_per_second</td><td>▁█</td></tr><tr><td>train/epoch</td><td>█▁▁</td></tr><tr><td>train/global_step</td><td>▁██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>3.26039</td></tr><tr><td>eval/runtime</td><td>0.165</td></tr><tr><td>eval/samples_per_second</td><td>78.807</td></tr><tr><td>eval/steps_per_second</td><td>12.124</td></tr><tr><td>total_flos</td><td>41896803041280.0</td></tr><tr><td>train/epoch</td><td>3.26667</td></tr><tr><td>train/global_step</td><td>10</td></tr><tr><td>train_loss</td><td>3.137</td></tr><tr><td>train_runtime</td><td>3.0502</td></tr><tr><td>train_samples_per_second</td><td>18.032</td></tr><tr><td>train_steps_per_second</td><td>1.639</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lora-tuning-1</strong> at: <a href='https://wandb.ai/11785_finetuning/ivan-testing/runs/1j21adxf' target=\"_blank\">https://wandb.ai/11785_finetuning/ivan-testing/runs/1j21adxf</a><br> View project at: <a href='https://wandb.ai/11785_finetuning/ivan-testing' target=\"_blank\">https://wandb.ai/11785_finetuning/ivan-testing</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250412_211711-1j21adxf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/ocean/projects/cis250068p/iwiryadi/idl-project/wandb/run-20250412_211946-vp93ouy5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/11785_finetuning/ivan-testing/runs/vp93ouy5' target=\"_blank\">lora-tuning-1</a></strong> to <a href='https://wandb.ai/11785_finetuning/ivan-testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/11785_finetuning/ivan-testing' target=\"_blank\">https://wandb.ai/11785_finetuning/ivan-testing</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/11785_finetuning/ivan-testing/runs/vp93ouy5' target=\"_blank\">https://wandb.ai/11785_finetuning/ivan-testing/runs/vp93ouy5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/11785_finetuning/ivan-testing/runs/vp93ouy5?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x14c18e17b610>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(entity=\"11785_finetuning\", project='ivan-testing', name=RUN_NAME, reinit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/iwiryadi/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:46, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.263658</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationTestCallback\n",
      "\n",
      "==================================================\n",
      "Generating samples at step 10:\n",
      "==================================================\n",
      "\n",
      "Prompt: Who is Barack Obama?\n",
      "Greedy: Who is Barack Obama? The answer is just a blank red marker.\n",
      "\n",
      "What can Obama say about Democrats?\n",
      "\n",
      "What do Obama say? If you look at Barack, he says things as though he says Obama. Obama is not even sure that the first thing that Barack says, he is not sure Obama does it.\n",
      "\n",
      "Do we know what Obama says? Obama tells us all about the world and Obama to that we are in the world.\n",
      "\n",
      "As for Obama? He doesn\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt: What is Carnegie Mellon University?\n",
      "Greedy: What is Carnegie Mellon University?\n",
      "\n",
      "There is a big difference between education, particularly in high schools (in the Carnegie Mellon University system), and academic performance, which is measured by math and science, education. Learning for the school-going school has always been a part of life and can be valued in a variety of ways, such as at school or business. Thus, to learn from someone with education, it becomes an important investment in that education that has to be done in education.\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt: Classify this restaurant review sentiment: 'The food was absolutely delicious but the service was extremely slow and the waiter seemed uninterested in helping us.'\n",
      "Greedy: Classify this restaurant review sentiment: 'The food was absolutely delicious but the service was extremely slow and the waiter seemed uninterested in helping us.'\n",
      "\n",
      "\"There are no extras or dessert portions. It's in the middle of the menu: 'This is a very hot dinner.' \"\n",
      "\n",
      "\"After being overdosed, we returned to the restaurant after breakfast at 4:00. We were very pleased with this experience. We knew all the ingredients of this place and also appreciated the time\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt: Compare and contrast Carnegie Mellon University's Computer Science and Information Systems programs in terms of research focus and career outcomes.\n",
      "Greedy: Compare and contrast Carnegie Mellon University's Computer Science and Information Systems programs in terms of research focus and career outcomes.\n",
      "\n",
      "The aim of this manuscript is to demonstrate a new approach to data management, and how it can contribute to the research. To provide a comprehensive review of this review.\n",
      "\n",
      "-This article was primarily created according to the Preferred Reporting Frameworks to Journal of Medical Literature (PMJMyM) guidelines, and was based only on the criteria defined in the following research topic\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt: Summarize in one sentence: Dr. Sarah Chen, lead scientist on the mission, called it 'the most significant discovery in the history of space exploration.' The finding suggests that Mars once had a much more hospitable environment with liquid water and possibly a thicker atmosphere. The agency plans to send a sample return mission within the next five years to bring these fossils back to Earth for more detailed analysis. This discovery has profound implications for our understanding of how life might develop throughout the universe.\n",
      "Greedy: Summarize in one sentence: Dr. Sarah Chen, lead scientist on the mission, called it 'the most significant discovery in the history of space exploration.' The finding suggests that Mars once had a much more hospitable environment with liquid water and possibly a thicker atmosphere. The agency plans to send a sample return mission within the next five years to bring these fossils back to Earth for more detailed analysis. This discovery has profound implications for our understanding of how life might develop throughout the universe.\n",
      "\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15, training_loss=4.202545674641927, metrics={'train_runtime': 48.4882, 'train_samples_per_second': 24.233, 'train_steps_per_second': 0.309, 'total_flos': 804418618392576.0, 'train_loss': 4.202545674641927, 'epoch': 4.8})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set format to PyTorch\n",
    "train_dataset.set_format(type=\"torch\")\n",
    "val_dataset.set_format(type=\"torch\")\n",
    "\n",
    "# Create training arguments with parameters\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size  = BATCH_SIZE,\n",
    "    per_device_eval_batch_size   = BATCH_SIZE,\n",
    "    gradient_accumulation_steps  = GRADIENT_ACCUMULATION_STEPS,\n",
    "    warmup_steps                 = WARMUP_STEPS,\n",
    "    num_train_epochs             = NUM_EPOCHS,\n",
    "    learning_rate                = LEARNING_RATE,\n",
    "    fp16                         = FP16,\n",
    "    logging_steps                = LOGGING_STEPS,\n",
    "    save_steps                   = SAVE_STEPS,\n",
    "    eval_strategy                = \"steps\",\n",
    "    eval_steps                   = EVAL_STEPS,\n",
    "    output_dir                   = OUTPUT_DIR,\n",
    "    optim                        = \"paged_adamw_8bit\", \n",
    "    save_total_limit             = SAVE_TOTAL_LIMIT,\n",
    "    \n",
    "    report_to                    = \"wandb\",\n",
    "    weight_decay                 = WEIGHT_DECAY,\n",
    "    # load_best_model_at_end       = True,\n",
    "    # metric_for_best_model        = \"eval_loss\",\n",
    "    # greater_is_better            = False,\n",
    ")\n",
    "\n",
    "\n",
    "# Set up the trainer with validation\n",
    "trainer = Trainer(\n",
    "    model                     = peft_model,\n",
    "    args                      = training_args,\n",
    "    train_dataset             = train_dataset,\n",
    "    eval_dataset              = val_dataset,\n",
    "    data_collator             = DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    compute_metrics           = None,  \n",
    "    callbacks                 = [generation_callback],  \n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "# Disable caching during training to avoid memory issues\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "peft_model.save_pretrained(f\"{OUTPUT_DIR}/{RUN_NAME}\")\n",
    "tokenizer.save_pretrained(f\"{OUTPUT_DIR}/{RUN_NAME}\")\n",
    "\n",
    "print(\"Training complete and model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_test_model():\n",
    "    print(\"\\nTesting fine-tuned model:\")\n",
    "    \n",
    "    # Load the base model and LoRA adapter\n",
    "    # base_model = GPTNeoXForCausalLM.from_pretrained(\n",
    "    #     MODEL_ID,\n",
    "    #     revision=MODEL_REVISION,\n",
    "    #     cache_dir=CACHE_DIR,\n",
    "    #     device_map=\"auto\"\n",
    "    # )\n",
    "    \n",
    "    # # Load and apply the fine-tuned LoRA weights\n",
    "    # fine_tuned_model = PeftModel.from_pretrained(\n",
    "    #     base_model, \n",
    "    #     f\"{OUTPUT_DIR}/lora_model\",\n",
    "    #     device_map=\"auto\"\n",
    "    # )\n",
    "    \n",
    "    fine_tuned_model = peft_model\n",
    "    \n",
    "    # Test the model with the prompts\n",
    "    for test_input_string in test_prompts:\n",
    "        inputs = tokenizer(test_input_string, return_tensors=\"pt\").to(DEVICE)\n",
    "        # print(tokens[0])\n",
    "        tokens = fine_tuned_model.generate(\n",
    "            **inputs, \n",
    "            max_length=100, \n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        print(\"Without sampling: \" + tokenizer.decode(tokens[0], skip_special_tokens=True))\n",
    "        \n",
    "        print(\"---------------\")\n",
    "        tokens = fine_tuned_model.generate(\n",
    "            **inputs, \n",
    "            max_length=100, \n",
    "            pad_token_id=tokenizer.eos_token_id, \n",
    "            do_sample=True,\n",
    "        )\n",
    "        print(\"With sampling   : \" + tokenizer.decode(tokens[0], skip_special_tokens=True))\n",
    "        \n",
    "        print(\"\\n===============\")\n",
    "    \n",
    "load_and_test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: False reverse\n",
      "Context: Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him.is a summary of\n",
      "Target: Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday\n",
      "Full sentence: Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him.is a summary ofHarry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday\n",
      "\n",
      "\n",
      "-3.3839298650308702 29.48642137573805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/iwiryadi/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/jet/home/iwiryadi/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from src.utils import * \n",
    "article = \"Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him.\"\n",
    "summary = \"Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday\"\n",
    "adverse_summary = \"Daniel Craig is recasted as James Bond again\"\n",
    "# In normal, query is sentence/article, and answer is summary/highlight (S->A direction)\n",
    "base = calculate_score(summary, article, model, tokenizer, backward=False, query_direction=\"reverse\", debug=True)\n",
    "\n",
    "print(base['normalized_log_prob'], base['perplexity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>▁▁</td></tr><tr><td>eval/samples_per_second</td><td>█▁</td></tr><tr><td>eval/steps_per_second</td><td>█▁</td></tr><tr><td>train/epoch</td><td>▁▁█</td></tr><tr><td>train/global_step</td><td>▁▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>3.26366</td></tr><tr><td>eval/runtime</td><td>0.1651</td></tr><tr><td>eval/samples_per_second</td><td>78.747</td></tr><tr><td>eval/steps_per_second</td><td>12.115</td></tr><tr><td>total_flos</td><td>804418618392576.0</td></tr><tr><td>train/epoch</td><td>4.8</td></tr><tr><td>train/global_step</td><td>15</td></tr><tr><td>train_loss</td><td>4.20255</td></tr><tr><td>train_runtime</td><td>48.4882</td></tr><tr><td>train_samples_per_second</td><td>24.233</td></tr><tr><td>train_steps_per_second</td><td>0.309</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lora-tuning-1</strong> at: <a href='https://wandb.ai/11785_finetuning/ivan-testing/runs/vp93ouy5' target=\"_blank\">https://wandb.ai/11785_finetuning/ivan-testing/runs/vp93ouy5</a><br> View project at: <a href='https://wandb.ai/11785_finetuning/ivan-testing' target=\"_blank\">https://wandb.ai/11785_finetuning/ivan-testing</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250412_211946-vp93ouy5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
